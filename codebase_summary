{
  "cmd/consumer/main.go": "- File Path: `cmd/consumer/main.go`\n- High-Level Purpose: This is the main entry point for the log processing consumer application. It initializes configuration, sets up logging, connects to Redis and PostgreSQL, creates the necessary repositories and use cases, and starts a loop to continuously process log batches from Redis Streams and write them to PostgreSQL.\n- Definitions in the File:\n  - Constants:\n    - `consumerGroup` (string): \"log-processors\". The name of the Redis consumer group this consumer belongs to.\n    - `processingInterval` (time.Duration): 1 * time.Second. The interval at which the consumer attempts to process a batch of logs.\n  - Functions:\n    - `main()`: Main function.\n      - Description:\n        1. Loads application configuration using `config.Load()`.\n        2. Initializes a structured logger using `logger.New()` and generates a unique consumer name.\n        3. Establishes connections to PostgreSQL (`database/sql`) and Redis (`github.com/redis/go-redis/v9`).\n        4. Creates `redisrepo.NewLogRepository` (as the buffer, without a WAL) and `postgres.NewLogRepository` (as the sink).\n        5. Instantiates `usecase.NewProcessLogsUseCase` with the configured repositories and retry settings.\n        6. Sets up a graceful shutdown mechanism using `context` and `os.Signal` (SIGINT, SIGTERM).\n        7. Enters a continuous loop, processing log batches at `processingInterval` until a shutdown signal is received or the context is canceled.\n- Notable Patterns or Logic:\n  - Application entry point (`main` function).\n  - Dependency injection for services (repositories, use cases).\n  - Graceful shutdown handling using `context` and `os.Signal`.\n  - Continuous polling loop for processing messages from a queue (Redis Stream).\n  - Separation of concerns: `main` orchestrates components, business logic is in `usecase`, data access in `repository`.\n",
  "cmd/ingest/main.go": "- File Path: `cmd/ingest/main.go`\n- High-Level Purpose: This is the main entry point for the log ingestor service. It handles application startup, configuration loading, dependency initialization (database, Redis, WAL, PII redactor, repositories, use cases, SSE broker), HTTP server setup, Prometheus metrics exposure, and graceful shutdown.\n- Definitions in the File:\n  - Functions:\n    - `main()`: Main function.\n      - Description:\n        1.  Loads application configuration.\n        2.  Initializes a structured logger.\n        3.  Sets up a Prometheus metrics server on a separate port (`:9091`).\n        4.  Establishes a context for graceful shutdown, listening for `SIGINT` and `SIGTERM`.\n        5.  Connects to PostgreSQL and Redis, with a warning if Redis is unavailable.\n        6.  Initializes the `WALRepository` for durable log storage.\n        7.  Creates `APIKeyRepository` (PostgreSQL with cache) and `RedisLogRepository` (Redis Streams with WAL failover).\n        8.  Starts a background goroutine for Redis health checking and WAL replay.\n        9.  Initializes the `PIIRedactor` based on configured fields.\n        10. Creates the `IngestLogUseCase` to handle log ingestion logic.\n        11. Initializes an `SSEBroker` for real-time event rate reporting.\n        12. Configures and starts the main HTTP ingest server, including routes, handlers, and middleware.\n        13. Waits for a shutdown signal, then gracefully shuts down both the ingest and admin/metrics HTTP servers.\n- Notable Patterns or Logic:\n  - Application entry point (`main` function).\n  - Dependency Injection for all major components.\n  - Graceful shutdown mechanism using `context` and `os.Signal`.\n  - Observability: Prometheus metrics endpoint and structured logging.\n  - Failover mechanism: Redis `LogRepository` integrates with `WALRepository`.\n  - HTTP server setup with custom timeouts and routing.\n  - Separation of concerns: `main` orchestrates, business logic in `usecase`, data access in `repository`.\n",
  "go.mod": "- File Path: `go.mod`\n- High-Level Purpose: This file defines the Go module path for the `log-ingestor` project and lists all direct and indirect external dependencies required for the application.\n- Definitions in the File:\n  - Module: `github.com/user/log-ingestor`\n  - Go Version: `1.21`\n  - Direct Dependencies:\n    - `github.com/caarlos0/env/v10 v10.0.0`: For loading environment variables into structs.\n    - `github.com/google/uuid v1.6.0`: For generating UUIDs.\n    - `github.com/joho/godotenv v1.5.1`: For loading `.env` files.\n    - `github.com/lib/pq v1.10.9`: PostgreSQL driver.\n    - `github.com/prometheus/client_golang v1.19.0`: Prometheus client library for Go applications.\n    - `github.com/redis/go-redis/v9 v9.5.1`: Redis client for Go.\n  - Indirect Dependencies: (Various transitive dependencies for Prometheus and Redis clients)\n- Notable Patterns or Logic:\n  - Standard Go module definition.\n  - Explicit versioning for dependencies.\n",
  "internal/adapter/api/handler/ingest_handler.go": "- File Path: `internal/adapter/api/handler/ingest_handler.go`\n- High-Level Purpose: This file defines an HTTP handler responsible for receiving and processing incoming log ingestion requests. It supports both single JSON objects and Newline Delimited JSON (NDJSON) streams, enforces maximum event size, and delegates the core ingestion logic to an `IngestLogUseCase`. It also reports accepted events to an SSE broker for real-time monitoring.\n- Definitions in the File:\n  - Constants:\n    - `contentTypeJSON` (string): \"application/json\".\n    - `contentTypeNDJSON` (string): \"application/x-ndjson\".\n  - Classes / Structs / Interfaces:\n    - `IngestHandler` (struct): Handles HTTP requests for log ingestion.\n      - Fields:\n        - `useCase` (*usecase.IngestLogUseCase): The business logic component for ingesting logs.\n        - `logger` (*slog.Logger): Structured logger.\n        - `maxEventSize` (int64): Configurable maximum allowed size for an individual log event payload.\n        - `metrics` (*metrics.IngestMetrics): Prometheus metrics collector for ingestion statistics.\n        - `sseBroker` (*SSEBroker): A Server-Sent Events broker to report processed event counts.\n  - Functions:\n    - `NewIngestHandler(uc *usecase.IngestLogUseCase, logger *slog.Logger, maxEventSize int64, m *metrics.IngestMetrics, sse *SSEBroker) *IngestHandler`: Public function.\n      - Description: Constructor for `IngestHandler`. Initializes the handler with the ingest use case, logger, maximum event size, metrics, and SSE broker.\n    - `(*IngestHandler) ServeHTTP(w http.ResponseWriter, r *http.Request)`: Public method (implements `http.Handler`).\n      - Description: The main entry point for HTTP requests. It validates the request method (must be POST), enforces `maxEventSize`, determines the content type (`application/json` or `application/x-ndjson`), and dispatches to the appropriate internal handler. It also records various ingestion metrics.\n    - `(*IngestHandler) handleSingleJSON(ctx context.Context, body io.Reader) error`: Internal method.\n      - Description: Reads the entire request body, unmarshals it as a single JSON `LogEvent`, enriches it with the raw payload, and passes it to the `IngestLogUseCase`. Reports a single event to the SSE broker.\n    - `(*IngestHandler) handleNDJSON(ctx context.Context, body io.Reader) error`: Internal method.\n      - Description: Uses a `bufio.Scanner` to read the request body line by line, treating each line as a separate JSON `LogEvent`. Each event is unmarshaled, enriched, and passed to the `IngestLogUseCase`. Reports the total count of processed events to the SSE broker.\n- Notable Patterns or Logic:\n  - HTTP Handler pattern (`http.Handler` interface).\n  - Content-Type based request dispatching.\n  - Request body size limiting using `http.MaxBytesReader`.\n  - Handling of both single JSON and Newline Delimited JSON (NDJSON) formats.\n  - Dependency injection for `IngestLogUseCase`, `slog.Logger`, `metrics.IngestMetrics`, and `SSEBroker`.\n  - Error handling with specific HTTP status codes (e.g., `405 Method Not Allowed`, `415 Unsupported Media Type`, `413 Request Entity Too Large`, `400 Bad Request`).\n",
  "internal/adapter/api/handler/sse_handler.go": "- File Path: `internal/adapter/api/handler/sse_handler.go`\n- High-Level Purpose: This file implements a Server-Sent Events (SSE) broker that allows clients to subscribe to a real-time stream of event processing rates. It aggregates event counts reported by other components and broadcasts the calculated rate periodically.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `SSEMessage` (struct): Defines the JSON structure for messages sent over the SSE stream.\n      - Fields:\n        - `Rate` (float64): The calculated event processing rate (events per second).\n    - `SSEBroker` (struct): Manages SSE client connections and broadcasts rate messages.\n      - Fields:\n        - `logger` (*slog.Logger): Structured logger.\n        - `clients` (map[chan []byte]struct{}): A map to keep track of all active SSE client channels.\n        - `mu` (sync.RWMutex): A read-write mutex to protect concurrent access to the `clients` map.\n        - `eventCounter` (chan int): A buffered channel used to receive event counts from other parts of the application.\n  - Functions:\n    - `NewSSEBroker(ctx context.Context, logger *slog.Logger) *SSEBroker`: Public function.\n      - Description: Constructor for `SSEBroker`. Initializes the broker with a logger and starts a background goroutine (`run`) to handle event aggregation and broadcasting.\n    - `(*SSEBroker) ServeHTTP(w http.ResponseWriter, r *http.Request)`: Public method (implements `http.Handler`).\n      - Description: Handles new HTTP requests for SSE connections. It sets appropriate HTTP headers for SSE, creates a new channel for the client, adds it to the broker's client list, and then continuously sends messages from that channel to the client until the connection is closed or the context is canceled.\n    - `(*SSEBroker) ReportEvents(count int)`: Public method.\n      - Description: Allows other components (e.g., `IngestHandler`) to report a number of processed events to the broker. It sends the count to the `eventCounter` channel, dropping the report if the channel is full to avoid blocking.\n    - `(*SSEBroker) addClient(client chan []byte)`: Internal method.\n      - Description: Adds a new client's message channel to the broker's list of active clients, protected by a write lock.\n    - `(*SSEBroker) removeClient(client chan []byte)`: Internal method.\n      - Description: Removes a client's message channel from the broker's list and closes the channel, protected by a write lock.\n    - `(*SSEBroker) broadcast(msg []byte)`: Internal method.\n      - Description: Sends a given message (as a byte slice) to all currently connected SSE clients. It uses a read lock and non-blocking sends to avoid being stalled by slow clients.\n    - `(*SSEBroker) run(ctx context.Context)`: Internal method.\n      - Description: The main event loop for the broker. It runs in a separate goroutine, aggregates event counts received via `eventCounter` over a 1-second interval, calculates the events-per-second rate, marshals it into an `SSEMessage`, and then broadcasts this message to all connected clients.\n- Notable Patterns or Logic:\n  - Server-Sent Events (SSE) implementation for real-time updates.\n  - Publisher-Subscriber (Pub/Sub) pattern for broadcasting messages to multiple clients.\n  - Concurrency management using goroutines, channels, and `sync.RWMutex`.\n  - Rate limiting/aggregation of events over time.\n  - Non-blocking channel sends for robustness against slow consumers.\n",
  "internal/adapter/api/middleware/auth.go": "- File Path: `internal/adapter/api/middleware/auth.go`\n- High-Level Purpose: This file defines an HTTP middleware for authenticating incoming requests by validating an API key provided in the `X-API-Key` header.\n- Definitions in the File:\n  - Constants:\n    - `APIKeyHeader` (string): \"X-API-Key\". Specifies the HTTP header name for the API key.\n  - Functions:\n    - `Auth(repo domain.APIKeyRepository, logger *slog.Logger) func(http.Handler) http.Handler`: Public function.\n      - Description: A middleware factory that returns an `http.Handler` function. It extracts the API key from the request header, uses the provided `APIKeyRepository` to validate it, and either allows the request to proceed or responds with an `Unauthorized` or `Internal Server Error` status.\n- Notable Patterns or Logic:\n  - HTTP Middleware pattern for request interception and processing.\n  - Dependency injection for the API key repository and logger.\n",
  "internal/adapter/api/router.go": "- File Path: `internal/adapter/api/router.go`\n- High-Level Purpose: This file is responsible for creating and configuring the main HTTP router for the log ingestor's API. It sets up various routes, applies middleware for authentication, and integrates different handlers for log ingestion, SSE events, and health checks.\n- Definitions in the File:\n  - Functions:\n    - `NewRouter(cfg *config.Config, logger *slog.Logger, apiKeyRepo domain.APIKeyRepository, ingestUseCase *usecase.IngestLogUseCase, m *metrics.IngestMetrics, sseBroker *handler.SSEBroker) http.Handler`: Public function.\n      - Description: Creates an `http.ServeMux` (multiplexer) and configures the application's HTTP routes. It applies an authentication middleware (`middleware.Auth`) to the `/ingest` endpoint, registers the `IngestHandler` for log submission, the `SSEBroker` for real-time event updates, and a simple `/health` endpoint.\n- Notable Patterns or Logic:\n  - HTTP Router configuration using `http.ServeMux`.\n  - Middleware pattern for cross-cutting concerns (e.g., authentication).\n  - Dependency Injection for handlers, middleware, and other services.\n  - Clear separation of concerns between routing, authentication, and business logic.",
  "internal/adapter/metrics/metrics.go": "- File Path: `internal/adapter/metrics/metrics.go`\n- High-Level Purpose: This file defines and initializes a set of Prometheus metrics specifically for the log ingestor service, allowing for monitoring of ingestion rates, byte volume, WAL activity, and API key cache performance.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `IngestMetrics` (struct): Holds references to all Prometheus metric instances used by the ingest service.\n      - Fields:\n        - `EventsTotal` (*prometheus.CounterVec): A counter vector to track the total number of ingested events, categorized by their processing status (e.g., \"accepted\", \"error_parse\").\n        - `BytesTotal` (prometheus.Counter): A counter to track the total number of bytes ingested by the service.\n        - `WALActive` (prometheus.Gauge): A gauge to indicate whether the Write-Ahead Log is currently active (1) or inactive (0), reflecting Redis availability.\n        - `APIKeyCacheHits` (prometheus.Counter): A counter for the total number of times an API key was found in the cache.\n        - `APIKeyCacheMisses` (prometheus.Counter): A counter for the total number of times an API key was not found in the cache, requiring a database lookup.\n  - Functions:\n    - `NewIngestMetrics() *IngestMetrics`: Public function.\n      - Description: Constructor for `IngestMetrics`. It initializes and registers all the Prometheus metrics (CounterVec, Counter, Gauge) with appropriate namespaces, subsystems, names, help strings, and labels.\n- Notable Patterns or Logic:\n  - Observability pattern using Prometheus.\n  - Centralized definition and initialization of application metrics.\n  - Use of `prometheus.CounterVec` for metrics with multiple dimensions (e.g., event status).\n  - `promauto` package for automatic registration of metrics.\n",
  "internal/adapter/pii/redactor.go": "- File Path: `internal/adapter/pii/redactor.go`\n- High-Level Purpose: This file defines a `Redactor` component responsible for identifying and replacing sensitive (PII) fields within the JSON `Metadata` of `LogEvent` objects with a placeholder.\n- Definitions in the File:\n  - Constants:\n    - `RedactedPlaceholder` (string): \"[REDACTED]\". The string used to replace redacted PII values.\n  - Classes / Structs / Interfaces:\n    - `Redactor` (struct): Manages the redaction of specified PII fields from log event metadata.\n      - Fields:\n        - `fieldsToRedact` (map[string]struct{}): A set of field names whose values should be redacted.\n        - `logger` (*slog.Logger): Structured logger instance.\n  - Functions:\n    - `NewRedactor(fields []string, logger *slog.Logger) *Redactor`: Public function.\n      - Description: Constructor for `Redactor`, initializing it with a list of field names to be redacted and a logger. It converts the list of fields into a map for efficient lookups.\n    - `(*Redactor) Redact(event *domain.LogEvent) error`: Public method.\n      - Description: Modifies the provided `LogEvent` in place. It unmarshals the `Metadata` field (expected to be JSON), iterates through the configured `fieldsToRedact`, and replaces any matching field values with `RedactedPlaceholder`. If any redaction occurs, it sets `event.PIIRedacted` to `true` and remarshals the modified metadata back into `event.Metadata`. It logs warnings or errors if JSON processing fails.\n- Notable Patterns or Logic:\n  - Data transformation/processing component.\n  - Dynamic JSON field manipulation using `encoding/json` for unmarshalling and marshalling.\n  - In-place modification of a domain object (`LogEvent`).",
  "internal/adapter/repository/postgres/apikey_repository.go": "- File Path: `internal/adapter/repository/postgres/apikey_repository.go`\n- High-Level Purpose: This file provides a PostgreSQL-backed implementation for the `domain.APIKeyRepository` interface, incorporating an in-memory, time-based cache to efficiently validate API keys.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `cacheEntry` (struct): Represents an entry in the API key cache.\n      - Fields:\n        - `isValid` (bool): True if the API key is valid.\n        - `expiresAt` (time.Time): Timestamp when the cache entry expires.\n    - `APIKeyRepository` (struct): Implements `domain.APIKeyRepository` with PostgreSQL and caching.\n      - Fields:\n        - `db` (*sql.DB): Database connection pool for PostgreSQL.\n        - `logger` (*slog.Logger): Structured logger.\n        - `cache` (map[string]cacheEntry): In-memory map storing cached API key validity.\n        - `mu` (sync.RWMutex): Read-write mutex to protect concurrent access to the cache.\n        - `cacheTTL` (time.Duration): Time-to-live for cache entries.\n        - `metrics` (*metrics.IngestMetrics): Prometheus metrics collector for cache hits/misses.\n  - Functions:\n    - `NewAPIKeyRepository(db *sql.DB, logger *slog.Logger, cacheTTL time.Duration, m *metrics.IngestMetrics) *APIKeyRepository`: Public function.\n      - Description: Constructor for `APIKeyRepository`. Initializes the repository with a database connection, logger, cache TTL, and metrics collector.\n    - `(*APIKeyRepository) IsValid(ctx context.Context, key string) (bool, error)`: Public method.\n      - Description: Checks if an API key is valid. It first attempts to retrieve the key from the in-memory cache. If not found or expired, it queries the PostgreSQL database, updates the cache, and returns the result. It uses a read-write mutex for safe concurrent cache access and records cache hit/miss metrics.\n- Notable Patterns or Logic:\n  - Repository pattern for API key validation.\n  - Read-through caching with time-based expiration.\n  - Concurrency control using `sync.RWMutex` for efficient cache access.\n  - Database query for API key validation, checking existence, active status, and expiration.\n  - Prometheus metrics integration for cache performance.\n",
  "internal/adapter/repository/postgres/log_repository.go": "- File Path: `internal/adapter/repository/postgres/log_repository.go`\n- High-Level Purpose: This file provides a PostgreSQL-backed implementation for the \"sink\" part of the `domain.LogRepository` interface, specifically designed for high-performance batch writing of log events using PostgreSQL's `COPY` protocol and idempotent upsert logic.\n- Definitions in the File:\n  - Constants:\n    - `logsTableName` (string): \"logs\". The name of the PostgreSQL table where logs are stored.\n  - Classes / Structs / Interfaces:\n    - `LogRepository` (struct): Implements the `domain.LogRepository` interface for PostgreSQL.\n      - Fields:\n        - `db` (*sql.DB): Database connection pool.\n        - `logger` (*slog.Logger): Structured logger instance.\n  - Functions:\n    - `NewLogRepository(db *sql.DB, logger *slog.Logger) *LogRepository`: Public function.\n      - Description: Constructor for `LogRepository`, initializing it with a database connection and logger.\n    - `(*LogRepository) WriteLogBatch(ctx context.Context, events []domain.LogEvent) error`: Public method.\n      - Description: Writes a batch of log events to PostgreSQL. It uses a database transaction, creates a temporary table, and leverages `pq.CopyIn` for efficient bulk data transfer. After staging, it performs an `INSERT ... ON CONFLICT (event_id) DO UPDATE` (upsert) from the temporary table into the main `logs` table, ensuring idempotency.\n    - `errNotImplemented` (var): Internal variable.\n      - Description: An error indicating that a method is not implemented for this specific repository type.\n    - `(*LogRepository) BufferLog(ctx context.Context, event domain.LogEvent) error`: Public method.\n      - Description: Returns `errNotImplemented` as this repository is not responsible for buffering.\n    - `(*LogRepository) ReadLogBatch(ctx context.Context, group, consumer string, count int) ([]domain.LogEvent, error)`: Public method.\n      - Description: Returns `nil, errNotImplemented` as this repository is not responsible for reading from a buffer.\n    - `(*LogRepository) AcknowledgeLogs(ctx context.Context, group string, messageIDs ...string) error`: Public method.\n      - Description: Returns `errNotImplemented` as this repository is not responsible for acknowledging buffered messages.\n- Notable Patterns or Logic:\n  - Repository pattern implementation for data sinking.\n  - High-performance bulk inserts using PostgreSQL's `COPY` protocol.\n  - Idempotent upsert logic using `ON CONFLICT DO UPDATE`.\n  - Transaction management for batch operations.\n  - Partial interface implementation, indicating a specialized role (sink only).\n",
  "internal/adapter/repository/redis/log_repository.go": "- File Path: `internal/adapter/repository/redis/log_repository.go`\n- High-Level Purpose: This file implements the `domain.LogRepository` interface using Redis Streams for buffering log events, providing mechanisms for writing, reading, acknowledging, and moving events to a Dead-Letter Queue. It includes a critical failover mechanism to a Write-Ahead Log (WAL) when Redis is unavailable.\n- Definitions in the File:\n  - Constants:\n    - `logStreamKey` (string): \"log_events\". The Redis Stream key used for buffering log events.\n  - Variables:\n    - `errNotImplemented` (error): An error indicating that a specific method is not implemented for this repository type.\n  - Classes / Structs / Interfaces:\n    - `LogRepository` (struct): Implements `domain.LogRepository` using Redis Streams with WAL failover.\n      - Fields:\n        - `client` (*redis.Client): The Redis client instance.\n        - `logger` (*slog.Logger): Structured logger.\n        - `wal` (domain.WALRepository): An optional Write-Ahead Log repository for durability during Redis outages.\n        - `dlqStreamKey` (string): The Redis Stream key for the Dead-Letter Queue.\n        - `isAvailable` (atomic.Bool): An atomic boolean flag indicating the current availability status of the Redis connection.\n        - `metrics` (*metrics.IngestMetrics): Prometheus metrics collector.\n  - Functions:\n    - `NewLogRepository(client *redis.Client, logger *slog.Logger, group, consumer, dlqStreamKey string, wal domain.WALRepository, m *metrics.IngestMetrics) (*LogRepository, error)`: Public function.\n      - Description: Constructor for `LogRepository`. Initializes the Redis client, logger, WAL (if provided), DLQ stream key, and metrics. It attempts to set up the Redis consumer group and assumes Redis is initially available.\n    - `(*LogRepository) StartHealthCheck(ctx context.Context, interval time.Duration)`: Public method.\n      - Description: Starts a background goroutine that periodically pings Redis. If Redis recovers from an outage, it triggers a `ReplayWAL`. If Redis becomes unavailable, it activates the WAL for buffering.\n    - `(*LogRepository) ReplayWAL(ctx context.Context) error`: Public method.\n      - Description: Reads all events from the configured WAL, buffers them into Redis, and then truncates the WAL upon successful replay. This is crucial for recovering events buffered during a Redis outage.\n    - `(*LogRepository) setupConsumerGroup(ctx context.Context, group string) error`: Internal method.\n      - Description: Creates a Redis consumer group for the `logStreamKey` if it doesn't already exist.\n    - `(*LogRepository) BufferLog(ctx context.Context, event domain.LogEvent) error`: Public method.\n      - Description: Buffers a single log event. If Redis is currently available, it writes to the Redis Stream. If Redis is unavailable (or a network error occurs during write), it falls back to writing the event to the WAL.\n    - `(*LogRepository) bufferLogToRedis(ctx context.Context, event domain.LogEvent) error`: Internal method.\n      - Description: Marshals a log event to JSON and adds it to the Redis Stream using `XADD`.\n    - `(*LogRepository) ReadLogBatch(ctx context.Context, group, consumer string, count int) ([]domain.LogEvent, error)`: Public method.\n      - Description: Reads a batch of log events from the Redis Stream for a specific consumer group and consumer using `XREADGROUP`.\n    - `(*LogRepository) AcknowledgeLogs(ctx context.Context, group string, messageIDs ...string) error`: Public method.\n      - Description: Acknowledges a list of processed messages in the Redis Stream for a given consumer group using `XACK`.\n    - `(*LogRepository) MoveToDLQ(ctx context.Context, events []domain.LogEvent) error`: Public method.\n      - Description: Moves a batch of log events to a Dead-Letter Queue (DLQ) Redis Stream, typically for events that failed processing after retries.\n    - `(*LogRepository) WriteLogBatch(ctx context.Context, events []domain.LogEvent) error`: Public method.\n      - Description: Returns `errNotImplemented` as this repository is designed for buffering/reading, not for final persistent storage writes.\n    - `isRedisBusyGroupError(err error) bool`: Internal function.\n      - Description: Helper to check if an error is a Redis `BUSYGROUP` error.\n    - `isNetworkError(err error) bool`: Internal function.\n      - Description: Helper to check if an error is a network-related error, indicating connectivity issues with Redis.\n- Notable Patterns or Logic:\n  - Repository pattern for Redis Streams.\n  - Write-Ahead Log (WAL) integration for failover and durability during Redis outages.\n  - Health checking and automatic WAL replay upon Redis recovery.\n  - Atomic operations (`atomic.Bool`) for managing Redis availability state.\n  - Redis Streams consumer group pattern for distributed message processing.\n  - Dead-Letter Queue (DLQ) implementation for handling unprocessable messages.\n  - Partial interface implementation, specializing in buffering and reading.\n",
  "internal/adapter/repository/wal/wal_repository.go": "- File Path: `internal/adapter/repository/wal/wal_repository.go`\n- High-Level Purpose: This file provides a file-based Write-Ahead Log (WAL) implementation for durable storage of log events, designed to handle temporary failures of primary storage systems. It supports segment rotation, replay, and truncation.\n- Definitions in the File:\n  - Constants:\n    - `segmentPrefix` (string): \"segment-\". Prefix for WAL segment filenames.\n    - `filePerm` (os.FileMode): 0644. Default file permissions for WAL segments.\n  - Classes / Structs / Interfaces:\n    - `WALRepository` (struct): Implements a file-based Write-Ahead Log.\n      - Fields:\n        - `dir` (string): Directory path where WAL segments are stored.\n        - `maxSegmentSize` (int64): Maximum size for a single WAL segment file before rotation.\n        - `maxTotalSize` (int64): Maximum total disk space allowed for all WAL segments.\n        - `logger` (*slog.Logger): Structured logger.\n        - `mu` (sync.Mutex): Mutex for protecting concurrent access to WAL operations.\n        - `currentSegment` (*os.File): File handle for the currently active WAL segment.\n        - `currentSize` (int64): Current size of the active WAL segment.\n  - Functions:\n    - `NewWALRepository(dir string, maxSegmentSize, maxTotalSize int64, logger *slog.Logger) (*WALRepository, error)`: Public function.\n      - Description: Constructor for `WALRepository`. Initializes the WAL directory, size limits, and logger. It attempts to open the latest existing segment or create a new one.\n    - `(*WALRepository) Write(ctx context.Context, event domain.LogEvent) error`: Public method.\n      - Description: Marshals a log event to JSON, appends a newline, and writes it to the current WAL segment. It checks for `maxTotalSize` before writing and rotates the segment if `maxSegmentSize` is reached.\n    - `(*WALRepository) Replay(ctx context.Context, handler func(event domain.LogEvent) error) error`: Public method.\n      - Description: Reads all events from all WAL segments in chronological order. For each event, it calls the provided `handler` function. It closes the current segment before replay and re-opens a new one after.\n    - `(*WALRepository) Truncate(ctx context.Context) error`: Public method.\n      - Description: Closes the current segment and removes all WAL segment files from the directory. It then opens a new, empty segment.\n    - `(*WALRepository) rotate() error`: Internal method.\n      - Description: Closes the current WAL segment (if open), generates a new segment filename based on the current timestamp, creates and opens the new file, and resets `currentSize`.\n    - `(*WALRepository) openLatestSegment() error`: Internal method.\n      - Description: Identifies the latest WAL segment file, opens it for appending, and sets `currentSize` to its current size. If no segments exist, it calls `rotate` to create a new one. If the latest segment is already full, it also calls `rotate`.\n    - `(*WALRepository) getSortedSegments() ([]string, error)`: Internal method.\n      - Description: Reads the WAL directory, filters for segment files, and returns their full paths sorted chronologically.\n    - `(*WALRepository) calculateTotalSize() (int64, error)`: Internal method.\n      - Description: Calculates the total disk space occupied by all WAL segment files.\n    - `(*WALRepository) Close() error`: Public method.\n      - Description: Closes the currently active WAL segment file, ensuring all buffered writes are flushed to disk.\n- Notable Patterns or Logic:\n  - Write-Ahead Log (WAL) implementation for durability and crash recovery.\n  - Segment-based storage with configurable `maxSegmentSize` and `maxTotalSize`.\n  - File system operations for managing log files (`os.MkdirAll`, `os.OpenFile`, `os.Remove`, `os.ReadDir`, `os.Stat`).\n  - Concurrency control using `sync.Mutex` for safe file access.\n  - Line-by-line processing of JSON events during replay using `bufio.Scanner`.\n",
  "internal/domain/log.go": "- File Path: `internal/domain/log.go`\n- High-Level Purpose: This file defines the `LogEvent` struct, which serves as the core data model for representing a single log event within the application's domain.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `LogEvent` (struct): Represents a single log event with various attributes.\n      - Fields:\n        - `ID` (string): Unique identifier for the log event.\n        - `ReceivedAt` (time.Time): Timestamp when the event was received by the system.\n        - `EventTime` (time.Time): Original timestamp of the event.\n        - `Source` (string): Origin of the log event (e.g., service name).\n        - `Level` (string): Log level (e.g., \"info\", \"error\").\n        - `Message` (string): The main log message.\n        - `Metadata` (json.RawMessage): Arbitrary JSON metadata associated with the event.\n        - `RawEvent` (json.RawMessage): The original raw payload of the event (not serialized for output).\n        - `PIIRedacted` (bool): Flag indicating if PII has been redacted from the event.\n        - `StreamMessageID` (string): Transient field for Redis Stream message ID (not serialized for output).\n- Notable Patterns or Logic:\n  - Domain model definition for log events.\n  - Use of `json.RawMessage` for flexible metadata storage.\n",
  "internal/domain/repository.go": "- File Path: `internal/domain/repository.go`\n- High-Level Purpose: This file defines the core interfaces for various repositories within the log ingestor application, establishing contracts for log event management, API key validation, and Write-Ahead Log operations.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `LogRepository` (interface): Defines methods for buffering, reading, writing, acknowledging, and moving log events to a DLQ.\n      - Methods:\n        - `BufferLog(ctx context.Context, event LogEvent) error`: Adds a single log event to a durable buffer.\n        - `ReadLogBatch(ctx context.Context, group, consumer string, count int) ([]LogEvent, error)`: Reads a batch of log events from the buffer for a consumer group.\n        - `WriteLogBatch(ctx context.Context, events []LogEvent) error`: Writes a batch of log events to final persistent storage.\n        - `AcknowledgeLogs(ctx context.Context, group string, messageIDs ...string) error`: Marks log events as processed in the buffer.\n        - `MoveToDLQ(ctx context.Context, events []LogEvent) error`: Moves events to a Dead-Letter Queue.\n    - `APIKeyRepository` (interface): Defines the contract for validating API keys.\n      - Methods:\n        - `IsValid(ctx context.Context, key string) (bool, error)`: Checks if an API key is valid.\n    - `WALRepository` (interface): Defines the contract for a Write-Ahead Log.\n      - Methods:\n        - `Write(ctx context.Context, event LogEvent) error`: Writes a single event to the WAL.\n        - `Replay(ctx context.Context, handler func(event LogEvent) error) error`: Reads all events from the WAL and processes them with a handler.\n        - `Truncate(ctx context.Context) error`: Removes all events/segments from the WAL.\n        - `Close() error`: Closes the WAL, ensuring resources are released.\n- Notable Patterns or Logic:\n  - Repository pattern for abstracting data access and persistence logic.\n  - Interface-driven design for loose coupling and testability.\n  - Defines the \"ports\" for the application's domain layer.\n",
  "internal/pkg/config/config.go": "- File Path: `internal/pkg/config/config.go`\n- High-Level Purpose: This file defines the application's configuration structure and provides a utility function to load these settings from environment variables, with support for `.env` files for local development.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `Config` (struct): Holds all application configuration parameters.\n      - Fields:\n        - `LogLevel` (string): Logging level (e.g., \"info\", \"debug\").\n        - `MaxEventSize` (int64): Maximum allowed size for an incoming log event (default 1MB).\n        - `WALPath` (string): Path for Write-Ahead Log files (default \"./wal\").\n        - `WALSegmentSize` (int64): Maximum size of a single WAL segment (default 100MB).\n        - `WALMaxDiskSize` (int64): Maximum total disk space for the WAL (default 1GB).\n        - `BackpressurePolicy` (string): Policy for handling backpressure (default \"block\").\n        - `RedisAddr` (string): Address of the Redis server (required).\n        - `RedisDLQStream` (string): Name of the Redis Dead-Letter Queue stream (default \"log_events_dlq\").\n        - `PostgresURL` (string): Connection URL for PostgreSQL (required).\n        - `APIKeyCacheTTL` (time.Duration): Time-to-live for API key cache entries (default 5m).\n        - `PIIRedactionFields` (string): Comma-separated list of fields to redact for PII (default \"email,password,credit_card,ssn\").\n        - `IngestServerAddr` (string): Address for the HTTP ingest server to listen on (default \":8080\").\n        - `ConsumerRetryCount` (int): Number of retries for consumer processing (default 3).\n        - `ConsumerRetryBackoff` (time.Duration): Initial backoff duration for consumer retries (default 1s).\n  - Functions:\n    - `Load() (*Config, error)`: Public function.\n      - Description: Loads configuration from environment variables. It first attempts to load a `.env` file (for local development) and then parses environment variables into the `Config` struct using `github.com/caarlos0/env`.\n- Notable Patterns or Logic:\n  - Centralized configuration management.\n  - Environment variable parsing using `github.com/caarlos0/env`.\n  - Support for `.env` files for development convenience using `github.com/joho/godotenv`.\n",
  "internal/pkg/logger/logger.go": "- File Path: `internal/pkg/logger/logger.go`\n- High-Level Purpose: This file provides a utility function to create and configure a structured logger using Go's `slog` package.\n- Definitions in the File:\n  - Functions:\n    - `New(level string) *slog.Logger`: Public function.\n      - Description: Creates and returns a new `slog.Logger` instance. It configures the logger to output JSON format to standard output and sets the logging level based on the provided string (e.g., \"debug\", \"info\", \"warn\", \"error\"). Defaults to \"info\" if an unknown level is provided.\n- Notable Patterns or Logic:\n  - Centralized logger initialization.\n  - Structured logging setup using `slog`.\n",
  "internal/usecase/ingest_log.go": "- File Path: `internal/usecase/ingest_log.go`\n- High-Level Purpose: This file defines the `IngestLogUseCase`, which encapsulates the business logic for processing and buffering log events, including enrichment, PII redaction, and interaction with the log repository.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `IngestLogUseCase` (struct): Handles the core business logic for ingesting a log event.\n      - Fields:\n        - `repo` (`domain.LogRepository`): An interface for buffering log events.\n        - `redactor` (`*pii.Redactor`): A component responsible for redacting PII from log events.\n        - `logger` (`*slog.Logger`): A structured logger instance.\n  - Functions:\n    - `NewIngestLogUseCase(repo domain.LogRepository, redactor *pii.Redactor, logger *slog.Logger) *IngestLogUseCase`: Public function.\n      - Description: Constructor for `IngestLogUseCase`, initializing it with the necessary repository, PII redactor, and logger.\n    - `(*IngestLogUseCase) Ingest(ctx context.Context, event *domain.LogEvent) error`: Public method.\n      - Description: Validates, enriches, redacts, and buffers a log event. It sets `ReceivedAt` and generates an `ID` if missing, then calls the `pii.Redactor` to redact sensitive information. Finally, it uses the `domain.LogRepository` to buffer the processed event. It logs warnings for redaction failures and errors for buffering failures.\n- Notable Patterns or Logic:\n  - Use Case pattern for encapsulating business logic.\n  - Dependency injection for external services (repository, redactor, logger).\n  - In-place modification of the `domain.LogEvent` object.\n",
  "internal/usecase/process_logs.go": "- File Path: `internal/usecase/process_logs.go`\n- High-Level Purpose: This file defines the `ProcessLogsUseCase`, which orchestrates the end-to-end processing of log events by reading them from a buffer, attempting to write them to a persistent sink with retries, and managing acknowledgments or moving failed events to a Dead-Letter Queue.\n- Definitions in the File:\n  - Constants:\n    - `defaultBatchSize` (int): 1000. The default number of log events to read in a single batch.\n  - Classes / Structs / Interfaces:\n    - `ProcessLogsUseCase` (struct): Orchestrates the processing of log events from buffer to sink.\n      - Fields:\n        - `bufferRepo` (`domain.LogRepository`): Repository for reading from the log buffer (e.g., Redis Stream).\n        - `sinkRepo` (`domain.LogRepository`): Repository for writing to the final persistent storage (e.g., PostgreSQL).\n        - `logger` (*slog.Logger): Structured logger.\n        - `group` (string): Consumer group name for the buffer.\n        - `consumer` (string): Unique consumer name.\n        - `retryCount` (int): Number of retries for writing to the sink.\n        - `retryBackoff` (time.Duration): Initial backoff duration for retries.\n  - Functions:\n    - `NewProcessLogsUseCase(bufferRepo, sinkRepo domain.LogRepository, logger *slog.Logger, group, consumer string, retryCount int, retryBackoff time.Duration) *ProcessLogsUseCase`: Public function.\n      - Description: Constructor for `ProcessLogsUseCase`, initializing it with buffer and sink repositories, logger, consumer group/name, and retry configuration.\n    - `(*ProcessLogsUseCase) ProcessBatch(ctx context.Context) (int, error)`: Public method.\n      - Description: Reads a batch of logs from the `bufferRepo`. It then attempts to write these logs to the `sinkRepo` using a retry mechanism. If writing fails after all retries, it moves the events to a Dead-Letter Queue via the `bufferRepo`. Finally, it acknowledges successfully processed or DLQ'd events in the `bufferRepo`.\n    - `(*ProcessLogsUseCase) writeWithRetry(ctx context.Context, events []domain.LogEvent) error`: Internal method.\n      - Description: Attempts to write a batch of log events to the `sinkRepo` with a configurable number of retries and exponential backoff between attempts.\n- Notable Patterns or Logic:\n  - Use Case pattern for orchestrating complex business logic.\n  - Dependency injection for buffer and sink repositories.\n  - Retry mechanism with exponential backoff for resilient writes to the sink.\n  - Dead-Letter Queue (DLQ) integration for handling persistently failing events.\n  - Consumer group pattern for distributed log processing.\n"
}