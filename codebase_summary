{
  "CHAOS.md": "- File Path: `CHAOS.md`\n- High-Level Purpose: This Markdown document outlines a comprehensive checklist for conducting chaos engineering experiments to validate the resilience and durability of the `log-ingestor` application under various simulated failure conditions.\n- Definitions in the File:\n  - Sections:\n    - `1. Redis Outage (Buffer Unavailability)`:\n      - Goal: Verify WAL fallback and no data loss during Redis failure.\n      - Procedure: Stop/start Redis during load, observe ingestor logs, `wal_active` metric, WAL segment files, and PostgreSQL data after recovery.\n    - `2. PostgreSQL Outage (Sink Unavailability)`:\n      - Goal: Verify consumer graceful handling of sink failures, retries, and Dead-Letter Queue (DLQ) usage.\n      - Procedure: Stop/start PostgreSQL during load, observe consumer logs, Redis pending messages, and DLQ stream.\n    - `3. Consumer Worker Crash`:\n      - Goal: Verify Redis consumer group reassigns pending messages from a crashed consumer.\n      - Procedure: Run multiple consumers, kill one during load, observe pending messages, `XCLAIM` by healthy consumer, and final data in PostgreSQL.\n    - `4. Network Partition / Latency`:\n      - Goal: Simulate network degradation to observe service behavior.\n      - Procedure: Introduce latency between services (ingestor-Redis, consumer-Redis, consumer-PostgreSQL) using `tc` or Docker network settings.\n      - Observations: Monitor ingestion latency, WAL fallback, consumer processing times, and Redis stream backlog.\n- Notable Patterns or Logic:\n  - Chaos Engineering principles applied to a distributed system.\n  - Structured documentation of test goals, procedures, and expected observations.\n  - Focus on resilience, durability, and fault tolerance.\n  - Use of specific tools/commands (`docker-compose`, `redis-cli`, `tc`, Prometheus metrics) for observation.\n",
  "Dockerfile.consumer": "- File Path: `Dockerfile.consumer`\n- High-Level Purpose: This Dockerfile defines a multi-stage build process to create a lightweight Docker image for the `consumer` Go application, which processes log events.\n- Definitions in the File:\n  - Stages:\n    - `builder` (stage): Uses `golang:1.21-alpine` as the base image.\n      - Description: Installs build dependencies (`git`), copies Go module files (`go.mod`, `go.sum`) to download dependencies, copies the application source code, and builds the `consumer` binary with static linking and optimizations.\n    - Final stage: Uses `alpine:latest` as the base image.\n      - Description: Copies the compiled `consumer` binary from the `builder` stage into the final image and sets it as the container's entrypoint.\n- Notable Patterns or Logic:\n  - Multi-stage Docker build for creating small, efficient production images.\n  - Alpine Linux base image for minimal footprint.\n  - Static compilation of Go binary (`CGO_ENABLED=0 GOOS=linux`).\n  - Optimized build flags (`-ldflags=\"-w -s\"`) to reduce binary size.\n",
  "Dockerfile.ingest": "- File Path: `Dockerfile.ingest`\n- High-Level Purpose: This Dockerfile defines a multi-stage build process to create a lightweight Docker image for the `ingest` Go application, which handles log ingestion and serves the frontend.\n- Definitions in the File:\n  - Stages:\n    - `builder` (stage): Uses `golang:1.21-alpine` as the base image.\n      - Description: Installs build dependencies (`git`), copies Go module files (`go.mod`, `go.sum`) to download dependencies, copies the application source code, and builds the `ingest` binary with static linking and optimizations.\n    - Final stage: Uses `alpine:latest` as the base image.\n      - Description: Copies the compiled `ingest` binary from the `builder` stage, copies the `frontend` static files, exposes ports 8080 and 9091, and sets the `ingest` binary as the container's entrypoint.\n- Notable Patterns or Logic:\n  - Multi-stage Docker build for creating small, efficient production images.\n  - Alpine Linux base image for minimal footprint.\n  - Static compilation of Go binary (`CGO_ENABLED=0 GOOS=linux`).\n  - Optimized build flags (`-ldflags=\"-w -s\"`) to reduce binary size.\n  - Includes static frontend assets directly in the final image.",
  "Makefile": "- File Path: `Makefile`\n- High-Level Purpose: This file defines a set of commands (targets) for building, testing, linting, cleaning, and managing Docker images and Docker Compose services for the `log-ingestor` application.\n- Definitions in the File:\n  - Constants:\n    - `APP_NAME` (string): \"log-ingestor\". The base name for the application.\n    - `INGEST_IMG_NAME` (string): \"log-ingestor-ingest\". Docker image name for the ingest service.\n    - `CONSUMER_IMG_NAME` (string): \"log-ingestor-consumer\". Docker image name for the consumer service.\n    - `TAG` (string): \"latest\" (default). Docker image tag.\n    - `REGISTRY` (string): \"your-registry\" (default). Docker container registry.\n  - Functions (Targets):\n    - `all`: Default target, runs `build`.\n      - Description: Builds the Go binaries.\n    - `build`: Public target.\n      - Description: Compiles the `ingest` and `consumer` Go binaries into the `bin/` directory.\n    - `test`: Public target.\n      - Description: Runs all Go unit tests with verbose output, race detection, and coverage reporting.\n    - `lint`: Public target.\n      - Description: Executes `golangci-lint` to analyze the Go codebase for potential issues.\n    - `clean`: Public target.\n      - Description: Removes generated build artifacts, specifically the `bin` directory and `coverage.out` file.\n    - `docker-build`: Public target.\n      - Description: Builds Docker images for both the `ingest` and `consumer` services using their respective Dockerfiles.\n    - `docker-push`: Public target.\n      - Description: Tags the locally built Docker images with a registry prefix and pushes them to the configured container registry.\n    - `compose-up`: Public target.\n      - Description: Starts all services defined in `docker-compose.yml` in detached mode, rebuilding images if necessary.\n    - `compose-down`: Public target.\n      - Description: Stops and removes all services, networks, and volumes created by Docker Compose.\n    - `compose-logs`: Public target.\n      - Description: Streams the logs from all services managed by Docker Compose.\n    - `help`: Public target.\n      - Description: Displays a formatted list of all available Makefile targets and their descriptions.\n- Notable Patterns or Logic:\n  - Standard Makefile structure for project automation.\n  - `PHONY` targets to ensure commands are always run.\n  - Self-documenting `help` target using `grep` and `awk`.\n  - Separation of concerns into Go, Docker, and Docker Compose command groups.\n",
  "cmd/consumer/main.go": "- File Path: `cmd/consumer/main.go`\n- High-Level Purpose: This is the main entry point for the log processing consumer application. It initializes configuration, sets up logging, connects to Redis and PostgreSQL, creates the necessary repositories and use cases, and starts a loop to continuously process log batches from Redis Streams and write them to PostgreSQL.\n- Definitions in the File:\n  - Constants:\n    - `consumerGroup` (string): \"log-processors\". The name of the Redis consumer group this consumer belongs to.\n    - `processingInterval` (time.Duration): 1 * time.Second. The interval at which the consumer attempts to process a batch of logs.\n  - Functions:\n    - `main()`: Main function.\n      - Description:\n        1. Loads application configuration using `config.Load()`.\n        2. Initializes a structured logger using `logger.New()` and generates a unique consumer name.\n        3. Establishes connections to PostgreSQL (`database/sql`) and Redis (`github.com/redis/go-redis/v9`).\n        4. Creates `redisrepo.NewLogRepository` (as the buffer, without a WAL) and `postgres.NewLogRepository` (as the sink).\n        5. Instantiates `usecase.NewProcessLogsUseCase` with the configured repositories and retry settings.\n        6. Sets up a graceful shutdown mechanism using `context` and `os.Signal` (SIGINT, SIGTERM).\n        7. Enters a continuous loop, processing log batches at `processingInterval` until a shutdown signal is received or the context is canceled.\n- Notable Patterns or Logic:\n  - Application entry point (`main` function).\n  - Dependency injection for services (repositories, use cases).\n  - Graceful shutdown handling using `context` and `os.Signal`.\n  - Continuous polling loop for processing messages from a queue (Redis Stream).\n  - Separation of concerns: `main` orchestrates components, business logic is in `usecase`, data access in `repository`.\n",
  "cmd/ingest/main.go": "- File Path: `cmd/ingest/main.go`\n- High-Level Purpose: This is the main entry point for the log ingestor service. It orchestrates the entire application startup, including loading configuration, initializing logging, establishing database and Redis connections, setting up various repositories (PostgreSQL, Redis, WAL), creating use cases and API handlers, configuring HTTP servers for both log ingestion and administrative/metrics endpoints, and managing graceful shutdown.\n- Definitions in the File:\n  - Functions:\n    - `main()`: Main function.\n      - Description:\n        1.  Loads application configuration using `config.Load()`.\n        2.  Initializes a structured logger using `logger.New()` and sets it as the default.\n        3.  Initializes Prometheus metrics using `metrics.NewIngestMetrics()`.\n        4.  Sets up and starts an `adminServer` on port `:9091` for Prometheus metrics and administrative API endpoints.\n        5.  Establishes a `context` for graceful shutdown, listening for `SIGINT` and `SIGTERM`.\n        6.  Connects to PostgreSQL (`database/sql`) and Redis (`github.com/redis/go-redis/v9`), handling potential Redis unavailability by logging a warning.\n        7.  Initializes `wal.NewWALRepository` for durable log storage during Redis outages.\n        8.  Initializes `postgres.NewAPIKeyRepository` for API key validation with caching.\n        9.  Initializes `redisrepo.NewLogRepository` for buffering logs to Redis Streams, integrating the WAL for failover.\n        10. Starts a background goroutine for Redis health checks and WAL replay (`redisLogRepo.StartHealthCheck`).\n        11. Initializes `redisrepo.NewAdminRepository` and `usecase.NewAdminStreamUseCase` to provide administrative functionality, mounting the `api.NewAdminRouter` on the admin server.\n        12. Initializes `pii.NewRedactor` with configured fields for PII redaction.\n        13. Initializes `usecase.NewIngestLogUseCase` with the Redis log repository, PII redactor, and logger.\n        14. Initializes `handler.NewSSEBroker` for real-time event rate updates.\n        15. Configures and starts the main `ingestServer` on `cfg.IngestServerAddr` with the `api.NewRouter`, applying a logging middleware.\n        16. Waits for a shutdown signal, then gracefully shuts down both the admin and ingest HTTP servers within a timeout.\n- Notable Patterns or Logic:\n  - Application entry point (`main` function).\n  - Dependency Injection for all major components (repositories, use cases, handlers, metrics, logger).\n  - Graceful shutdown handling using `context` and `os.Signal`.\n  - Dual HTTP servers: one for ingestion, one for admin/metrics.\n  - Observability integration with Prometheus (`promhttp.Handler`).\n  - Write-Ahead Log (WAL) as a failover mechanism for Redis.\n  - Centralized configuration loading.\n  - Structured logging.\n  - Use of `_ \"github.com/lib/pq\"` for PostgreSQL driver registration.",
  "docker-compose.yml": "- File Path: `docker-compose.yml`\n- High-Level Purpose: This Docker Compose file defines and configures a multi-service application environment for the `log-ingestor` project, including PostgreSQL, Redis, the ingestor service, and the consumer service, along with their dependencies, ports, volumes, and health checks.\n- Definitions in the File:\n  - Services:\n    - `postgres`: PostgreSQL database service.\n      - Image: `postgres:16-alpine`.\n      - Environment: Configures database name, user, and password (with defaults).\n      - Ports: Exposes port 5432.\n      - Volumes: Mounts a named volume for persistent data and an initialization script.\n      - Healthcheck: Checks if PostgreSQL is ready.\n      - Restart policy: `unless-stopped`.\n    - `redis`: Redis in-memory data store service.\n      - Image: `redis:7-alpine`.\n      - Command: Starts Redis with AOF persistence enabled.\n      - Ports: Exposes port 6379.\n      - Volumes: Mounts a named volume for persistent data.\n      - Healthcheck: Checks if Redis is responsive.\n      - Restart policy: `unless-stopped`.\n    - `ingestor`: The log ingestion service.\n      - Build: Uses `Dockerfile.ingest` from the current context.\n      - Ports: Exposes 8080 (Ingest API) and 9091 (Admin/Metrics API).\n      - Environment: Loads variables from a `.env` file.\n      - Volumes: Mounts the local `./wal` directory for Write-Ahead Log persistence.\n      - Dependencies: Requires `postgres` and `redis` to be healthy.\n      - Restart policy: `unless-stopped`.\n    - `consumer`: The log processing consumer service.\n      - Build: Uses `Dockerfile.consumer` from the current context.\n      - Deployment: Configured to run 2 replicas.\n      - Environment: Loads variables from a `.env` file.\n      - Dependencies: Requires `postgres` and `redis` to be healthy.\n      - Restart policy: `unless-stopped`.\n  - Volumes:\n    - `postgres-data`: Named volume for PostgreSQL data persistence.\n    - `redis-data`: Named volume for Redis data persistence.\n- Notable Patterns or Logic:\n  - Docker Compose for defining and running multi-container applications.\n  - Service health checks to ensure dependencies are ready before dependent services start.\n  - Named volumes for persistent data storage.\n  - `env_file` for externalizing environment variables.\n  - `depends_on` with `service_healthy` condition for robust startup order.\n  - Example of service scaling (`deploy.replicas`).\n",
  "frontend/css/main.css": "- File Path: `frontend/css/main.css`\n- High-Level Purpose: This CSS file defines the visual styling for the Log Ingestor frontend application, covering global styles, layout, typography, color scheme, and specific visual elements like status indicators and the sparkline graph.\n- Definitions in the File:\n  - CSS Variables (defined in `:root`):\n    - `--bg-color`: Main background color.\n    - `--card-bg-color`: Background color for metric cards.\n    - `--text-color`: Default text color.\n    - `--primary-color`: Accent color for key elements.\n    - `--green`, `--yellow`, `--red`: Colors for status indicators (connected, reconnecting, error).\n    - `--border-color`: Color for borders and separators.\n  - Styles (selectors and properties):\n    - `body`: Sets font family, background, text color, margin, padding, and uses flexbox for centering content.\n    - `.container`: Defines maximum width and centers the main content area.\n    - `header`: Styles the header layout using flexbox, adds a bottom border, and padding.\n    - `h1`: Styles the main title with the primary color.\n    - `.status`: Styles the layout of the status indicator and text using flexbox.\n    - `#status-indicator`: Styles the circular status light, including size, border-radius, and transition.\n    - `#status-indicator.connected`, `#status-indicator.reconnecting`, `#status-indicator.error`: Define specific background colors for different connection states.\n    - `.metric-card`: Styles the container for individual metric displays, including background, border, border-radius, and padding.\n    - `.metric-card h2`: Styles the title within a metric card.\n    - `.metric-card p`: Styles the paragraph displaying the metric value.\n    - `#ingest-rate`: Styles the live ingest rate value with bold text and the primary color.\n    - `#sparkline-container`: Styles the container for the sparkline graph, including height, background, padding, and border.\n    - `#sparkline`: Styles the sparkline graph area itself, using flexbox to arrange bars at the bottom and justify them to the end.\n    - `.sparkline-bar`: Styles individual bars within the sparkline, defining their flexible width, background color, opacity, and a height transition.\n- Notable Patterns or Logic:\n  - Use of CSS Custom Properties (`--variable-name`) for a maintainable and easily customizable color scheme.\n  - Flexbox layout (`display: flex`) for responsive and organized arrangement of elements (e.g., `body`, `header`, `#sparkline`).\n  - Semantic class and ID naming conventions for clarity.\n  - State-based styling for the status indicator (`.connected`, `.reconnecting`, `.error`).",
  "frontend/index.html": "- File Path: `frontend/index.html`\n- High-Level Purpose: This file provides the main HTML structure for the Log Ingestor's live metrics dashboard, including the page title, links to stylesheets and JavaScript files, and placeholders for displaying the ingest rate and connection status.\n- Definitions in the File:\n  - HTML Structure:\n    - `\u003c!DOCTYPE html\u003e`: Standard HTML5 declaration.\n    - `\u003chtml lang=\"en\"\u003e`: Root element.\n    - `\u003chead\u003e`: Contains metadata, character set, viewport settings, page title (\"Log Ingestor - Live Metrics\"), and a link to `css/main.css`.\n    - `\u003cbody\u003e`: Contains the visible content of the page.\n      - `\u003cdiv class=\"container\"\u003e`: Main content wrapper.\n      - `\u003cheader\u003e`: Contains the main title (`\u003ch1\u003eLog Ingestor Metrics\u003c/h1\u003e`) and a status display.\n        - `\u003cdiv class=\"status\"\u003e`: Displays connection status.\n          - `\u003cdiv id=\"status-indicator\"\u003e`: A circular indicator for connection status.\n          - `\u003cspan id=\"status-text\"\u003e`: Text description of the connection status.\n      - `\u003cmain\u003e`: Contains the primary metrics display.\n        - `\u003cdiv class=\"metric-card\"\u003e`: A card for displaying the live ingest rate.\n          - `\u003ch2\u003eLive Ingest Rate\u003c/h2\u003e`: Title for the metric.\n          - `\u003cp\u003e\u003cspan id=\"ingest-rate\"\u003e`: Displays the numerical ingest rate.\n          - `\u003cdiv id=\"sparkline-container\"\u003e`: Container for the sparkline graph.\n            - `\u003cdiv id=\"sparkline\"\u003e`: The actual sparkline graph area.\n    - JavaScript Imports:\n      - `\u003cscript src=\"js/ui.js\"\u003e\u003c/script\u003e`: Imports the UI utility script.\n      - `\u003cscript src=\"js/sse_client.js\"\u003e\u003c/script\u003e`: Imports the Server-Sent Events client script.\n      - `\u003cscript src=\"js/app.js\"\u003e\u003c/script\u003e`: Imports the main application logic script.\n- Notable Patterns or Logic:\n  - Standard HTML5 document structure.\n  - Semantic HTML elements for content organization.\n  - External resource linking for CSS and JavaScript.\n  - Use of IDs for JavaScript to target specific DOM elements for dynamic updates.\n",
  "frontend/js/app.js": "- File Path: `frontend/js/app.js`\n- High-Level Purpose: This is the main client-side application logic. It initializes the UI, manages the data points for the sparkline, and orchestrates the connection to the Server-Sent Events (SSE) endpoint to receive and display live ingest rate metrics.\n- Definitions in the File:\n  - Constants:\n    - `MAX_SPARKLINE_POINTS` (number): 60. Specifies the maximum number of historical data points to retain for the sparkline graph (representing the last 60 seconds).\n  - Variables:\n    - `dataPoints` (array): An array to store the historical ingest rate values for the sparkline.\n  - Functions:\n    - `handleNewData(data)`: Internal function.\n      - Description: Callback function invoked when new data is received from the SSE stream. It extracts the `Rate` from the incoming data, adds it to the `dataPoints` array (maintaining `MAX_SPARKLINE_POINTS` by shifting older entries), and then calls `ui.updateIngestRate` and `ui.drawSparkline` to refresh the display.\n    - `handleStatusChange(status, message)`: Internal function.\n      - Description: Callback function invoked when the SSE connection status changes. It delegates to `ui.updateStatus` to update the visual status indicator and text on the page.\n- Notable Patterns or Logic:\n  - Immediately Invoked Function Expression (IIFE) to encapsulate the application's scope and prevent global variable pollution.\n  - Event listener for `DOMContentLoaded` to ensure the script runs after the HTML is fully loaded.\n  - Integration with `ui.js` for UI updates and `sse_client.js` for SSE connectivity.\n  - Data buffering and shifting (`dataPoints.push`, `dataPoints.shift`) to maintain a fixed-size window of historical data for the sparkline.\n",
  "frontend/js/sse_client.js": "- File Path: `frontend/js/sse_client.js`\n- High-Level Purpose: This JavaScript file provides a utility object (`sseClient`) for establishing and managing a Server-Sent Events (SSE) connection. It handles opening the connection, parsing incoming messages, and reporting connection status changes via callbacks.\n- Definitions in the File:\n  - Object: `sseClient`\n    - Functions (methods on `sseClient` object):\n      - `connect(url, { onData, onStatusChange })`: Public method.\n        - Description: Initiates an `EventSource` connection to the specified `url`. It registers event handlers for:\n          - `onopen`: Called when the connection is successfully established, invoking the `onStatusChange` callback with 'connected' status.\n          - `onmessage`: Called when a new message is received. It attempts to parse the message data as JSON and then invokes the `onData` callback with the parsed object. Logs an error if JSON parsing fails.\n          - `onerror`: Called when the connection encounters an error or is lost. It invokes the `onStatusChange` callback with 'reconnecting' status, as `EventSource` automatically attempts to reconnect.\n        - Returns: The `EventSource` instance, allowing for manual closing if needed.\n- Notable Patterns or Logic:\n  - Module pattern (implicit global object `sseClient`) for organizing SSE-related functionality.\n  - Use of the browser's native `EventSource` API for Server-Sent Events.\n  - Callback pattern for handling asynchronous events (data reception, status changes).\n  - Automatic reconnection handling provided by the `EventSource` API.\n",
  "frontend/js/ui.js": "- File Path: `frontend/js/ui.js`\n- High-Level Purpose: This JavaScript file defines a global `ui` object that encapsulates functions for updating various elements of the web page's user interface, such as the ingest rate display, connection status, and a sparkline graph.\n- Definitions in the File:\n  - Object: `ui`\n    - Properties:\n      - `elements` (object): Stores references to key DOM elements:\n        - `ingestRate` (HTMLElement): The element displaying the numerical ingest rate.\n        - `statusIndicator` (HTMLElement): The circular status indicator.\n        - `statusText` (HTMLElement): The text describing the connection status.\n        - `sparkline` (HTMLElement): The container for the sparkline graph.\n    - Functions (methods on `ui` object):\n      - `updateIngestRate(rate)`: Public method.\n        - Description: Updates the `ingestRate` element's text content with the provided `rate`, formatted to two decimal places.\n      - `updateStatus(status, message)`: Public method.\n        - Description: Updates the `statusIndicator`'s CSS class (e.g., 'connected', 'reconnecting', 'error') and the `statusText`'s content to reflect the current connection status.\n      - `drawSparkline(dataPoints, maxPoints)`: Public method.\n        - Description: Clears the existing sparkline and redraws it using a new set of `dataPoints`. It pads the data with zeros if fewer points are available than `maxPoints`, calculates bar heights relative to the maximum value in the dataset, and ensures a minimum visible height for bars.\n- Notable Patterns or Logic:\n  - Module pattern (implicit global object `ui`) for organizing UI-related functions.\n  - Direct DOM manipulation for updating content and styles.\n  - Simple data visualization logic for a sparkline graph.\n",
  "go.mod": "- File Path: `go.mod`\n- High-Level Purpose: This file defines the Go module path for the `log-ingestor` project and lists all direct and indirect external dependencies required for the application.\n- Definitions in the File:\n  - Module: `github.com/user/log-ingestor`\n  - Go Version: `1.21`\n  - Direct Dependencies:\n    - `github.com/caarlos0/env/v10 v10.0.0`: For loading environment variables into structs.\n    - `github.com/google/uuid v1.6.0`: For generating UUIDs.\n    - `github.com/joho/godotenv v1.5.1`: For loading `.env` files.\n    - `github.com/lib/pq v1.10.9`: PostgreSQL driver.\n    - `github.com/prometheus/client_golang v1.19.0`: Prometheus client library for Go applications.\n    - `github.com/redis/go-redis/v9 v9.5.1`: Redis client for Go.\n  - Indirect Dependencies: (Various transitive dependencies for Prometheus and Redis clients)\n- Notable Patterns or Logic:\n  - Standard Go module definition.\n  - Explicit versioning for dependencies.\n",
  "internal/adapter/api/admin_router.go": "- File Path: `internal/adapter/api/admin_router.go`\n- High-Level Purpose: This file is responsible for creating and configuring the HTTP router for administrative operations related to Redis Streams. It defines various API endpoints for querying stream information, managing pending messages, and performing stream maintenance.\n- Definitions in the File:\n  - Functions:\n    - `NewAdminRouter(adminUseCase *usecase.AdminStreamUseCase, logger *slog.Logger) http.Handler`: Public function.\n      - Description: Creates an `http.ServeMux` and configures HTTP routes for administrative tasks. It registers handlers for getting group/consumer info, pending message summaries/details, claiming messages, acknowledging messages, and trimming streams. It leverages Go 1.22+ path patterns for routing.\n- Notable Patterns or Logic:\n  - HTTP Router configuration using `http.NewServeMux`.\n  - Go 1.22+ HTTP path patterns (e.g., `/{streamName}/`).\n  - Dependency Injection for the `AdminStreamUseCase` and `slog.Logger`.\n  - Clear separation of concerns between routing and handler logic.\n",
  "internal/adapter/api/handler/admin_handler.go": "- File Path: `internal/adapter/api/handler/admin_handler.go`\n- High-Level Purpose: This file defines an HTTP handler for administrative API endpoints related to Redis Streams. It parses request parameters, delegates the actual business logic to an `AdminStreamUseCase`, and formats responses as JSON.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `AdminHandler` (struct): Handles HTTP requests for stream administration.\n      - Fields:\n        - `uc` (*usecase.AdminStreamUseCase): The use case component for administrative logic.\n        - `logger` (*slog.Logger): Structured logger.\n  - Functions:\n    - `NewAdminHandler(uc *usecase.AdminStreamUseCase, logger *slog.Logger) *AdminHandler`: Public function.\n      - Description: Constructor for `AdminHandler`, initializing it with the admin stream use case and logger.\n    - `(*AdminHandler) HealthCheck(w http.ResponseWriter, r *http.Request)`: Public method.\n      - Description: A simple health check endpoint that returns \"ok\" with a 200 status.\n    - `(*AdminHandler) GetGroupInfo(w http.ResponseWriter, r *http.Request)`: Public method.\n      - Description: Handles `GET /admin/streams/{streamName}/groups` requests, retrieves consumer group information from the use case, and responds with JSON.\n    - `(*AdminHandler) GetConsumerInfo(w http.ResponseWriter, r *http.Request)`: Public method.\n      - Description: Handles `GET /admin/streams/{streamName}/groups/{groupName}/consumers` requests, retrieves consumer information from the use case, and responds with JSON.\n    - `(*AdminHandler) GetPendingSummary(w http.ResponseWriter, r *http.Request)`: Public method.\n      - Description: Handles `GET /admin/streams/{streamName}/groups/{groupName}/pending` requests, retrieves a summary of pending messages from the use case, and responds with JSON.\n    - `(*AdminHandler) GetPendingMessages(w http.ResponseWriter, r *http.Request)`: Public method.\n      - Description: Handles `GET /admin/streams/{streamName}/groups/{groupName}/pending/messages` requests, parses query parameters for consumer, start ID, and count, retrieves detailed pending messages from the use case, and responds with JSON.\n    - `(*AdminHandler) ClaimMessages(w http.ResponseWriter, r *http.Request)`: Public method.\n      - Description: Handles `POST /admin/streams/{streamName}/groups/{groupName}/claim` requests, decodes a JSON payload containing consumer, min idle time, and message IDs, delegates to the use case to claim messages, and responds with the claimed events.\n    - `(*AdminHandler) AcknowledgeMessages(w http.ResponseWriter, r *http.Request)`: Public method.\n      - Description: Handles `POST /admin/streams/{streamName}/groups/{groupName}/ack` requests, decodes a JSON payload with message IDs, delegates to the use case to acknowledge messages, and responds with the count of acknowledged messages.\n    - `(*AdminHandler) TrimStream(w http.ResponseWriter, r *http.Request)`: Public method.\n      - Description: Handles `POST /admin/streams/{streamName}/trim` requests, decodes a JSON payload with `maxlen`, delegates to the use case to trim the stream, and responds with the count of trimmed messages.\n    - `(*AdminHandler) respondWithJSON(w http.ResponseWriter, code int, payload interface{})`: Internal method.\n      - Description: A helper method to marshal a payload to JSON, set the `Content-Type` header, write the HTTP status code, and send the response.\n- Notable Patterns or Logic:\n  - HTTP Handler pattern.\n  - Dependency Injection for `AdminStreamUseCase` and `slog.Logger`.\n  - Request parameter parsing from `r.PathValue` (Go 1.22+) and `r.URL.Query()`.\n  - JSON request body decoding and JSON response encoding.\n  - Centralized error logging and HTTP error responses.\n",
  "internal/adapter/api/handler/ingest_handler.go": "- File Path: `internal/adapter/api/handler/ingest_handler.go`\n- High-Level Purpose: This file defines an HTTP handler responsible for receiving and processing incoming log ingestion requests. It supports both single JSON objects and Newline Delimited JSON (NDJSON) streams, enforces maximum event size, and delegates the core ingestion logic to an `IngestLogUseCase`. It also reports accepted events to an SSE broker for real-time monitoring.\n- Definitions in the File:\n  - Constants:\n    - `contentTypeJSON` (string): \"application/json\".\n    - `contentTypeNDJSON` (string): \"application/x-ndjson\".\n  - Classes / Structs / Interfaces:\n    - `IngestHandler` (struct): Handles HTTP requests for log ingestion.\n      - Fields:\n        - `useCase` (*usecase.IngestLogUseCase): The business logic component for ingesting logs.\n        - `logger` (*slog.Logger): Structured logger.\n        - `maxEventSize` (int64): Configurable maximum allowed size for an individual log event payload.\n        - `metrics` (*metrics.IngestMetrics): Prometheus metrics collector for ingestion statistics.\n        - `sseBroker` (*SSEBroker): A Server-Sent Events broker to report processed event counts.\n  - Functions:\n    - `NewIngestHandler(uc *usecase.IngestLogUseCase, logger *slog.Logger, maxEventSize int64, m *metrics.IngestMetrics, sse *SSEBroker) *IngestHandler`: Public function.\n      - Description: Constructor for `IngestHandler`. Initializes the handler with the ingest use case, logger, maximum event size, metrics, and SSE broker.\n    - `(*IngestHandler) ServeHTTP(w http.ResponseWriter, r *http.Request)`: Public method (implements `http.Handler`).\n      - Description: The main entry point for HTTP requests. It validates the request method (must be POST), enforces `maxEventSize`, determines the content type (`application/json` or `application/x-ndjson`), and dispatches to the appropriate internal handler. It also records various ingestion metrics.\n    - `(*IngestHandler) handleSingleJSON(ctx context.Context, body io.Reader) error`: Internal method.\n      - Description: Reads the entire request body, unmarshals it as a single JSON `LogEvent`, enriches it with the raw payload, and passes it to the `IngestLogUseCase`. Reports a single event to the SSE broker.\n    - `(*IngestHandler) handleNDJSON(ctx context.Context, body io.Reader) error`: Internal method.\n      - Description: Uses a `bufio.Scanner` to read the request body line by line, treating each line as a separate JSON `LogEvent`. Each event is unmarshaled, enriched, and passed to the `IngestLogUseCase`. Reports the total count of processed events to the SSE broker.\n- Notable Patterns or Logic:\n  - HTTP Handler pattern (`http.Handler` interface).\n  - Content-Type based request dispatching.\n  - Request body size limiting using `http.MaxBytesReader`.\n  - Handling of both single JSON and Newline Delimited JSON (NDJSON) formats.\n  - Dependency injection for `IngestLogUseCase`, `slog.Logger`, `metrics.IngestMetrics`, and `SSEBroker`.\n  - Error handling with specific HTTP status codes (e.g., `405 Method Not Allowed`, `415 Unsupported Media Type`, `413 Request Entity Too Large`, `400 Bad Request`).\n",
  "internal/adapter/api/handler/ingest_handler_test.go": "- File Path: `internal/adapter/api/handler/ingest_handler_test.go`\n- High-Level Purpose: This file contains unit tests for the `IngestHandler`, which is responsible for handling HTTP log ingestion requests. It covers various scenarios including valid JSON and NDJSON inputs, invalid HTTP methods, unsupported content types, malformed payloads, use case errors, and payload size limits.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `MockIngestUseCase` (struct): A mock implementation of `usecase.IngestLogUseCase` for testing.\n      - Fields:\n        - `IngestFunc` (func(ctx context.Context, event *domain.LogEvent) error): A function that can be set to control the behavior of the `Ingest` method.\n    - `tests` (struct): Defines a test case structure for the `IngestHandler`.\n      - Fields: `name`, `method`, `contentType`, `body`, `mockIngestErr`, `expectedStatus`, `expectedBody`.\n  - Functions:\n    - `(*MockIngestUseCase) Ingest(ctx context.Context, event *domain.LogEvent) error`: Public method.\n      - Description: Executes the `IngestFunc` if it's set, otherwise returns `nil`.\n    - `TestIngestHandler(t *testing.T)`: Public function.\n      - Description: A test suite for the `ServeHTTP` method of `IngestHandler`. It iterates through a slice of `tests` structs, running each as a sub-test. For each test case, it sets up a mock `IngestLogUseCase`, creates an `IngestHandler`, constructs an `httptest.NewRequest`, and then asserts the HTTP status code and response body against the expected values. It specifically tests `http.MaxBytesReader` for the \"Payload Too Large\" scenario.\n- Notable Patterns or Logic:\n  - Unit testing using Go's `testing` package.\n  - HTTP handler testing using `net/http/httptest`.\n  - Table-driven tests for comprehensive coverage of different HTTP request scenarios.\n  - Mocking dependencies (`MockIngestUseCase`, `metrics.NewIngestMetrics`, `NewSSEBroker`) to isolate handler logic.\n  - Testing various HTTP error conditions (e.g., `405 Method Not Allowed`, `415 Unsupported Media Type`, `400 Bad Request`, `413 Request Entity Too Large`, `500 Internal Server Error`).",
  "internal/adapter/api/handler/sse_handler.go": "- File Path: `internal/adapter/api/handler/sse_handler.go`\n- High-Level Purpose: This file implements a Server-Sent Events (SSE) broker that allows clients to subscribe to a real-time stream of event processing rates. It aggregates event counts reported by other components and broadcasts the calculated rate periodically.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `SSEMessage` (struct): Defines the JSON structure for messages sent over the SSE stream.\n      - Fields:\n        - `Rate` (float64): The calculated event processing rate (events per second).\n    - `SSEBroker` (struct): Manages SSE client connections and broadcasts rate messages.\n      - Fields:\n        - `logger` (*slog.Logger): Structured logger.\n        - `clients` (map[chan []byte]struct{}): A map to keep track of all active SSE client channels.\n        - `mu` (sync.RWMutex): A read-write mutex to protect concurrent access to the `clients` map.\n        - `eventCounter` (chan int): A buffered channel used to receive event counts from other parts of the application.\n  - Functions:\n    - `NewSSEBroker(ctx context.Context, logger *slog.Logger) *SSEBroker`: Public function.\n      - Description: Constructor for `SSEBroker`. Initializes the broker with a logger and starts a background goroutine (`run`) to handle event aggregation and broadcasting.\n    - `(*SSEBroker) ServeHTTP(w http.ResponseWriter, r *http.Request)`: Public method (implements `http.Handler`).\n      - Description: Handles new HTTP requests for SSE connections. It sets appropriate HTTP headers for SSE, creates a new channel for the client, adds it to the broker's client list, and then continuously sends messages from that channel to the client until the connection is closed or the context is canceled.\n    - `(*SSEBroker) ReportEvents(count int)`: Public method.\n      - Description: Allows other components (e.g., `IngestHandler`) to report a number of processed events to the broker. It sends the count to the `eventCounter` channel, dropping the report if the channel is full to avoid blocking.\n    - `(*SSEBroker) addClient(client chan []byte)`: Internal method.\n      - Description: Adds a new client's message channel to the broker's list of active clients, protected by a write lock.\n    - `(*SSEBroker) removeClient(client chan []byte)`: Internal method.\n      - Description: Removes a client's message channel from the broker's list and closes the channel, protected by a write lock.\n    - `(*SSEBroker) broadcast(msg []byte)`: Internal method.\n      - Description: Sends a given message (as a byte slice) to all currently connected SSE clients. It uses a read lock and non-blocking sends to avoid being stalled by slow clients.\n    - `(*SSEBroker) run(ctx context.Context)`: Internal method.\n      - Description: The main event loop for the broker. It runs in a separate goroutine, aggregates event counts received via `eventCounter` over a 1-second interval, calculates the events-per-second rate, marshals it into an `SSEMessage`, and then broadcasts this message to all connected clients.\n- Notable Patterns or Logic:\n  - Server-Sent Events (SSE) implementation for real-time updates.\n  - Publisher-Subscriber (Pub/Sub) pattern for broadcasting messages to multiple clients.\n  - Concurrency management using goroutines, channels, and `sync.RWMutex`.\n  - Rate limiting/aggregation of events over time.\n  - Non-blocking channel sends for robustness against slow consumers.\n",
  "internal/adapter/api/middleware/auth.go": "- File Path: `internal/adapter/api/middleware/auth.go`\n- High-Level Purpose: This file defines an HTTP middleware for authenticating incoming requests by validating an API key provided in the `X-API-Key` header.\n- Definitions in the File:\n  - Constants:\n    - `APIKeyHeader` (string): \"X-API-Key\". Specifies the HTTP header name for the API key.\n  - Functions:\n    - `Auth(repo domain.APIKeyRepository, logger *slog.Logger) func(http.Handler) http.Handler`: Public function.\n      - Description: A middleware factory that returns an `http.Handler` function. It extracts the API key from the request header, uses the provided `APIKeyRepository` to validate it, and either allows the request to proceed or responds with an `Unauthorized` or `Internal Server Error` status.\n- Notable Patterns or Logic:\n  - HTTP Middleware pattern for request interception and processing.\n  - Dependency injection for the API key repository and logger.\n",
  "internal/adapter/api/router.go": "- File Path: `internal/adapter/api/router.go`\n- High-Level Purpose: This file is responsible for creating and configuring the main HTTP router for the log ingestor's API. It sets up various routes, applies middleware for authentication, and integrates different handlers for log ingestion, SSE events, and health checks.\n- Definitions in the File:\n  - Functions:\n    - `NewRouter(cfg *config.Config, logger *slog.Logger, apiKeyRepo domain.APIKeyRepository, ingestUseCase *usecase.IngestLogUseCase, m *metrics.IngestMetrics, sseBroker *handler.SSEBroker) http.Handler`: Public function.\n      - Description: Creates an `http.ServeMux` (multiplexer) and configures the application's HTTP routes. It applies an authentication middleware (`middleware.Auth`) to the `/ingest` endpoint, registers the `IngestHandler` for log submission, the `SSEBroker` for real-time event updates, and a simple `/health` endpoint.\n- Notable Patterns or Logic:\n  - HTTP Router configuration using `http.ServeMux`.\n  - Middleware pattern for cross-cutting concerns (e.g., authentication).\n  - Dependency Injection for handlers, middleware, and other services.\n  - Clear separation of concerns between routing, authentication, and business logic.",
  "internal/adapter/metrics/metrics.go": "- File Path: `internal/adapter/metrics/metrics.go`\n- High-Level Purpose: This file defines and initializes a set of Prometheus metrics specifically for the log ingestor service, allowing for monitoring of ingestion rates, byte volume, WAL activity, and API key cache performance.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `IngestMetrics` (struct): Holds references to all Prometheus metric instances used by the ingest service.\n      - Fields:\n        - `EventsTotal` (*prometheus.CounterVec): A counter vector to track the total number of ingested events, categorized by their processing status (e.g., \"accepted\", \"error_parse\").\n        - `BytesTotal` (prometheus.Counter): A counter to track the total number of bytes ingested by the service.\n        - `WALActive` (prometheus.Gauge): A gauge to indicate whether the Write-Ahead Log is currently active (1) or inactive (0), reflecting Redis availability.\n        - `APIKeyCacheHits` (prometheus.Counter): A counter for the total number of times an API key was found in the cache.\n        - `APIKeyCacheMisses` (prometheus.Counter): A counter for the total number of times an API key was not found in the cache, requiring a database lookup.\n  - Functions:\n    - `NewIngestMetrics() *IngestMetrics`: Public function.\n      - Description: Constructor for `IngestMetrics`. It initializes and registers all the Prometheus metrics (CounterVec, Counter, Gauge) with appropriate namespaces, subsystems, names, help strings, and labels.\n- Notable Patterns or Logic:\n  - Observability pattern using Prometheus.\n  - Centralized definition and initialization of application metrics.\n  - Use of `prometheus.CounterVec` for metrics with multiple dimensions (e.g., event status).\n  - `promauto` package for automatic registration of metrics.\n",
  "internal/adapter/pii/redactor.go": "- File Path: `internal/adapter/pii/redactor.go`\n- High-Level Purpose: This file defines a `Redactor` component responsible for identifying and replacing sensitive (PII) fields within the JSON `Metadata` of `LogEvent` objects with a placeholder.\n- Definitions in the File:\n  - Constants:\n    - `RedactedPlaceholder` (string): \"[REDACTED]\". The string used to replace redacted PII values.\n  - Classes / Structs / Interfaces:\n    - `Redactor` (struct): Manages the redaction of specified PII fields from log event metadata.\n      - Fields:\n        - `fieldsToRedact` (map[string]struct{}): A set of field names whose values should be redacted.\n        - `logger` (*slog.Logger): Structured logger instance.\n  - Functions:\n    - `NewRedactor(fields []string, logger *slog.Logger) *Redactor`: Public function.\n      - Description: Constructor for `Redactor`, initializing it with a list of field names to be redacted and a logger. It converts the list of fields into a map for efficient lookups.\n    - `(*Redactor) Redact(event *domain.LogEvent) error`: Public method.\n      - Description: Modifies the provided `LogEvent` in place. It unmarshals the `Metadata` field (expected to be JSON), iterates through the configured `fieldsToRedact`, and replaces any matching field values with `RedactedPlaceholder`. If any redaction occurs, it sets `event.PIIRedacted` to `true` and remarshals the modified metadata back into `event.Metadata`. It logs warnings or errors if JSON processing fails.\n- Notable Patterns or Logic:\n  - Data transformation/processing component.\n  - Dynamic JSON field manipulation using `encoding/json` for unmarshalling and marshalling.\n  - In-place modification of a domain object (`LogEvent`).",
  "internal/adapter/pii/redactor_test.go": "- File Path: `internal/adapter/pii/redactor_test.go`\n- High-Level Purpose: This file contains unit tests for the `Redactor` component, ensuring it correctly identifies and redacts specified PII fields from JSON metadata within `LogEvent` objects, and handles various input conditions including invalid JSON.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `tests` (struct): Defines a test case structure for the `Redactor`.\n      - Fields: `name`, `inputMetadata`, `expectedMetadata`, `expectRedacted`, `expectErr`.\n  - Functions:\n    - `TestRedactor(t *testing.T)`: Public function.\n      - Description: Iterates through a slice of `tests` structs, running each as a sub-test. For each test case, it initializes a `LogEvent` with the `inputMetadata`, calls the `Redact` method, and then asserts against the `expectedMetadata`, `PIIRedacted` flag, and expected error state. It uses `json.Unmarshal` to compare metadata maps, avoiding issues with JSON key order.\n- Notable Patterns or Logic:\n  - Unit testing using Go's `testing` package.\n  - Table-driven tests for comprehensive coverage of different scenarios.\n  - JSON manipulation and comparison for verifying redaction.\n  - Testing error handling for invalid JSON input.\n",
  "internal/adapter/repository/postgres/apikey_repository.go": "- File Path: `internal/adapter/repository/postgres/apikey_repository.go`\n- High-Level Purpose: This file provides a PostgreSQL-backed implementation for the `domain.APIKeyRepository` interface, incorporating an in-memory, time-based cache to efficiently validate API keys.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `cacheEntry` (struct): Represents an entry in the API key cache.\n      - Fields:\n        - `isValid` (bool): True if the API key is valid.\n        - `expiresAt` (time.Time): Timestamp when the cache entry expires.\n    - `APIKeyRepository` (struct): Implements `domain.APIKeyRepository` with PostgreSQL and caching.\n      - Fields:\n        - `db` (*sql.DB): Database connection pool for PostgreSQL.\n        - `logger` (*slog.Logger): Structured logger.\n        - `cache` (map[string]cacheEntry): In-memory map storing cached API key validity.\n        - `mu` (sync.RWMutex): Read-write mutex to protect concurrent access to the cache.\n        - `cacheTTL` (time.Duration): Time-to-live for cache entries.\n        - `metrics` (*metrics.IngestMetrics): Prometheus metrics collector for cache hits/misses.\n  - Functions:\n    - `NewAPIKeyRepository(db *sql.DB, logger *slog.Logger, cacheTTL time.Duration, m *metrics.IngestMetrics) *APIKeyRepository`: Public function.\n      - Description: Constructor for `APIKeyRepository`. Initializes the repository with a database connection, logger, cache TTL, and metrics collector.\n    - `(*APIKeyRepository) IsValid(ctx context.Context, key string) (bool, error)`: Public method.\n      - Description: Checks if an API key is valid. It first attempts to retrieve the key from the in-memory cache. If not found or expired, it queries the PostgreSQL database, updates the cache, and returns the result. It uses a read-write mutex for safe concurrent cache access and records cache hit/miss metrics.\n- Notable Patterns or Logic:\n  - Repository pattern for API key validation.\n  - Read-through caching with time-based expiration.\n  - Concurrency control using `sync.RWMutex` for efficient cache access.\n  - Database query for API key validation, checking existence, active status, and expiration.\n  - Prometheus metrics integration for cache performance.\n",
  "internal/adapter/repository/postgres/log_repository.go": "- File Path: `internal/adapter/repository/postgres/log_repository.go`\n- High-Level Purpose: This file provides a PostgreSQL-backed implementation for the \"sink\" part of the `domain.LogRepository` interface, specifically designed for high-performance batch writing of log events using PostgreSQL's `COPY` protocol and idempotent upsert logic.\n- Definitions in the File:\n  - Constants:\n    - `logsTableName` (string): \"logs\". The name of the PostgreSQL table where logs are stored.\n  - Classes / Structs / Interfaces:\n    - `LogRepository` (struct): Implements the `domain.LogRepository` interface for PostgreSQL.\n      - Fields:\n        - `db` (*sql.DB): Database connection pool.\n        - `logger` (*slog.Logger): Structured logger instance.\n  - Functions:\n    - `NewLogRepository(db *sql.DB, logger *slog.Logger) *LogRepository`: Public function.\n      - Description: Constructor for `LogRepository`, initializing it with a database connection and logger.\n    - `(*LogRepository) WriteLogBatch(ctx context.Context, events []domain.LogEvent) error`: Public method.\n      - Description: Writes a batch of log events to PostgreSQL. It uses a database transaction, creates a temporary table, and leverages `pq.CopyIn` for efficient bulk data transfer. After staging, it performs an `INSERT ... ON CONFLICT (event_id) DO UPDATE` (upsert) from the temporary table into the main `logs` table, ensuring idempotency.\n    - `errNotImplemented` (var): Internal variable.\n      - Description: An error indicating that a method is not implemented for this specific repository type.\n    - `(*LogRepository) BufferLog(ctx context.Context, event domain.LogEvent) error`: Public method.\n      - Description: Returns `errNotImplemented` as this repository is not responsible for buffering.\n    - `(*LogRepository) ReadLogBatch(ctx context.Context, group, consumer string, count int) ([]domain.LogEvent, error)`: Public method.\n      - Description: Returns `nil, errNotImplemented` as this repository is not responsible for reading from a buffer.\n    - `(*LogRepository) AcknowledgeLogs(ctx context.Context, group string, messageIDs ...string) error`: Public method.\n      - Description: Returns `errNotImplemented` as this repository is not responsible for acknowledging buffered messages.\n- Notable Patterns or Logic:\n  - Repository pattern implementation for data sinking.\n  - High-performance bulk inserts using PostgreSQL's `COPY` protocol.\n  - Idempotent upsert logic using `ON CONFLICT DO UPDATE`.\n  - Transaction management for batch operations.\n  - Partial interface implementation, indicating a specialized role (sink only).\n",
  "internal/adapter/repository/redis/admin_repository.go": "- File Path: `internal/adapter/repository/redis/admin_repository.go`\n- High-Level Purpose: This file provides a Redis-backed implementation for administrative operations on Redis Streams, allowing retrieval of information about consumer groups, consumers, pending messages, and performing actions like claiming messages or trimming streams.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `AdminRepository` (struct): Implements the `domain.StreamAdminRepository` interface for Redis.\n      - Fields:\n        - `client` (*redis.Client): The Redis client instance.\n        - `logger` (*slog.Logger): Structured logger.\n  - Functions:\n    - `NewAdminRepository(client *redis.Client, logger *slog.Logger) *AdminRepository`: Public function.\n      - Description: Constructor for `AdminRepository`, initializing it with a Redis client and logger.\n    - `(*AdminRepository) GetGroupInfo(ctx context.Context, stream string) ([]domain.ConsumerGroupInfo, error)`: Public method.\n      - Description: Retrieves information about all consumer groups for a specified Redis Stream using `XInfoGroups`.\n    - `(*AdminRepository) GetConsumerInfo(ctx context.Context, stream, group string) ([]domain.ConsumerInfo, error)`: Public method.\n      - Description: Retrieves information about individual consumers within a specific consumer group for a Redis Stream using `XInfoConsumers`.\n    - `(*AdminRepository) GetPendingSummary(ctx context.Context, stream, group string) (*domain.PendingMessageSummary, error)`: Public method.\n      - Description: Provides a summary of pending messages for a given consumer group in a Redis Stream using `XPending`.\n    - `(*AdminRepository) GetPendingMessages(ctx context.Context, stream, group, consumer string, startID string, count int64) ([]domain.PendingMessageDetail, error)`: Public method.\n      - Description: Retrieves detailed information about pending messages for a specific consumer or group in a Redis Stream using `XPendingExt`.\n    - `(*AdminRepository) ClaimMessages(ctx context.Context, stream, group, consumer string, minIdleTime time.Duration, messageIDs []string) ([]domain.LogEvent, error)`: Public method.\n      - Description: Claims a list of pending messages for a new consumer in a Redis Stream, moving them from the pending entry list to the new consumer using `XClaim`. It attempts to unmarshal the message data into `LogEvent` objects.\n    - `(*AdminRepository) AcknowledgeMessages(ctx context.Context, stream, group string, messageIDs ...string) (int64, error)`: Public method.\n      - Description: Acknowledges a list of messages as processed for a consumer group in a Redis Stream using `XAck`.\n    - `(*AdminRepository) TrimStream(ctx context.Context, stream string, maxLen int64) (int64, error)`: Public method.\n      - Description: Trims a Redis Stream to a maximum length, removing older entries using `XTrimMaxLen`.\n- Notable Patterns or Logic:\n  - Repository pattern for Redis Stream administration.\n  - Direct mapping of Redis Stream commands (`XINFO`, `XPENDING`, `XCLAIM`, `XACK`, `XTRIM`) to Go methods.\n  - Error handling and wrapping for Redis client operations.\n",
  "internal/adapter/repository/redis/log_repository.go": "- File Path: `internal/adapter/repository/redis/log_repository.go`\n- High-Level Purpose: This file implements the `domain.LogRepository` interface using Redis Streams for buffering log events, providing mechanisms for writing, reading, acknowledging, and moving events to a Dead-Letter Queue. It includes a critical failover mechanism to a Write-Ahead Log (WAL) when Redis is unavailable.\n- Definitions in the File:\n  - Constants:\n    - `logStreamKey` (string): \"log_events\". The Redis Stream key used for buffering log events.\n  - Variables:\n    - `errNotImplemented` (error): An error indicating that a specific method is not implemented for this repository type.\n  - Classes / Structs / Interfaces:\n    - `LogRepository` (struct): Implements `domain.LogRepository` using Redis Streams with WAL failover.\n      - Fields:\n        - `client` (*redis.Client): The Redis client instance.\n        - `logger` (*slog.Logger): Structured logger.\n        - `wal` (domain.WALRepository): An optional Write-Ahead Log repository for durability during Redis outages.\n        - `dlqStreamKey` (string): The Redis Stream key for the Dead-Letter Queue.\n        - `isAvailable` (atomic.Bool): An atomic boolean flag indicating the current availability status of the Redis connection.\n        - `metrics` (*metrics.IngestMetrics): Prometheus metrics collector.\n  - Functions:\n    - `NewLogRepository(client *redis.Client, logger *slog.Logger, group, consumer, dlqStreamKey string, wal domain.WALRepository, m *metrics.IngestMetrics) (*LogRepository, error)`: Public function.\n      - Description: Constructor for `LogRepository`. Initializes the Redis client, logger, WAL (if provided), DLQ stream key, and metrics. It attempts to set up the Redis consumer group and assumes Redis is initially available.\n    - `(*LogRepository) StartHealthCheck(ctx context.Context, interval time.Duration)`: Public method.\n      - Description: Starts a background goroutine that periodically pings Redis. If Redis recovers from an outage, it triggers a `ReplayWAL`. If Redis becomes unavailable, it activates the WAL for buffering.\n    - `(*LogRepository) ReplayWAL(ctx context.Context) error`: Public method.\n      - Description: Reads all events from the configured WAL, buffers them into Redis, and then truncates the WAL upon successful replay. This is crucial for recovering events buffered during a Redis outage.\n    - `(*LogRepository) setupConsumerGroup(ctx context.Context, group string) error`: Internal method.\n      - Description: Creates a Redis consumer group for the `logStreamKey` if it doesn't already exist.\n    - `(*LogRepository) BufferLog(ctx context.Context, event domain.LogEvent) error`: Public method.\n      - Description: Buffers a single log event. If Redis is currently available, it writes to the Redis Stream. If Redis is unavailable (or a network error occurs during write), it falls back to writing the event to the WAL.\n    - `(*LogRepository) bufferLogToRedis(ctx context.Context, event domain.LogEvent) error`: Internal method.\n      - Description: Marshals a log event to JSON and adds it to the Redis Stream using `XADD`.\n    - `(*LogRepository) ReadLogBatch(ctx context.Context, group, consumer string, count int) ([]domain.LogEvent, error)`: Public method.\n      - Description: Reads a batch of log events from the Redis Stream for a specific consumer group and consumer using `XREADGROUP`.\n    - `(*LogRepository) AcknowledgeLogs(ctx context.Context, group string, messageIDs ...string) error`: Public method.\n      - Description: Acknowledges a list of processed messages in the Redis Stream for a given consumer group using `XACK`.\n    - `(*LogRepository) MoveToDLQ(ctx context.Context, events []domain.LogEvent) error`: Public method.\n      - Description: Moves a batch of log events to a Dead-Letter Queue (DLQ) Redis Stream, typically for events that failed processing after retries.\n    - `(*LogRepository) WriteLogBatch(ctx context.Context, events []domain.LogEvent) error`: Public method.\n      - Description: Returns `errNotImplemented` as this repository is designed for buffering/reading, not for final persistent storage writes.\n    - `isRedisBusyGroupError(err error) bool`: Internal function.\n      - Description: Helper to check if an error is a Redis `BUSYGROUP` error.\n    - `isNetworkError(err error) bool`: Internal function.\n      - Description: Helper to check if an error is a network-related error, indicating connectivity issues with Redis.\n- Notable Patterns or Logic:\n  - Repository pattern for Redis Streams.\n  - Write-Ahead Log (WAL) integration for failover and durability during Redis outages.\n  - Health checking and automatic WAL replay upon Redis recovery.\n  - Atomic operations (`atomic.Bool`) for managing Redis availability state.\n  - Redis Streams consumer group pattern for distributed message processing.\n  - Dead-Letter Queue (DLQ) implementation for handling unprocessable messages.\n  - Partial interface implementation, specializing in buffering and reading.\n",
  "internal/adapter/repository/wal/wal_repository.go": "- File Path: `internal/adapter/repository/wal/wal_repository.go`\n- High-Level Purpose: This file provides a file-based Write-Ahead Log (WAL) implementation for durable storage of log events, designed to handle temporary failures of primary storage systems. It supports segment rotation, replay, and truncation.\n- Definitions in the File:\n  - Constants:\n    - `segmentPrefix` (string): \"segment-\". Prefix for WAL segment filenames.\n    - `filePerm` (os.FileMode): 0644. Default file permissions for WAL segments.\n  - Classes / Structs / Interfaces:\n    - `WALRepository` (struct): Implements a file-based Write-Ahead Log.\n      - Fields:\n        - `dir` (string): Directory path where WAL segments are stored.\n        - `maxSegmentSize` (int64): Maximum size for a single WAL segment file before rotation.\n        - `maxTotalSize` (int64): Maximum total disk space allowed for all WAL segments.\n        - `logger` (*slog.Logger): Structured logger.\n        - `mu` (sync.Mutex): Mutex for protecting concurrent access to WAL operations.\n        - `currentSegment` (*os.File): File handle for the currently active WAL segment.\n        - `currentSize` (int64): Current size of the active WAL segment.\n  - Functions:\n    - `NewWALRepository(dir string, maxSegmentSize, maxTotalSize int64, logger *slog.Logger) (*WALRepository, error)`: Public function.\n      - Description: Constructor for `WALRepository`. Initializes the WAL directory, size limits, and logger. It attempts to open the latest existing segment or create a new one.\n    - `(*WALRepository) Write(ctx context.Context, event domain.LogEvent) error`: Public method.\n      - Description: Marshals a log event to JSON, appends a newline, and writes it to the current WAL segment. It checks for `maxTotalSize` before writing and rotates the segment if `maxSegmentSize` is reached.\n    - `(*WALRepository) Replay(ctx context.Context, handler func(event domain.LogEvent) error) error`: Public method.\n      - Description: Reads all events from all WAL segments in chronological order. For each event, it calls the provided `handler` function. It closes the current segment before replay and re-opens a new one after.\n    - `(*WALRepository) Truncate(ctx context.Context) error`: Public method.\n      - Description: Closes the current segment and removes all WAL segment files from the directory. It then opens a new, empty segment.\n    - `(*WALRepository) rotate() error`: Internal method.\n      - Description: Closes the current WAL segment (if open), generates a new segment filename based on the current timestamp, creates and opens the new file, and resets `currentSize`.\n    - `(*WALRepository) openLatestSegment() error`: Internal method.\n      - Description: Identifies the latest WAL segment file, opens it for appending, and sets `currentSize` to its current size. If no segments exist, it calls `rotate` to create a new one. If the latest segment is already full, it also calls `rotate`.\n    - `(*WALRepository) getSortedSegments() ([]string, error)`: Internal method.\n      - Description: Reads the WAL directory, filters for segment files, and returns their full paths sorted chronologically.\n    - `(*WALRepository) calculateTotalSize() (int64, error)`: Internal method.\n      - Description: Calculates the total disk space occupied by all WAL segment files.\n    - `(*WALRepository) Close() error`: Public method.\n      - Description: Closes the currently active WAL segment file, ensuring all buffered writes are flushed to disk.\n- Notable Patterns or Logic:\n  - Write-Ahead Log (WAL) implementation for durability and crash recovery.\n  - Segment-based storage with configurable `maxSegmentSize` and `maxTotalSize`.\n  - File system operations for managing log files (`os.MkdirAll`, `os.OpenFile`, `os.Remove`, `os.ReadDir`, `os.Stat`).\n  - Concurrency control using `sync.Mutex` for safe file access.\n  - Line-by-line processing of JSON events during replay using `bufio.Scanner`.\n",
  "internal/adapter/repository/wal/wal_repository_test.go": "- File Path: `internal/adapter/repository/wal/wal_repository_test.go`\n- High-Level Purpose: This file contains unit tests for the `WALRepository` (Write-Ahead Log) implementation, verifying its core functionalities such as writing, replaying, segment rotation, truncation, and adherence to maximum total size limits.\n- Definitions in the File:\n  - Functions:\n    - `setupTestWAL(t *testing.T, maxSegmentSize, maxTotalSize int64) (*WALRepository, func())`: Internal function.\n      - Description: A helper function that creates a temporary directory for WAL segments, initializes a `WALRepository` with specified size limits and a logger, and returns the repository instance along with a cleanup function.\n    - `TestWAL_WriteAndReplay(t *testing.T)`: Public function.\n      - Description: Tests the fundamental write and replay mechanism. It writes several events, closes the WAL (to flush data), re-opens it (simulating a restart), and then replays the events, asserting that all original events are recovered in order.\n    - `TestWAL_SegmentRotation(t *testing.T)`: Public function.\n      - Description: Tests that the WAL correctly rotates to a new segment file when the `maxSegmentSize` is exceeded by writing multiple events. It then verifies that at least two segment files are created.\n    - `TestWAL_Truncate(t *testing.T)`: Public function.\n      - Description: Tests the `Truncate` functionality. It writes an event, verifies a segment exists, then truncates the WAL, and finally asserts that all old segments are removed and a new, empty segment is created.\n    - `TestWAL_MaxTotalSize(t *testing.T)`: Public function.\n      - Description: Tests the `maxTotalSize` limit. It attempts to write events until the total size of all segments exceeds the configured maximum, expecting an error to be returned.\n- Notable Patterns or Logic:\n  - Unit testing using Go's `testing` package.\n  - Helper functions for test setup and teardown (`setupTestWAL`, `cleanup`).\n  - File system interaction for managing WAL segments (`os.MkdirTemp`, `os.RemoveAll`, `os.Stat`).\n  - Testing durability and recovery mechanisms of a Write-Ahead Log.\n",
  "internal/domain/admin.go": "- File Path: `internal/domain/admin.go`\n- High-Level Purpose: This file defines the core data structures (domain models) used to represent administrative information about Redis Streams, including details about consumer groups, individual consumers, and summaries/details of pending messages.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `ConsumerGroupInfo` (struct): Represents information about a Redis Stream consumer group.\n      - Fields: `Name` (string), `Consumers` (int64), `Pending` (int64), `LastDeliveredID` (string).\n    - `ConsumerInfo` (struct): Represents information about a specific consumer within a group.\n      - Fields: `Name` (string), `Pending` (int64), `Idle` (time.Duration).\n    - `PendingMessageSummary` (struct): Provides a high-level summary of pending messages for a consumer group.\n      - Fields: `Total` (int64), `FirstMessageID` (string), `LastMessageID` (string), `ConsumerTotals` (map[string]int64).\n    - `PendingMessageDetail` (struct): Represents detailed information about a single pending message.\n      - Fields: `ID` (string), `Consumer` (string), `IdleTime` (time.Duration), `RetryCount` (int64).\n- Notable Patterns or Logic:\n  - Domain model definition.\n  - Use of `json` struct tags for serialization/deserialization.\n",
  "internal/domain/log.go": "- File Path: `internal/domain/log.go`\n- High-Level Purpose: This file defines the `LogEvent` struct, which serves as the core data model for representing a single log event within the application's domain.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `LogEvent` (struct): Represents a single log event with various attributes.\n      - Fields:\n        - `ID` (string): Unique identifier for the log event.\n        - `ReceivedAt` (time.Time): Timestamp when the event was received by the system.\n        - `EventTime` (time.Time): Original timestamp of the event.\n        - `Source` (string): Origin of the log event (e.g., service name).\n        - `Level` (string): Log level (e.g., \"info\", \"error\").\n        - `Message` (string): The main log message.\n        - `Metadata` (json.RawMessage): Arbitrary JSON metadata associated with the event.\n        - `RawEvent` (json.RawMessage): The original raw payload of the event (not serialized for output).\n        - `PIIRedacted` (bool): Flag indicating if PII has been redacted from the event.\n        - `StreamMessageID` (string): Transient field for Redis Stream message ID (not serialized for output).\n- Notable Patterns or Logic:\n  - Domain model definition for log events.\n  - Use of `json.RawMessage` for flexible metadata storage.\n",
  "internal/domain/mocks/repository_mocks.go": "- File Path: `internal/domain/mocks/repository_mocks.go`\n- High-Level Purpose: This file provides a mock implementation of the `domain.LogRepository` interface, designed for unit testing other components that interact with a log repository without requiring a real database or message queue.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `MockLogRepository` (struct): A mock implementation of `domain.LogRepository`.\n      - Fields:\n        - `mu` (sync.Mutex): Mutex to protect concurrent access to mock data.\n        - `BufferedEvents` ([]domain.LogEvent): Stores events passed to `BufferLog`.\n        - `WrittenEvents` ([]domain.LogEvent): Stores events passed to `WriteLogBatch`.\n        - `AckedMessageIDs` ([]string): Stores message IDs passed to `AcknowledgeLogs`.\n        - `DLQEvents` ([]domain.LogEvent): Stores events passed to `MoveToDLQ`.\n        - `ReadBatchResult` ([]domain.LogEvent): Configurable return value for `ReadLogBatch`.\n        - `BufferErr` (error): Configurable error to return from `BufferLog`.\n        - `ReadErr` (error): Configurable error to return from `ReadLogBatch`.\n        - `WriteErr` (error): Configurable error to return from `WriteLogBatch`.\n        - `AckErr` (error): Configurable error to return from `AcknowledgeLogs`.\n        - `DLQErr` (error): Configurable error to return from `MoveToDLQ`.\n  - Functions:\n    - `(*MockLogRepository) BufferLog(ctx context.Context, event domain.LogEvent) error`: Public method.\n      - Description: Appends the event to `BufferedEvents` or returns `BufferErr` if set.\n    - `(*MockLogRepository) ReadLogBatch(ctx context.Context, group, consumer string, count int) ([]domain.LogEvent, error)`: Public method.\n      - Description: Returns `ReadBatchResult` or `ReadErr` if set.\n    - `(*MockLogRepository) WriteLogBatch(ctx context.Context, events []domain.LogEvent) error`: Public method.\n      - Description: Appends the events to `WrittenEvents` or returns `WriteErr` if set.\n    - `(*MockLogRepository) AcknowledgeLogs(ctx context.Context, group string, messageIDs ...string) error`: Public method.\n      - Description: Appends the message IDs to `AckedMessageIDs` or returns `AckErr` if set.\n    - `(*MockLogRepository) MoveToDLQ(ctx context.Context, events []domain.LogEvent) error`: Public method.\n      - Description: Appends the events to `DLQEvents` or returns `DLQErr` if set.\n- Notable Patterns or Logic:\n  - Mock object pattern for testing.\n  - Dependency injection for testing components that rely on `domain.LogRepository`.\n  - Concurrency safety for mock state using `sync.Mutex`.\n",
  "internal/domain/repository.go": "- File Path: `internal/domain/repository.go`\n- High-Level Purpose: This file defines a set of interfaces that abstract various data access and administrative operations across the application, promoting loose coupling and allowing for different implementations (e.g., Redis, PostgreSQL, WAL).\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `LogRepository` (interface): Defines methods for buffering, reading batches, writing batches, acknowledging, and moving log events to a Dead-Letter Queue.\n      - Methods: `BufferLog`, `ReadLogBatch`, `WriteLogBatch`, `AcknowledgeLogs`, `MoveToDLQ`.\n    - `APIKeyRepository` (interface): Defines a method for validating API keys.\n      - Methods: `IsValid`.\n    - `WALRepository` (interface): Defines methods for interacting with a Write-Ahead Log, including writing, replaying, truncating, and closing.\n      - Methods: `Write`, `Replay`, `Truncate`, `Close`.\n    - `StreamAdminRepository` (interface): Defines methods for administrative operations on a stream, such as getting group/consumer info, pending message summaries, claiming messages, acknowledging messages, and trimming streams.\n      - Methods: `GetGroupInfo`, `GetConsumerInfo`, `GetPendingSummary`, `GetPendingMessages`, `ClaimMessages`, `AcknowledgeMessages`, `TrimStream`.\n- Notable Patterns or Logic:\n  - Repository pattern using Go interfaces.\n  - Interface segregation principle, with distinct interfaces for different concerns.\n  - Dependency inversion principle, allowing concrete implementations to be swapped.\n",
  "internal/pkg/config/config.go": "- File Path: `internal/pkg/config/config.go`\n- High-Level Purpose: This file defines the application's configuration structure and provides a utility function to load these settings from environment variables, with support for `.env` files for local development.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `Config` (struct): Holds all application configuration parameters.\n      - Fields:\n        - `LogLevel` (string): Logging level (e.g., \"info\", \"debug\").\n        - `MaxEventSize` (int64): Maximum allowed size for an incoming log event (default 1MB).\n        - `WALPath` (string): Path for Write-Ahead Log files (default \"./wal\").\n        - `WALSegmentSize` (int64): Maximum size of a single WAL segment (default 100MB).\n        - `WALMaxDiskSize` (int64): Maximum total disk space for the WAL (default 1GB).\n        - `BackpressurePolicy` (string): Policy for handling backpressure (default \"block\").\n        - `RedisAddr` (string): Address of the Redis server (required).\n        - `RedisDLQStream` (string): Name of the Redis Dead-Letter Queue stream (default \"log_events_dlq\").\n        - `PostgresURL` (string): Connection URL for PostgreSQL (required).\n        - `APIKeyCacheTTL` (time.Duration): Time-to-live for API key cache entries (default 5m).\n        - `PIIRedactionFields` (string): Comma-separated list of fields to redact for PII (default \"email,password,credit_card,ssn\").\n        - `IngestServerAddr` (string): Address for the HTTP ingest server to listen on (default \":8080\").\n        - `ConsumerRetryCount` (int): Number of retries for consumer processing (default 3).\n        - `ConsumerRetryBackoff` (time.Duration): Initial backoff duration for consumer retries (default 1s).\n  - Functions:\n    - `Load() (*Config, error)`: Public function.\n      - Description: Loads configuration from environment variables. It first attempts to load a `.env` file (for local development) and then parses environment variables into the `Config` struct using `github.com/caarlos0/env`.\n- Notable Patterns or Logic:\n  - Centralized configuration management.\n  - Environment variable parsing using `github.com/caarlos0/env`.\n  - Support for `.env` files for development convenience using `github.com/joho/godotenv`.\n",
  "internal/pkg/logger/logger.go": "- File Path: `internal/pkg/logger/logger.go`\n- High-Level Purpose: This file provides a utility function to create and configure a structured logger using Go's `slog` package.\n- Definitions in the File:\n  - Functions:\n    - `New(level string) *slog.Logger`: Public function.\n      - Description: Creates and returns a new `slog.Logger` instance. It configures the logger to output JSON format to standard output and sets the logging level based on the provided string (e.g., \"debug\", \"info\", \"warn\", \"error\"). Defaults to \"info\" if an unknown level is provided.\n- Notable Patterns or Logic:\n  - Centralized logger initialization.\n  - Structured logging setup using `slog`.\n",
  "internal/usecase/admin_stream.go": "- File Path: `internal/usecase/admin_stream.go`\n- High-Level Purpose: This file defines the `AdminStreamUseCase`, which acts as an intermediary between the API layer and the `StreamAdminRepository` for administrative operations on Redis Streams. It encapsulates the business logic for these operations, primarily by delegating to the repository.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `AdminStreamUseCase` (struct): Provides use cases for stream administration.\n      - Fields:\n        - `repo` (`domain.StreamAdminRepository`): The repository interface for stream administrative operations.\n  - Functions:\n    - `NewAdminStreamUseCase(repo domain.StreamAdminRepository) *AdminStreamUseCase`: Public function.\n      - Description: Constructor for `AdminStreamUseCase`, initializing it with the `StreamAdminRepository`.\n    - `(*AdminStreamUseCase) GetGroupInfo(ctx context.Context, stream string) ([]domain.ConsumerGroupInfo, error)`: Public method.\n      - Description: Delegates to the repository to retrieve consumer group information for a stream.\n    - `(*AdminStreamUseCase) GetConsumerInfo(ctx context.Context, stream, group string) ([]domain.ConsumerInfo, error)`: Public method.\n      - Description: Delegates to the repository to retrieve consumer information for a specific group.\n    - `(*AdminStreamUseCase) GetPendingSummary(ctx context.Context, stream, group string) (*domain.PendingMessageSummary, error)`: Public method.\n      - Description: Delegates to the repository to retrieve a summary of pending messages for a group.\n    - `(*AdminStreamUseCase) GetPendingMessages(ctx context.Context, stream, group, consumer string, startID string, count int64) ([]domain.PendingMessageDetail, error)`: Public method.\n      - Description: Delegates to the repository to retrieve detailed pending messages, applying default values for `startID` and `count` if not provided.\n    - `(*AdminStreamUseCase) ClaimMessages(ctx context.Context, stream, group, consumer string, minIdleTime time.Duration, messageIDs []string) ([]domain.LogEvent, error)`: Public method.\n      - Description: Delegates to the repository to claim pending messages for a consumer.\n    - `(*AdminStreamUseCase) AcknowledgeMessages(ctx context.Context, stream, group string, messageIDs ...string) (int64, error)`: Public method.\n      - Description: Delegates to the repository to acknowledge messages.\n    - `(*AdminStreamUseCase) TrimStream(ctx context.Context, stream string, maxLen int64) (int64, error)`: Public method.\n      - Description: Delegates to the repository to trim a stream to a maximum length.\n- Notable Patterns or Logic:\n  - Use Case pattern for encapsulating business logic.\n  - Dependency Injection for the `StreamAdminRepository`.\n  - Simple delegation to the repository, with some minor input validation/defaulting (e.g., for `GetPendingMessages`).\n",
  "internal/usecase/ingest_log.go": "- File Path: `internal/usecase/ingest_log.go`\n- High-Level Purpose: This file defines the `IngestLogUseCase`, which encapsulates the business logic for processing and buffering log events, including enrichment, PII redaction, and interaction with the log repository.\n- Definitions in the File:\n  - Classes / Structs / Interfaces:\n    - `IngestLogUseCase` (struct): Handles the core business logic for ingesting a log event.\n      - Fields:\n        - `repo` (`domain.LogRepository`): An interface for buffering log events.\n        - `redactor` (`*pii.Redactor`): A component responsible for redacting PII from log events.\n        - `logger` (`*slog.Logger`): A structured logger instance.\n  - Functions:\n    - `NewIngestLogUseCase(repo domain.LogRepository, redactor *pii.Redactor, logger *slog.Logger) *IngestLogUseCase`: Public function.\n      - Description: Constructor for `IngestLogUseCase`, initializing it with the necessary repository, PII redactor, and logger.\n    - `(*IngestLogUseCase) Ingest(ctx context.Context, event *domain.LogEvent) error`: Public method.\n      - Description: Validates, enriches, redacts, and buffers a log event. It sets `ReceivedAt` and generates an `ID` if missing, then calls the `pii.Redactor` to redact sensitive information. Finally, it uses the `domain.LogRepository` to buffer the processed event. It logs warnings for redaction failures and errors for buffering failures.\n- Notable Patterns or Logic:\n  - Use Case pattern for encapsulating business logic.\n  - Dependency injection for external services (repository, redactor, logger).\n  - In-place modification of the `domain.LogEvent` object.\n",
  "internal/usecase/ingest_log_test.go": "- File Path: `internal/usecase/ingest_log_test.go`\n- High-Level Purpose: This file contains unit tests for the `IngestLogUseCase`, verifying its functionality in processing log events, including ID generation, timestamp setting, PII redaction, and handling repository errors.\n- Definitions in the File:\n  - Functions:\n    - `TestIngestLogUseCase_Ingest(t *testing.T)`: Public function.\n      - Description: A test suite for the `Ingest` method, containing several sub-tests:\n        - `Successful Ingestion`: Verifies that a log event is enriched with an ID and `ReceivedAt` timestamp, and then successfully buffered by the mock repository.\n        - `Repository Error`: Simulates an error from the underlying log repository during buffering and ensures the error is propagated.\n        - `PII Redaction`: Tests that the PII redactor correctly identifies and redacts sensitive fields within the event's metadata, and sets the `PIIRedacted` flag.\n- Notable Patterns or Logic:\n  - Unit testing using Go's `testing` package.\n  - Table-driven tests (implicitly via `t.Run` sub-tests).\n  - Mocking dependencies (`mocks.MockLogRepository`, `pii.Redactor`) to isolate the use case logic.\n  - Testing data enrichment and transformation.\n",
  "internal/usecase/process_logs.go": "- File Path: `internal/usecase/process_logs.go`\n- High-Level Purpose: This file defines the `ProcessLogsUseCase`, which orchestrates the end-to-end processing of log events by reading them from a buffer, attempting to write them to a persistent sink with retries, and managing acknowledgments or moving failed events to a Dead-Letter Queue.\n- Definitions in the File:\n  - Constants:\n    - `defaultBatchSize` (int): 1000. The default number of log events to read in a single batch.\n  - Classes / Structs / Interfaces:\n    - `ProcessLogsUseCase` (struct): Orchestrates the processing of log events from buffer to sink.\n      - Fields:\n        - `bufferRepo` (`domain.LogRepository`): Repository for reading from the log buffer (e.g., Redis Stream).\n        - `sinkRepo` (`domain.LogRepository`): Repository for writing to the final persistent storage (e.g., PostgreSQL).\n        - `logger` (*slog.Logger): Structured logger.\n        - `group` (string): Consumer group name for the buffer.\n        - `consumer` (string): Unique consumer name.\n        - `retryCount` (int): Number of retries for writing to the sink.\n        - `retryBackoff` (time.Duration): Initial backoff duration for retries.\n  - Functions:\n    - `NewProcessLogsUseCase(bufferRepo, sinkRepo domain.LogRepository, logger *slog.Logger, group, consumer string, retryCount int, retryBackoff time.Duration) *ProcessLogsUseCase`: Public function.\n      - Description: Constructor for `ProcessLogsUseCase`, initializing it with buffer and sink repositories, logger, consumer group/name, and retry configuration.\n    - `(*ProcessLogsUseCase) ProcessBatch(ctx context.Context) (int, error)`: Public method.\n      - Description: Reads a batch of logs from the `bufferRepo`. It then attempts to write these logs to the `sinkRepo` using a retry mechanism. If writing fails after all retries, it moves the events to a Dead-Letter Queue via the `bufferRepo`. Finally, it acknowledges successfully processed or DLQ'd events in the `bufferRepo`.\n    - `(*ProcessLogsUseCase) writeWithRetry(ctx context.Context, events []domain.LogEvent) error`: Internal method.\n      - Description: Attempts to write a batch of log events to the `sinkRepo` with a configurable number of retries and exponential backoff between attempts.\n- Notable Patterns or Logic:\n  - Use Case pattern for orchestrating complex business logic.\n  - Dependency injection for buffer and sink repositories.\n  - Retry mechanism with exponential backoff for resilient writes to the sink.\n  - Dead-Letter Queue (DLQ) integration for handling persistently failing events.\n  - Consumer group pattern for distributed log processing.\n",
  "internal/usecase/process_logs_test.go": "- File Path: `internal/usecase/process_logs_test.go`\n- High-Level Purpose: This file contains unit tests for the `ProcessLogsUseCase`, ensuring its correct behavior in various scenarios, including successful processing, handling sink failures with retries and Dead-Letter Queue (DLQ) integration, and managing buffer read errors.\n- Definitions in the File:\n  - Functions:\n    - `TestProcessLogsUseCase_ProcessBatch(t *testing.T)`: Public function.\n      - Description: A test suite for the `ProcessBatch` method, containing several sub-tests:\n        - `Successful Processing`: Verifies that events are read from the buffer, written to the sink, and acknowledged.\n        - `Sink Failure with Retry and DLQ`: Simulates a sink write error, ensuring retries are attempted, events are moved to the DLQ, and then acknowledged from the buffer.\n        - `Buffer Read Error`: Tests that an error during buffer reading is propagated.\n        - `No Events to Process`: Verifies that the use case handles an empty batch gracefully without errors.\n- Notable Patterns or Logic:\n  - Unit testing using Go's `testing` package.\n  - Table-driven tests (implicitly via `t.Run` sub-tests).\n  - Mocking dependencies (`mocks.MockLogRepository`) to isolate the use case logic.\n  - Testing retry mechanisms and Dead-Letter Queue functionality.\n",
  "tests/integration/ingest_flow_test.go": "- File Path: `tests/integration/ingest_flow_test.go`\n- High-Level Purpose: This file contains integration tests for the log ingestion pipeline, verifying the end-to-end flow from ingesting NDJSON events to their persistence in PostgreSQL, including idempotency checks. It orchestrates the Docker Compose environment for testing.\n- Definitions in the File:\n  - Constants:\n    - `ingestorURL` (string): The URL of the ingestor service.\n    - `postgresDSN` (string): The PostgreSQL connection string for the test database.\n  - Functions:\n    - `TestMain(m *testing.M)`: Public function.\n      - Description: The entry point for integration tests. It starts the Docker Compose environment, waits for PostgreSQL to become healthy, runs all defined tests, and then shuts down the Docker Compose environment.\n    - `shutdown()`: Internal function.\n      - Description: Executes `docker-compose down -v` to stop and remove all services and volumes.\n    - `waitForPostgres() bool`: Internal function.\n      - Description: Attempts to connect and ping the PostgreSQL database repeatedly for a set duration, returning `true` if successful.\n    - `countLogsInDB(t *testing.T) int`: Internal function.\n      - Description: Connects to the PostgreSQL database and queries the `logs` table to count the number of entries. Fails the test if connection or query fails.\n    - `TestIngestionFlow(t *testing.T)`: Public function.\n      - Description: The main integration test case. It performs the following steps:\n        1.  Waits for the consumer to start.\n        2.  Verifies the initial log count in the database is zero.\n        3.  Generates a batch of unique log events in NDJSON format and sends them to the ingestor.\n        4.  Asserts that the ingestor returns `202 Accepted`.\n        5.  Polls the database to verify that all events from the batch are eventually processed and stored.\n        6.  Sends the *same* batch of events again to test idempotency.\n        7.  Asserts that the ingestor returns `202 Accepted` again.\n        8.  Verifies that the log count in the database remains unchanged, confirming idempotency.\n- Notable Patterns or Logic:\n  - Integration testing using Go's `testing` package.\n  - Orchestration of external services (PostgreSQL, Redis, Ingestor, Consumer) using `docker-compose`.\n  - Retry logic (`waitForPostgres`, polling `countLogsInDB`) to handle eventual consistency in distributed systems.\n  - Testing for idempotency by re-submitting the same data.\n",
  "tools/load-tester/main.go": "- File Path: `tools/load-tester/main.go`\n- High-Level Purpose: This file provides a command-line utility for load testing the log ingestion service. It allows users to configure the target URL, API key, concurrency, duration, and requests per second (RPS) to simulate various load conditions.\n- Definitions in the File:\n  - Functions:\n    - `main()`: Main function.\n      - Description:\n        1.  Parses command-line flags for `url`, `api-key`, `concurrency`, `duration`, and `rps`.\n        2.  Initializes a `rate.Limiter` to control the overall requests per second.\n        3.  Spawns multiple goroutines (workers) based on the `concurrency` flag.\n        4.  Each worker continuously generates unique log events, sends them as JSON POST requests to the target URL with the specified API key, and records success or error counts.\n        5.  The test runs for the configured `duration` or until the context is canceled.\n        6.  After all workers complete, it prints a summary of total requests, successful requests, errors, and the actual RPS achieved.\n- Notable Patterns or Logic:\n  - Command-line argument parsing using `flag` package.\n  - Concurrency management using `sync.WaitGroup` and goroutines.\n  - Rate limiting using `golang.org/x/time/rate` to control request throughput.\n  - Atomic counters (`atomic.Int64`) for thread-safe statistics collection.\n  - Context-based cancellation for graceful shutdown of workers.\n"
}