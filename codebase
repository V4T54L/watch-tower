{
  "CHAOS.md": "# Chaos \u0026 Resilience Test Checklist\n\nThis document outlines a series of chaos engineering experiments to validate the resilience and durability of the log ingestion service. These tests should be performed in a staging environment that closely mirrors production.\n\n## 1. Redis Outage (Buffer Unavailability)\n\n**Goal:** Verify that the Write-Ahead Log (WAL) fallback mechanism works correctly and no data is lost during a transient Redis failure.\n\n**Procedure:**\n1.  Start a sustained load test against the `/ingest` endpoint.\n2.  While the load is active, stop the Redis container: `docker-compose stop redis`.\n3.  **Observe:**\n    - `[ ]` The ingestor service logs should indicate Redis is unavailable and it's falling back to the WAL.\n    - `[ ]` The `/ingest` endpoint should remain responsive, returning `202 Accepted`.\n    - `[ ]` The `wal_active` Prometheus metric should switch to `1`.\n    - `[ ]` WAL segment files should be created and grow in the configured WAL directory.\n4.  After a few minutes, restart the Redis container: `docker-compose start redis`.\n5.  **Observe:**\n    - `[ ]` The ingestor service logs should indicate Redis is available again and that it's starting to replay events from the WAL.\n    - `[ ]` The `wal_active` Prometheus metric should return to `0`.\n    - `[ ]` After replay is complete, the WAL directory should be truncated (emptied).\n    - `[ ]` Query PostgreSQL to confirm that events ingested during the outage have been successfully processed and stored. There should be no data loss.\n\n## 2. PostgreSQL Outage (Sink Unavailability)\n\n**Goal:** Verify that the consumer workers handle sink failures gracefully, retry, and eventually move messages to the Dead-Letter Queue (DLQ) without losing data from the buffer.\n\n**Procedure:**\n1.  Start a sustained load test.\n2.  While the load is active, stop the PostgreSQL container: `docker-compose stop postgres`.\n3.  **Observe:**\n    - `[ ]` The consumer worker logs should show errors when trying to write to PostgreSQL, followed by retry attempts with backoff.\n    - `[ ]` Use the admin API or `redis-cli` to check pending messages in the Redis Stream (`XPENDING log_events log-processors`). The number of pending messages should grow significantly.\n    - `[ ]` After the configured number of retries, the consumer logs should indicate that failed batches are being moved to the DLQ stream (`log_events_dlq`).\n    - `[ ]` The main `log_events` stream should continue to be acknowledged, preventing it from growing indefinitely with unprocessable messages.\n4.  Restart the PostgreSQL container: `docker-compose start postgres`.\n5.  **Observe:**\n    - `[ ]` New messages should start processing and sinking to PostgreSQL successfully.\n    - `[ ]` Messages in the DLQ remain there. An operator would need to manually inspect and replay them. This confirms the DLQ mechanism is working as designed.\n\n## 3. Consumer Worker Crash\n\n**Goal:** Verify that the Redis consumer group correctly reassigns pending messages from a crashed consumer to a healthy one, ensuring at-least-once processing.\n\n**Procedure:**\n1.  Ensure at least two consumer replicas are running (`docker-compose up --scale consumer=2`).\n2.  Start a sustained load test.\n3.  Identify one of the consumer containers and stop it: `docker kill \u003cconsumer_container_id\u003e`.\n4.  **Observe:**\n    - `[ ]` Use the admin API or `redis-cli` to inspect pending messages. Messages that were being processed by the killed consumer will become idle.\n    - `[ ]` After a short period (the idle timeout), the healthy consumer should claim the pending messages from the dead consumer (`XCLAIM`). This can be observed in the healthy consumer's logs or by monitoring `XPENDING`.\n    - `[ ]` Query PostgreSQL to confirm that all data is eventually processed and stored. There should be no data loss.\n\n## 4. Network Partition / Latency\n\n**Goal:** Simulate network degradation to observe service behavior under non-ideal network conditions.\n\n**Procedure:**\n1.  Use a tool like `tc` (traffic control) on Linux or a Docker network with latency settings to introduce high latency between:\n    - The ingestor and Redis.\n    - The consumer and Redis.\n    - The consumer and PostgreSQL.\n2.  **Observe:**\n    - `[ ]` **Ingestor \u003c-\u003e Redis Latency:** Ingestion latency (`p99`) should increase. If latency is high enough to cause timeouts, the WAL fallback should engage.\n    - `[ ]` **Consumer \u003c-\u003e PostgreSQL Latency:** Consumer batch processing times should increase. This may lead to a backlog in the Redis Stream, causing `XPENDING` to grow. The system should remain stable but process logs more slowly.\n",
  "Dockerfile.consumer": "# Stage 1: Build the application\nFROM golang:1.21-alpine AS builder\n\nWORKDIR /app\n\n# Install build dependencies\nRUN apk add --no-cache git\n\n# Copy go.mod and go.sum files to download dependencies\nCOPY go.mod go.sum ./\nRUN go mod download\n\n# Copy the rest of the source code\nCOPY . .\n\n# Build the consumer binary\nRUN CGO_ENABLED=0 GOOS=linux go build -ldflags=\"-w -s\" -o /consumer ./cmd/consumer\n\n# Stage 2: Create the final lightweight image\nFROM alpine:latest\n\nWORKDIR /app\n\n# Copy the binary from the builder stage\nCOPY --from=builder /consumer /app/consumer\n\n# Set the entrypoint\nENTRYPOINT [\"/app/consumer\"]\n",
  "Dockerfile.ingest": "# Stage 1: Build the application\nFROM golang:1.21-alpine AS builder\n\nWORKDIR /app\n\n# Install build dependencies\nRUN apk add --no-cache git\n\n# Copy go.mod and go.sum files to download dependencies\nCOPY go.mod go.sum ./\nRUN go mod download\n\n# Copy the rest of the source code\nCOPY . .\n\n# Build the ingest binary\nRUN CGO_ENABLED=0 GOOS=linux go build -ldflags=\"-w -s\" -o /ingest ./cmd/ingest\n\n# Stage 2: Create the final lightweight image\nFROM alpine:latest\n\nWORKDIR /app\n\n# Copy the binary from the builder stage\nCOPY --from=builder /ingest /app/ingest\n\n# Copy the frontend static files\nCOPY frontend ./frontend\n\n# Expose the ingest and admin/metrics ports\nEXPOSE 8080\nEXPOSE 9091\n\n# Set the entrypoint\nENTRYPOINT [\"/app/ingest\"]\n",
  "Makefile": ".PHONY: all build test lint clean docker-build docker-push compose-up compose-down compose-logs help\n\n# Variables\nAPP_NAME := log-ingestor\nINGEST_IMG_NAME := ${APP_NAME}-ingest\nCONSUMER_IMG_NAME := ${APP_NAME}-consumer\nTAG ?= latest\nREGISTRY ?= your-registry # Replace with your container registry\n\nall: build\n\n# ====================================================================================\n# Go Commands\n# ====================================================================================\n\n## build: Build the Go binaries\nbuild:\n\t@echo \"--\u003e Building Go binaries...\"\n\t@go build -o bin/ingest ./cmd/ingest\n\t@go build -o bin/consumer ./cmd/consumer\n\n## test: Run unit tests with coverage\ntest:\n\t@echo \"--\u003e Running tests...\"\n\t@go test -v -race -cover ./...\n\n## lint: Run the linter\nlint:\n\t@echo \"--\u003e Linting code...\"\n\t@if ! command -v golangci-lint \u0026\u003e /dev/null; then \\\n\t\techo \"golangci-lint not found. Please install it: https://golangci-lint.run/usage/install/\"; \\\n\t\texit 1; \\\n\tfi\n\t@golangci-lint run\n\n## clean: Clean up build artifacts\nclean:\n\t@echo \"--\u003e Cleaning up...\"\n\t@rm -rf ./bin ./coverage.out\n\n# ====================================================================================\n# Docker Commands\n# ====================================================================================\n\n## docker-build: Build Docker images for ingest and consumer services\ndocker-build:\n\t@echo \"--\u003e Building Docker images...\"\n\t@docker build -f Dockerfile.ingest -t ${INGEST_IMG_NAME}:${TAG} .\n\t@docker build -f Dockerfile.consumer -t ${CONSUMER_IMG_NAME}:${TAG} .\n\n## docker-push: Push Docker images to the registry\ndocker-push:\n\t@echo \"--\u003e Pushing Docker images to ${REGISTRY}...\"\n\t@docker tag ${INGEST_IMG_NAME}:${TAG} ${REGISTRY}/${INGEST_IMG_NAME}:${TAG}\n\t@docker tag ${CONSUMER_IMG_NAME}:${TAG} ${REGISTRY}/${CONSUMER_IMG_NAME}:${TAG}\n\t@docker push ${REGISTRY}/${INGEST_IMG_NAME}:${TAG}\n\t@docker push ${REGISTRY}/${CONSUMER_IMG_NAME}:${TAG}\n\n# ====================================================================================\n# Docker Compose Commands\n# ====================================================================================\n\n## compose-up: Start all services using Docker Compose\ncompose-up:\n\t@echo \"--\u003e Starting services with Docker Compose...\"\n\t@docker-compose up -d --build\n\n## compose-down: Stop and remove all services from Docker Compose\ncompose-down:\n\t@echo \"--\u003e Stopping services with Docker Compose...\"\n\t@docker-compose down\n\n## compose-logs: Follow logs from all services\ncompose-logs:\n\t@echo \"--\u003e Following logs...\"\n\t@docker-compose logs -f\n\n# ====================================================================================\n# Help\n# ====================================================================================\n\n## help: Show this help message\nhelp:\n\t@echo \"Usage: make \u003ctarget\u003e\"\n\t@echo \"\"\n\t@echo \"Targets:\"\n\t@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = \":.*?## \"}; {printf \"\\033[36m%-20s\\033[0m %s\\n\", $$1, $$2}'\n\n.DEFAULT_GOAL := help\n",
  "cmd/consumer/main.go": "package main\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"fmt\"\n\t\"log\"\n\t\"log/slog\"\n\t\"os\"\n\t\"os/signal\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/google/uuid\"\n\t\"github.com/redis/go-redis/v9\"\n\t\"github.com/user/log-ingestor/internal/adapter/repository/postgres\"\n\tredisrepo \"github.com/user/log-ingestor/internal/adapter/repository/redis\"\n\t\"github.com/user/log-ingestor/internal/pkg/config\"\n\t\"github.com/user/log-ingestor/internal/pkg/logger\"\n\t\"github.com/user/log-ingestor/internal/usecase\"\n\n\t_ \"github.com/lib/pq\"\n)\n\nconst (\n\tconsumerGroup    = \"log-processors\"\n\tprocessingInterval = 1 * time.Second\n)\n\nfunc main() {\n\tcfg, err := config.Load()\n\tif err != nil {\n\t\tlog.Fatalf(\"failed to load config: %v\", err)\n\t}\n\n\tappLogger := logger.New(cfg.LogLevel)\n\thostname, _ := os.Hostname()\n\tconsumerName := fmt.Sprintf(\"%s-%s\", hostname, uuid.NewString()[:8])\n\tappLogger = appLogger.With(\"consumer_name\", consumerName)\n\n\t// Database Connection\n\tdb, err := sql.Open(\"postgres\", cfg.PostgresURL)\n\tif err != nil {\n\t\tlog.Fatalf(\"failed to connect to postgres: %v\", err)\n\t}\n\tdefer db.Close()\n\n\t// Redis Client\n\tredisClient := redis.NewClient(\u0026redis.Options{\n\t\tAddr: cfg.RedisAddr,\n\t})\n\tif err := redisClient.Ping(context.Background()).Err(); err != nil {\n\t\tlog.Fatalf(\"failed to connect to redis: %v\", err)\n\t}\n\n\t// Repositories\n\t// The consumer doesn't need a WAL, so we pass nil.\n\tredisBufferRepo, err := redisrepo.NewLogRepository(redisClient, appLogger, consumerGroup, consumerName, cfg.RedisDLQStream, nil)\n\tif err != nil {\n\t\tlog.Fatalf(\"failed to create redis buffer repository: %v\", err)\n\t}\n\tpgSinkRepo := postgres.NewLogRepository(db, appLogger)\n\n\t// Use Case\n\tprocessUseCase := usecase.NewProcessLogsUseCase(\n\t\tredisBufferRepo,\n\t\tpgSinkRepo,\n\t\tappLogger,\n\t\tconsumerGroup,\n\t\tconsumerName,\n\t\tcfg.ConsumerRetryCount,\n\t\tcfg.ConsumerRetryBackoff,\n\t)\n\n\t// Graceful Shutdown Context\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tstop := make(chan os.Signal, 1)\n\tsignal.Notify(stop, syscall.SIGINT, syscall.SIGTERM)\n\n\tgo func() {\n\t\t\u003c-stop\n\t\tappLogger.Info(\"Shutting down consumer...\")\n\t\tcancel()\n\t}()\n\n\tappLogger.Info(\"Starting consumer worker\")\n\tticker := time.NewTicker(processingInterval)\n\tdefer ticker.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase \u003c-ctx.Done():\n\t\t\tappLogger.Info(\"Consumer stopped\")\n\t\t\treturn\n\t\tcase \u003c-ticker.C:\n\t\t\tprocessed, err := processUseCase.ProcessBatch(ctx)\n\t\t\tif err != nil {\n\t\t\t\tappLogger.Error(\"Error processing batch\", \"error\", err)\n\t\t\t}\n\t\t\tif processed \u003e 0 {\n\t\t\t\tappLogger.Debug(\"Processed batch\", \"count\", processed)\n\t\t\t}\n\t\t}\n\t}\n}",
  "cmd/ingest/main.go": "package main\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"errors\"\n\t\"log/slog\"\n\t\"net/http\"\n\t\"os\"\n\t\"os/signal\"\n\t\"strings\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n\t\"github.com/redis/go-redis/v9\"\n\n\t\"github.com/user/log-ingestor/internal/adapter/api\"\n\t\"github.com/user/log-ingestor/internal/adapter/api/handler\"\n\t\"github.com/user/log-ingestor/internal/adapter/api/middleware\"\n\t\"github.com/user/log-ingestor/internal/adapter/metrics\"\n\t\"github.com/user/log-ingestor/internal/adapter/pii\"\n\t\"github.com/user/log-ingestor/internal/adapter/repository/postgres\"\n\tredisrepo \"github.com/user/log-ingestor/internal/adapter/repository/redis\"\n\t\"github.com/user/log-ingestor/internal/adapter/repository/wal\"\n\t\"github.com/user/log-ingestor/internal/pkg/config\"\n\t\"github.com/user/log-ingestor/internal/pkg/logger\"\n\t\"github.com/user/log-ingestor/internal/usecase\"\n\n\t_ \"github.com/lib/pq\" // Keep for postgres driver\n)\n\nfunc main() {\n\tcfg, err := config.Load()\n\tif err != nil {\n\t\tslog.Error(\"failed to load config\", \"error\", err)\n\t\tos.Exit(1)\n\t}\n\n\tlogger := logger.New(cfg.LogLevel)\n\tslog.SetDefault(logger)\n\n\tm := metrics.NewIngestMetrics()\n\n\t// --- Start Admin and Metrics Server ---\n\tadminMux := http.NewServeMux()\n\tadminMux.Handle(\"/metrics\", promhttp.Handler())\n\n\tadminServer := \u0026http.Server{\n\t\tAddr:    \":9091\",\n\t\tHandler: adminMux,\n\t}\n\n\tgo func() {\n\t\tlogger.Info(\"starting admin \u0026 metrics server\", \"addr\", adminServer.Addr)\n\t\tif err := adminServer.ListenAndServe(); err != nil \u0026\u0026 err != http.ErrServerClosed {\n\t\t\tlogger.Error(\"admin \u0026 metrics server failed\", \"error\", err)\n\t\t}\n\t}()\n\n\t// --- Graceful Shutdown Context ---\n\tctx, stop := signal.NotifyContext(context.Background(), syscall.SIGINT, syscall.SIGTERM)\n\tdefer stop()\n\n\t// --- Database and Redis Connections ---\n\tdb, err := sql.Open(\"postgres\", cfg.PostgresURL)\n\tif err != nil {\n\t\tlogger.Error(\"failed to connect to postgres\", \"error\", err)\n\t\tos.Exit(1)\n\t}\n\tdefer db.Close()\n\n\tredisOpts, err := redis.ParseURL(cfg.RedisAddr)\n\tif err != nil {\n\t\tlogger.Error(\"failed to parse redis url\", \"error\", err)\n\t\tos.Exit(1)\n\t}\n\tredisClient := redis.NewClient(redisOpts)\n\tif err := redisClient.Ping(ctx).Err(); err != nil {\n\t\tlogger.Warn(\"could not connect to redis, will proceed in WAL-only mode\", \"error\", err)\n\t}\n\n\t// --- Initialize Repositories ---\n\twalRepo, err := wal.NewWALRepository(cfg.WALPath, cfg.WALSegmentSize, cfg.WALMaxDiskSize, logger)\n\tif err != nil {\n\t\tlogger.Error(\"failed to initialize WAL repository\", \"error\", err)\n\t\tos.Exit(1)\n\t}\n\tdefer walRepo.Close()\n\n\tapiKeyRepo := postgres.NewAPIKeyRepository(db, logger, cfg.APIKeyCacheTTL, m)\n\tredisLogRepo, err := redisrepo.NewLogRepository(redisClient, logger, \"log-processors\", \"ingest-service\", cfg.RedisDLQStream, walRepo, m)\n\tif err != nil \u0026\u0026 !errors.Is(err, redisrepo.ErrRedisNotAvailable) {\n\t\tlogger.Error(\"failed to initialize redis log repository\", \"error\", err)\n\t\tos.Exit(1)\n\t}\n\n\t// Start Redis health check and WAL replay loop\n\tgo redisLogRepo.StartHealthCheck(ctx, 5*time.Second)\n\n\t// --- Initialize Admin API ---\n\tredisAdminRepo := redisrepo.NewAdminRepository(redisClient, logger)\n\tadminUseCase := usecase.NewAdminStreamUseCase(redisAdminRepo)\n\tadminRouter := api.NewAdminRouter(adminUseCase, logger)\n\tadminMux.Handle(\"/\", adminRouter) // Mount admin router at the root of the admin server\n\n\t// --- Initialize Use Cases and Services ---\n\tpiiRedactor := pii.NewRedactor(strings.Split(cfg.PIIRedactionFields, \",\"), logger)\n\tingestUseCase := usecase.NewIngestLogUseCase(redisLogRepo, piiRedactor, logger)\n\n\t// --- Initialize SSE Broker ---\n\tsseBroker := handler.NewSSEBroker(ctx, logger)\n\n\t// --- Initialize Ingest Server ---\n\tingestRouter := api.NewRouter(cfg, logger, apiKeyRepo, ingestUseCase, m, sseBroker)\n\tingestServer := \u0026http.Server{\n\t\tAddr:         cfg.IngestServerAddr,\n\t\tHandler:      middleware.Logging(logger)(ingestRouter),\n\t\tReadTimeout:  5 * time.Second,\n\t\tWriteTimeout: 10 * time.Second,\n\t\tIdleTimeout:  15 * time.Second,\n\t}\n\n\tgo func() {\n\t\tlogger.Info(\"starting ingest server\", \"addr\", ingestServer.Addr)\n\t\tif err := ingestServer.ListenAndServe(); err != nil \u0026\u0026 err != http.ErrServerClosed {\n\t\t\tlogger.Error(\"ingest server failed\", \"error\", err)\n\t\t\tstop() // Trigger shutdown on server error\n\t\t}\n\t}()\n\n\t// --- Wait for shutdown signal ---\n\t\u003c-ctx.Done()\n\tlogger.Info(\"shutting down servers...\")\n\n\tshutdownCtx, cancelShutdown := context.WithTimeout(context.Background(), 10*time.Second)\n\tdefer cancelShutdown()\n\n\tif err := adminServer.Shutdown(shutdownCtx); err != nil {\n\t\tlogger.Error(\"admin server shutdown failed\", \"error\", err)\n\t}\n\tif err := ingestServer.Shutdown(shutdownCtx); err != nil {\n\t\tlogger.Error(\"ingest server shutdown failed\", \"error\", err)\n\t}\n\n\tlogger.Info(\"servers shut down gracefully\")\n}\n",
  "docker-compose.yml": "version: '3.8'\n\nservices:\n  postgres:\n    image: postgres:16-alpine\n    container_name: log_ingestor_postgres\n    environment:\n      POSTGRES_DB: ${POSTGRES_DB:-logs}\n      POSTGRES_USER: ${POSTGRES_USER:-user}\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-password}\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres-data:/var/lib/postgresql/data\n      - ./scripts/init.sql:/docker-entrypoint-initdb.d/init.sql\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U ${POSTGRES_USER:-user} -d ${POSTGRES_DB:-logs}\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n    restart: unless-stopped\n\n  redis:\n    image: redis:7-alpine\n    container_name: log_ingestor_redis\n    command: redis-server --appendonly yes\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - redis-data:/data\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n    restart: unless-stopped\n\n  ingestor:\n    build:\n      context: .\n      dockerfile: Dockerfile.ingest\n    container_name: log_ingestor_ingest\n    ports:\n      - \"8080:8080\" # Ingest API\n      - \"9091:9091\" # Admin/Metrics API\n    env_file:\n      - .env\n    volumes:\n      - ./wal:/app/wal # Mount WAL directory for persistence and inspection\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n    restart: unless-stopped\n\n  consumer:\n    build:\n      context: .\n      dockerfile: Dockerfile.consumer\n    container_name: log_ingestor_consumer\n    deploy:\n      replicas: 2 # Example of scaling consumers\n    env_file:\n      - .env\n    depends_on:\n      postgres:\n        condition: service_healthy\n      redis:\n        condition: service_healthy\n    restart: unless-stopped\n\nvolumes:\n  postgres-data:\n  redis-data:\n```",
  "frontend/css/main.css": ":root {\n    --bg-color: #1a1a1a;\n    --card-bg-color: #2a2a2a;\n    --text-color: #e0e0e0;\n    --primary-color: #00bfff;\n    --green: #4caf50;\n    --yellow: #ffeb3b;\n    --red: #f44336;\n    --border-color: #444;\n}\n\nbody {\n    font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif;\n    background-color: var(--bg-color);\n    color: var(--text-color);\n    margin: 0;\n    padding: 2rem;\n    display: flex;\n    justify-content: center;\n    align-items: flex-start;\n    min-height: 100vh;\n}\n\n.container {\n    width: 100%;\n    max-width: 800px;\n}\n\nheader {\n    display: flex;\n    justify-content: space-between;\n    align-items: center;\n    margin-bottom: 2rem;\n    border-bottom: 1px solid var(--border-color);\n    padding-bottom: 1rem;\n}\n\nh1 {\n    color: var(--primary-color);\n    margin: 0;\n}\n\n.status {\n    display: flex;\n    align-items: center;\n    gap: 0.5rem;\n}\n\n#status-indicator {\n    width: 12px;\n    height: 12px;\n    border-radius: 50%;\n    transition: background-color 0.3s ease;\n}\n\n#status-indicator.connected { background-color: var(--green); }\n#status-indicator.reconnecting { background-color: var(--yellow); }\n#status-indicator.error { background-color: var(--red); }\n\n.metric-card {\n    background-color: var(--card-bg-color);\n    border: 1px solid var(--border-color);\n    border-radius: 8px;\n    padding: 1.5rem 2rem;\n}\n\n.metric-card h2 {\n    margin-top: 0;\n    border-bottom: 1px solid var(--border-color);\n    padding-bottom: 0.5rem;\n    margin-bottom: 1rem;\n}\n\n.metric-card p {\n    font-size: 2rem;\n    margin: 0 0 1.5rem 0;\n    text-align: center;\n}\n\n#ingest-rate {\n    font-weight: bold;\n    color: var(--primary-color);\n}\n\n#sparkline-container {\n    height: 100px;\n    background-color: #111;\n    border-radius: 4px;\n    padding: 10px;\n    border: 1px solid var(--border-color);\n}\n\n#sparkline {\n    width: 100%;\n    height: 100%;\n    display: flex;\n    align-items: flex-end;\n    justify-content: flex-end;\n    gap: 2px;\n    overflow: hidden;\n}\n\n.sparkline-bar {\n    flex: 1 1 0;\n    background-color: var(--primary-color);\n    opacity: 0.7;\n    transition: height 0.2s ease-out;\n}\n",
  "frontend/index.html": "\u003c!DOCTYPE html\u003e\n\u003chtml lang=\"en\"\u003e\n\u003chead\u003e\n    \u003cmeta charset=\"UTF-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e\n    \u003ctitle\u003eLog Ingestor - Live Metrics\u003c/title\u003e\n    \u003clink rel=\"stylesheet\" href=\"css/main.css\"\u003e\n\u003c/head\u003e\n\u003cbody\u003e\n    \u003cdiv class=\"container\"\u003e\n        \u003cheader\u003e\n            \u003ch1\u003eLog Ingestor Metrics\u003c/h1\u003e\n            \u003cdiv class=\"status\"\u003e\n                \u003cspan\u003eStatus:\u003c/span\u003e\n                \u003cdiv id=\"status-indicator\" class=\"reconnecting\"\u003e\u003c/div\u003e\n                \u003cspan id=\"status-text\"\u003eConnecting...\u003c/span\u003e\n            \u003c/div\u003e\n        \u003c/header\u003e\n        \u003cmain\u003e\n            \u003cdiv class=\"metric-card\"\u003e\n                \u003ch2\u003eLive Ingest Rate\u003c/h2\u003e\n                \u003cp\u003e\u003cspan id=\"ingest-rate\"\u003e0.00\u003c/span\u003e events/sec\u003c/p\u003e\n                \u003cdiv id=\"sparkline-container\"\u003e\n                    \u003cdiv id=\"sparkline\"\u003e\u003c/div\u003e\n                \u003c/div\u003e\n            \u003c/div\u003e\n        \u003c/main\u003e\n    \u003c/div\u003e\n\n    \u003cscript src=\"js/ui.js\"\u003e\u003c/script\u003e\n    \u003cscript src=\"js/sse_client.js\"\u003e\u003c/script\u003e\n    \u003cscript src=\"js/app.js\"\u003e\u003c/script\u003e\n\u003c/body\u003e\n\u003c/html\u003e\n",
  "frontend/js/app.js": "(function() {\n    const MAX_SPARKLINE_POINTS = 60; // Show last 60 seconds of data\n    let dataPoints = [];\n\n    const handleNewData = (data) =\u003e {\n        const rate = data.Rate || 0;\n        \n        // Update data for sparkline\n        dataPoints.push(rate);\n        if (dataPoints.length \u003e MAX_SPARKLINE_POINTS) {\n            dataPoints.shift(); // Keep the array size fixed\n        }\n\n        // Update UI\n        ui.updateIngestRate(rate);\n        ui.drawSparkline(dataPoints, MAX_SPARKLINE_POINTS);\n    };\n\n    const handleStatusChange = (status, message) =\u003e {\n        ui.updateStatus(status, message);\n    };\n\n    document.addEventListener('DOMContentLoaded', () =\u003e {\n        // Initialize UI\n        ui.updateStatus('reconnecting', 'Initializing...');\n        ui.drawSparkline(dataPoints, MAX_SPARKLINE_POINTS);\n\n        // Connect to SSE endpoint\n        sseClient.connect('/events', {\n            onData: handleNewData,\n            onStatusChange: handleStatusChange,\n        });\n    });\n})();\n",
  "frontend/js/sse_client.js": "const sseClient = {\n    connect: (url, { onData, onStatusChange }) =\u003e {\n        const eventSource = new EventSource(url);\n\n        eventSource.onopen = () =\u003e {\n            onStatusChange('connected', 'Live connection established.');\n        };\n\n        eventSource.onmessage = (event) =\u003e {\n            try {\n                const data = JSON.parse(event.data);\n                onData(data);\n            } catch (error) {\n                console.error('Failed to parse SSE message:', error);\n            }\n        };\n\n        eventSource.onerror = () =\u003e {\n            // The EventSource API automatically handles reconnection.\n            // This event fires when the connection is lost and before a reconnection attempt.\n            onStatusChange('reconnecting', 'Connection lost. Reconnecting...');\n        };\n\n        return eventSource; // Return instance to allow for closing it later if needed\n    }\n};\n",
  "frontend/js/ui.js": "const ui = {\n    elements: {\n        ingestRate: document.getElementById('ingest-rate'),\n        statusIndicator: document.getElementById('status-indicator'),\n        statusText: document.getElementById('status-text'),\n        sparkline: document.getElementById('sparkline'),\n    },\n\n    updateIngestRate: (rate) =\u003e {\n        if (ui.elements.ingestRate) {\n            ui.elements.ingestRate.textContent = rate.toFixed(2);\n        }\n    },\n\n    updateStatus: (status, message) =\u003e {\n        if (ui.elements.statusIndicator \u0026\u0026 ui.elements.statusText) {\n            ui.elements.statusIndicator.className = status; // 'connected', 'reconnecting', 'error'\n            ui.elements.statusText.textContent = message;\n        }\n    },\n\n    drawSparkline: (dataPoints, maxPoints) =\u003e {\n        if (!ui.elements.sparkline) return;\n\n        const sparkline = ui.elements.sparkline;\n        sparkline.innerHTML = ''; // Clear previous bars\n\n        const maxValue = Math.max(...dataPoints, 1); // Avoid division by zero\n\n        // Pad the data with zeros if there are fewer points than maxPoints\n        const paddedData = Array(maxPoints - dataPoints.length).fill(0).concat(dataPoints);\n\n        paddedData.forEach(value =\u003e {\n            const bar = document.createElement('div');\n            bar.className = 'sparkline-bar';\n            const height = (value / maxValue) * 100;\n            bar.style.height = `${Math.max(height, 1)}%`; // Ensure a minimum visible height\n            sparkline.appendChild(bar);\n        });\n    }\n};\n```",
  "go.mod": "module github.com/user/log-ingestor\n\ngo 1.21\n\nrequire (\n\tgithub.com/caarlos0/env/v10 v10.0.0\n\tgithub.com/google/uuid v1.6.0\n\tgithub.com/joho/godotenv v1.5.1\n\tgithub.com/lib/pq v1.10.9\n\tgithub.com/prometheus/client_golang v1.19.0\n\tgithub.com/redis/go-redis/v9 v9.5.1\n)\n\nrequire (\n\tgithub.com/beorn7/perks v1.0.1 // indirect\n\tgithub.com/cespare/xxhash/v2 v2.2.0 // indirect\n\tgithub.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f // indirect\n\tgithub.com/prometheus/client_model v0.5.0 // indirect\n\tgithub.com/prometheus/common v0.48.0 // indirect\n\tgithub.com/prometheus/procfs v0.12.0 // indirect\n\tgolang.org/x/sys v0.16.0 // indirect\n\tgoogle.golang.org/protobuf v1.32.0 // indirect\n)\n",
  "internal/adapter/api/admin_router.go": "package api\n\nimport (\n\t\"log/slog\"\n\t\"net/http\"\n\n\t\"github.com/user/log-ingestor/internal/adapter/api/handler\"\n\t\"github.comcom/user/log-ingestor/internal/usecase\"\n)\n\n// NewAdminRouter creates and configures the HTTP router for admin operations.\n// Note: This router uses path patterns (e.g., \"/{streamName}/\") available in Go 1.22+.\nfunc NewAdminRouter(adminUseCase *usecase.AdminStreamUseCase, logger *slog.Logger) http.Handler {\n\tmux := http.NewServeMux()\n\tadminHandler := handler.NewAdminHandler(adminUseCase, logger)\n\n\tmux.HandleFunc(\"GET /health\", adminHandler.HealthCheck)\n\n\t// Stream Info\n\tmux.HandleFunc(\"GET /admin/streams/{streamName}/groups\", adminHandler.GetGroupInfo)\n\tmux.HandleFunc(\"GET /admin/streams/{streamName}/groups/{groupName}/consumers\", adminHandler.GetConsumerInfo)\n\n\t// Pending Messages\n\tmux.HandleFunc(\"GET /admin/streams/{streamName}/groups/{groupName}/pending\", adminHandler.GetPendingSummary)\n\tmux.HandleFunc(\"GET /admin/streams/{streamName}/groups/{groupName}/pending/messages\", adminHandler.GetPendingMessages)\n\n\t// Stream Operations\n\tmux.HandleFunc(\"POST /admin/streams/{streamName}/groups/{groupName}/claim\", adminHandler.ClaimMessages)\n\tmux.HandleFunc(\"POST /admin/streams/{streamName}/groups/{groupName}/ack\", adminHandler.AcknowledgeMessages)\n\tmux.HandleFunc(\"POST /admin/streams/{streamName}/trim\", adminHandler.TrimStream)\n\n\treturn mux\n}\n",
  "internal/adapter/api/handler/admin_handler.go": "package handler\n\nimport (\n\t\"encoding/json\"\n\t\"log/slog\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"time\"\n\n\t\"github.com/user/log-ingestor/internal/usecase\"\n)\n\n// AdminHandler handles HTTP requests for stream administration.\ntype AdminHandler struct {\n\tuc     *usecase.AdminStreamUseCase\n\tlogger *slog.Logger\n}\n\n// NewAdminHandler creates a new AdminHandler.\nfunc NewAdminHandler(uc *usecase.AdminStreamUseCase, logger *slog.Logger) *AdminHandler {\n\treturn \u0026AdminHandler{uc: uc, logger: logger}\n}\n\n// HealthCheck is a simple health check endpoint.\nfunc (h *AdminHandler) HealthCheck(w http.ResponseWriter, r *http.Request) {\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\tw.WriteHeader(http.StatusOK)\n\tjson.NewEncoder(w).Encode(map[string]string{\"status\": \"ok\"})\n}\n\n// GetGroupInfo handles requests to get consumer group info.\n// GET /admin/streams/{streamName}/groups\nfunc (h *AdminHandler) GetGroupInfo(w http.ResponseWriter, r *http.Request) {\n\tstreamName := r.PathValue(\"streamName\")\n\tif streamName == \"\" {\n\t\thttp.Error(w, \"streamName is required\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tgroups, err := h.uc.GetGroupInfo(r.Context(), streamName)\n\tif err != nil {\n\t\th.logger.Error(\"failed to get group info\", \"error\", err)\n\t\thttp.Error(w, \"Internal server error\", http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\th.respondWithJSON(w, http.StatusOK, groups)\n}\n\n// GetConsumerInfo handles requests to get consumer info for a group.\n// GET /admin/streams/{streamName}/groups/{groupName}/consumers\nfunc (h *AdminHandler) GetConsumerInfo(w http.ResponseWriter, r *http.Request) {\n\tstreamName := r.PathValue(\"streamName\")\n\tgroupName := r.PathValue(\"groupName\")\n\n\tconsumers, err := h.uc.GetConsumerInfo(r.Context(), streamName, groupName)\n\tif err != nil {\n\t\th.logger.Error(\"failed to get consumer info\", \"error\", err)\n\t\thttp.Error(w, \"Internal server error\", http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\th.respondWithJSON(w, http.StatusOK, consumers)\n}\n\n// GetPendingSummary handles requests to get a summary of pending messages.\n// GET /admin/streams/{streamName}/groups/{groupName}/pending\nfunc (h *AdminHandler) GetPendingSummary(w http.ResponseWriter, r *http.Request) {\n\tstreamName := r.PathValue(\"streamName\")\n\tgroupName := r.PathValue(\"groupName\")\n\n\tsummary, err := h.uc.GetPendingSummary(r.Context(), streamName, groupName)\n\tif err != nil {\n\t\th.logger.Error(\"failed to get pending summary\", \"error\", err)\n\t\thttp.Error(w, \"Internal server error\", http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\th.respondWithJSON(w, http.StatusOK, summary)\n}\n\n// GetPendingMessages handles requests to list pending messages.\n// GET /admin/streams/{streamName}/groups/{groupName}/pending/messages?consumer={consumerName}\u0026start={startID}\u0026count={count}\nfunc (h *AdminHandler) GetPendingMessages(w http.ResponseWriter, r *http.Request) {\n\tstreamName := r.PathValue(\"streamName\")\n\tgroupName := r.PathValue(\"groupName\")\n\tconsumerName := r.URL.Query().Get(\"consumer\")\n\tstartID := r.URL.Query().Get(\"start\")\n\tcountStr := r.URL.Query().Get(\"count\")\n\n\tvar count int64 = 100 // default\n\tif countStr != \"\" {\n\t\tvar err error\n\t\tcount, err = strconv.ParseInt(countStr, 10, 64)\n\t\tif err != nil {\n\t\t\thttp.Error(w, \"invalid count parameter\", http.StatusBadRequest)\n\t\t\treturn\n\t\t}\n\t}\n\n\tmessages, err := h.uc.GetPendingMessages(r.Context(), streamName, groupName, consumerName, startID, count)\n\tif err != nil {\n\t\th.logger.Error(\"failed to get pending messages\", \"error\", err)\n\t\thttp.Error(w, \"Internal server error\", http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\th.respondWithJSON(w, http.StatusOK, messages)\n}\n\n// ClaimMessages handles requests to claim pending messages.\n// POST /admin/streams/{streamName}/groups/{groupName}/claim\nfunc (h *AdminHandler) ClaimMessages(w http.ResponseWriter, r *http.Request) {\n\tstreamName := r.PathValue(\"streamName\")\n\tgroupName := r.PathValue(\"groupName\")\n\n\tvar payload struct {\n\t\tConsumer    string   `json:\"consumer\"`\n\t\tMinIdleTime string   `json:\"min_idle_time\"`\n\t\tMessageIDs  []string `json:\"message_ids\"`\n\t}\n\tif err := json.NewDecoder(r.Body).Decode(\u0026payload); err != nil {\n\t\thttp.Error(w, \"invalid request body\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tminIdle, err := time.ParseDuration(payload.MinIdleTime)\n\tif err != nil {\n\t\thttp.Error(w, \"invalid min_idle_time format\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tclaimed, err := h.uc.ClaimMessages(r.Context(), streamName, groupName, payload.Consumer, minIdle, payload.MessageIDs)\n\tif err != nil {\n\t\th.logger.Error(\"failed to claim messages\", \"error\", err)\n\t\thttp.Error(w, \"Internal server error\", http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\th.respondWithJSON(w, http.StatusOK, claimed)\n}\n\n// AcknowledgeMessages handles requests to acknowledge messages.\n// POST /admin/streams/{streamName}/groups/{groupName}/ack\nfunc (h *AdminHandler) AcknowledgeMessages(w http.ResponseWriter, r *http.Request) {\n\tstreamName := r.PathValue(\"streamName\")\n\tgroupName := r.PathValue(\"groupName\")\n\n\tvar payload struct {\n\t\tMessageIDs []string `json:\"message_ids\"`\n\t}\n\tif err := json.NewDecoder(r.Body).Decode(\u0026payload); err != nil {\n\t\thttp.Error(w, \"invalid request body\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tif len(payload.MessageIDs) == 0 {\n\t\thttp.Error(w, \"message_ids cannot be empty\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tcount, err := h.uc.AcknowledgeMessages(r.Context(), streamName, groupName, payload.MessageIDs...)\n\tif err != nil {\n\t\th.logger.Error(\"failed to acknowledge messages\", \"error\", err)\n\t\thttp.Error(w, \"Internal server error\", http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\th.respondWithJSON(w, http.StatusOK, map[string]int64{\"acknowledged\": count})\n}\n\n// TrimStream handles requests to trim a stream.\n// POST /admin/streams/{streamName}/trim\nfunc (h *AdminHandler) TrimStream(w http.ResponseWriter, r *http.Request) {\n\tstreamName := r.PathValue(\"streamName\")\n\n\tvar payload struct {\n\t\tMaxLen int64 `json:\"maxlen\"`\n\t}\n\tif err := json.NewDecoder(r.Body).Decode(\u0026payload); err != nil {\n\t\thttp.Error(w, \"invalid request body\", http.StatusBadRequest)\n\t\treturn\n\t}\n\tif payload.MaxLen \u003c= 0 {\n\t\thttp.Error(w, \"maxlen must be a positive integer\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\ttrimmedCount, err := h.uc.TrimStream(r.Context(), streamName, payload.MaxLen)\n\tif err != nil {\n\t\th.logger.Error(\"failed to trim stream\", \"error\", err)\n\t\thttp.Error(w, \"Internal server error\", http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\th.respondWithJSON(w, http.StatusOK, map[string]int64{\"trimmed\": trimmedCount})\n}\n\nfunc (h *AdminHandler) respondWithJSON(w http.ResponseWriter, code int, payload interface{}) {\n\tresponse, err := json.Marshal(payload)\n\tif err != nil {\n\t\th.logger.Error(\"failed to marshal JSON response\", \"error\", err)\n\t\tw.WriteHeader(http.StatusInternalServerError)\n\t\tw.Write([]byte(\"Internal Server Error\"))\n\t\treturn\n\t}\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\tw.WriteHeader(code)\n\tw.Write(response)\n}\n",
  "internal/adapter/api/handler/ingest_handler.go": "package handler\n\nimport (\n\t\"bufio\"\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"io\"\n\t\"log/slog\"\n\t\"net/http\"\n\t\"strings\"\n\n\t\"github.com/user/log-ingestor/internal/adapter/metrics\"\n\t\"github.com/user/log-ingestor/internal/domain\"\n\t\"github.com/user/log-ingestor/internal/usecase\"\n)\n\nconst (\n\tcontentTypeJSON   = \"application/json\"\n\tcontentTypeNDJSON = \"application/x-ndjson\"\n)\n\n// IngestHandler handles HTTP requests for log ingestion.\ntype IngestHandler struct {\n\tuseCase      *usecase.IngestLogUseCase\n\tlogger       *slog.Logger\n\tmaxEventSize int64\n\tmetrics      *metrics.IngestMetrics\n\tsseBroker    *SSEBroker\n}\n\n// NewIngestHandler creates a new IngestHandler.\nfunc NewIngestHandler(uc *usecase.IngestLogUseCase, logger *slog.Logger, maxEventSize int64, m *metrics.IngestMetrics, sse *SSEBroker) *IngestHandler {\n\treturn \u0026IngestHandler{\n\t\tuseCase:      uc,\n\t\tlogger:       logger,\n\t\tmaxEventSize: maxEventSize,\n\t\tmetrics:      m,\n\t\tsseBroker:    sse,\n\t}\n}\n\n// ServeHTTP processes incoming log ingestion requests.\nfunc (h *IngestHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n\tif r.Method != http.MethodPost {\n\t\thttp.Error(w, \"Only POST method is allowed\", http.StatusMethodNotAllowed)\n\t\treturn\n\t}\n\n\th.metrics.BytesTotal.Add(float64(r.ContentLength))\n\t// Enforce max body size\n\tr.Body = http.MaxBytesReader(w, r.Body, h.maxEventSize)\n\n\tcontentType := r.Header.Get(\"Content-Type\")\n\tvar err error\n\n\tswitch {\n\tcase strings.HasPrefix(contentType, contentTypeJSON):\n\t\terr = h.handleSingleJSON(r.Context(), r.Body)\n\tcase strings.HasPrefix(contentType, contentTypeNDJSON):\n\t\terr = h.handleNDJSON(r.Context(), r.Body)\n\tdefault:\n\t\th.metrics.EventsTotal.WithLabelValues(\"error_media_type\").Inc()\n\t\thttp.Error(w, \"Unsupported Content-Type. Use application/json or application/x-ndjson.\", http.StatusUnsupportedMediaType)\n\t\treturn\n\t}\n\n\tif err != nil {\n\t\tvar maxBytesErr *http.MaxBytesError\n\t\tif errors.As(err, \u0026maxBytesErr) {\n\t\t\th.metrics.EventsTotal.WithLabelValues(\"error_size\").Inc()\n\t\t\thttp.Error(w, \"Payload too large\", http.StatusRequestEntityTooLarge)\n\t\t} else {\n\t\t\th.logger.Error(\"Failed to process request\", \"error\", err)\n\t\t\thttp.Error(w, \"Failed to process request\", http.StatusBadRequest)\n\t\t}\n\t\treturn\n\t}\n\n\tw.WriteHeader(http.StatusAccepted)\n}\n\nfunc (h *IngestHandler) handleSingleJSON(ctx context.Context, body io.Reader) error {\n\tbodyBytes, err := io.ReadAll(body)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tvar event domain.LogEvent\n\tif err := json.Unmarshal(bodyBytes, \u0026event); err != nil {\n\t\th.metrics.EventsTotal.WithLabelValues(\"error_parse\").Inc()\n\t\treturn err\n\t}\n\tevent.RawEvent = bodyBytes\n\n\tif err := h.useCase.Ingest(ctx, \u0026event); err != nil {\n\t\th.metrics.EventsTotal.WithLabelValues(\"error_buffer\").Inc()\n\t\treturn err\n\t}\n\n\th.metrics.EventsTotal.WithLabelValues(\"accepted\").Inc()\n\th.sseBroker.ReportEvents(1)\n\treturn nil\n}\n\nfunc (h *IngestHandler) handleNDJSON(ctx context.Context, body io.Reader) error {\n\tscanner := bufio.NewScanner(body)\n\tvar processedCount int\n\tfor scanner.Scan() {\n\t\tline := scanner.Bytes()\n\t\tif len(line) == 0 {\n\t\t\tcontinue\n\t\t}\n\n\t\tvar event domain.LogEvent\n\t\tif err := json.Unmarshal(line, \u0026event); err != nil {\n\t\t\th.logger.Warn(\"Failed to unmarshal NDJSON line, skipping\", \"error\", err)\n\t\t\th.metrics.EventsTotal.WithLabelValues(\"error_parse\").Inc()\n\t\t\tcontinue\n\t\t}\n\t\tevent.RawEvent = line\n\n\t\tif err := h.useCase.Ingest(ctx, \u0026event); err != nil {\n\t\t\th.logger.Error(\"Failed to ingest event from NDJSON stream\", \"error\", err)\n\t\t\th.metrics.EventsTotal.WithLabelValues(\"error_buffer\").Inc()\n\t\t\t// Continue processing other lines\n\t\t\tcontinue\n\t\t}\n\t\tprocessedCount++\n\t}\n\n\tif err := scanner.Err(); err != nil {\n\t\treturn err\n\t}\n\n\tif processedCount \u003e 0 {\n\t\th.metrics.EventsTotal.WithLabelValues(\"accepted\").Add(float64(processedCount))\n\t\th.sseBroker.ReportEvents(processedCount)\n\t}\n\n\treturn nil\n}\n",
  "internal/adapter/api/handler/ingest_handler_test.go": "package handler\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"io\"\n\t\"log/slog\"\n\t\"net/http\"\n\t\"net/http/httptest\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/user/log-ingestor/internal/adapter/metrics\"\n\t\"github.com/user/log-ingestor/internal/domain\"\n\t\"github.com/user/log-ingestor/internal/usecase\"\n)\n\n// MockIngestUseCase is a mock implementation of the IngestLogUseCase.\ntype MockIngestUseCase struct {\n\tIngestFunc func(ctx context.Context, event *domain.LogEvent) error\n}\n\nfunc (m *MockIngestUseCase) Ingest(ctx context.Context, event *domain.LogEvent) error {\n\tif m.IngestFunc != nil {\n\t\treturn m.IngestFunc(ctx, event)\n\t}\n\treturn nil\n}\n\nfunc TestIngestHandler(t *testing.T) {\n\tlogger := slog.New(slog.NewTextHandler(io.Discard, nil))\n\tmockMetrics := metrics.NewIngestMetrics()\n\tmockSSEBroker := NewSSEBroker(context.Background(), logger)\n\n\ttests := []struct {\n\t\tname           string\n\t\tmethod         string\n\t\tcontentType    string\n\t\tbody           string\n\t\tmockIngestErr  error\n\t\texpectedStatus int\n\t\texpectedBody   string\n\t}{\n\t\t{\n\t\t\tname:           \"Valid Single JSON\",\n\t\t\tmethod:         http.MethodPost,\n\t\t\tcontentType:    \"application/json\",\n\t\t\tbody:           `{\"message\": \"hello\"}`,\n\t\t\texpectedStatus: http.StatusAccepted,\n\t\t\texpectedBody:   \"\",\n\t\t},\n\t\t{\n\t\t\tname:           \"Valid NDJSON\",\n\t\t\tmethod:         http.MethodPost,\n\t\t\tcontentType:    \"application/x-ndjson\",\n\t\t\tbody:           `{\"message\": \"line 1\"}` + \"\\n\" + `{\"message\": \"line 2\"}`,\n\t\t\texpectedStatus: http.StatusAccepted,\n\t\t\texpectedBody:   \"\",\n\t\t},\n\t\t{\n\t\t\tname:           \"Invalid Method\",\n\t\t\tmethod:         http.MethodGet,\n\t\t\tcontentType:    \"application/json\",\n\t\t\tbody:           `{}`,\n\t\t\texpectedStatus: http.StatusMethodNotAllowed,\n\t\t\texpectedBody:   \"Method Not Allowed\\n\",\n\t\t},\n\t\t{\n\t\t\tname:           \"Unsupported Content-Type\",\n\t\t\tmethod:         http.MethodPost,\n\t\t\tcontentType:    \"text/plain\",\n\t\t\tbody:           `hello`,\n\t\t\texpectedStatus: http.StatusUnsupportedMediaType,\n\t\t\texpectedBody:   \"Unsupported Media Type: text/plain\\n\",\n\t\t},\n\t\t{\n\t\t\tname:           \"Bad JSON\",\n\t\t\tmethod:         http.MethodPost,\n\t\t\tcontentType:    \"application/json\",\n\t\t\tbody:           `{\"message\": \"hello\"`,\n\t\t\texpectedStatus: http.StatusBadRequest,\n\t\t\texpectedBody:   \"Bad Request: Failed to decode JSON\\n\",\n\t\t},\n\t\t{\n\t\t\tname:           \"Bad NDJSON line\",\n\t\t\tmethod:         http.MethodPost,\n\t\t\tcontentType:    \"application/x-ndjson\",\n\t\t\tbody:           `{\"message\": \"line 1\"}` + \"\\n\" + `{\"message\": \"bad`,\n\t\t\texpectedStatus: http.StatusBadRequest,\n\t\t\texpectedBody:   \"Bad Request: Failed to decode NDJSON line\\n\",\n\t\t},\n\t\t{\n\t\t\tname:           \"Ingest Use Case Error\",\n\t\t\tmethod:         http.MethodPost,\n\t\t\tcontentType:    \"application/json\",\n\t\t\tbody:           `{\"message\": \"fail me\"}`,\n\t\t\tmockIngestErr:  errors.New(\"internal buffer error\"),\n\t\t\texpectedStatus: http.StatusInternalServerError,\n\t\t\texpectedBody:   \"Internal Server Error\\n\",\n\t\t},\n\t\t{\n\t\t\tname:           \"Payload Too Large\",\n\t\t\tmethod:         http.MethodPost,\n\t\t\tcontentType:    \"application/json\",\n\t\t\tbody:           `{\"message\": \"this payload is definitely too large for the test limit\"}`,\n\t\t\texpectedStatus: http.StatusRequestEntityTooLarge,\n\t\t\texpectedBody:   \"http: request body too large\\n\",\n\t\t},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tmockUseCase := \u0026MockIngestUseCase{\n\t\t\t\tIngestFunc: func(ctx context.Context, event *domain.LogEvent) error {\n\t\t\t\t\treturn tt.mockIngestErr\n\t\t\t\t},\n\t\t\t}\n\t\t\t// Use a small max size for the \"Payload Too Large\" test\n\t\t\tmaxSize := int64(1024)\n\t\t\tif tt.name == \"Payload Too Large\" {\n\t\t\t\tmaxSize = 50\n\t\t\t}\n\n\t\t\thandler := NewIngestHandler(mockUseCase, logger, maxSize, mockMetrics, mockSSEBroker)\n\n\t\t\treq := httptest.NewRequest(tt.method, \"/ingest\", bytes.NewBufferString(tt.body))\n\t\t\treq.Header.Set(\"Content-Type\", tt.contentType)\n\t\t\trr := httptest.NewRecorder()\n\n\t\t\thandler.ServeHTTP(rr, req)\n\n\t\t\tif status := rr.Code; status != tt.expectedStatus {\n\t\t\t\tt.Errorf(\"handler returned wrong status code: got %v want %v\", status, tt.expectedStatus)\n\t\t\t}\n\n\t\t\tif body := rr.Body.String(); body != tt.expectedBody {\n\t\t\t\tt.Errorf(\"handler returned unexpected body: got %q want %q\", body, tt.expectedBody)\n\t\t\t}\n\t\t})\n\t}\n}\n",
  "internal/adapter/api/handler/sse_handler.go": "```go\npackage handler\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net/http\"\n\t\"sync\"\n\t\"time\"\n)\n\n// SSEMessage defines the structure of the message sent to the frontend.\ntype SSEMessage struct {\n\tRate float64 `json:\"rate\"`\n}\n\n// SSEBroker manages SSE client connections and broadcasts messages.\ntype SSEBroker struct {\n\tlogger       *slog.Logger\n\tclients      map[chan []byte]struct{}\n\tmu           sync.RWMutex\n\teventCounter chan int\n}\n\n// NewSSEBroker creates a new SSEBroker and starts its processing loop.\nfunc NewSSEBroker(ctx context.Context, logger *slog.Logger) *SSEBroker {\n\tbroker := \u0026SSEBroker{\n\t\tlogger:       logger,\n\t\tclients:      make(map[chan []byte]struct{}),\n\t\teventCounter: make(chan int, 1000), // Buffered channel\n\t}\n\tgo broker.run(ctx)\n\treturn broker\n}\n\n// ServeHTTP handles new client connections for the SSE stream.\nfunc (b *SSEBroker) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n\tflusher, ok := w.(http.Flusher)\n\tif !ok {\n\t\thttp.Error(w, \"Streaming unsupported!\", http.StatusInternalServerError)\n\t\treturn\n\t}\n\n\tw.Header().Set(\"Content-Type\", \"text/event-stream\")\n\tw.Header().Set(\"Cache-Control\", \"no-cache\")\n\tw.Header().Set(\"Connection\", \"keep-alive\")\n\tw.Header().Set(\"Access-Control-Allow-Origin\", \"*\")\n\n\tmessageChan := make(chan []byte)\n\tb.addClient(messageChan)\n\tdefer b.removeClient(messageChan)\n\n\tctx := r.Context()\n\tfor {\n\t\tselect {\n\t\tcase \u003c-ctx.Done():\n\t\t\treturn\n\t\tcase msg, ok := \u003c-messageChan:\n\t\t\tif !ok {\n\t\t\t\treturn // Channel was closed\n\t\t\t}\n\t\t\tfmt.Fprintf(w, \"data: %s\\n\\n\", msg)\n\t\t\tflusher.Flush()\n\t\t}\n\t}\n}\n\n// ReportEvents is called by the ingest handler to report the number of events processed.\nfunc (b *SSEBroker) ReportEvents(count int) {\n\tselect {\n\tcase b.eventCounter \u003c- count:\n\tdefault:\n\t\t// Channel is full, drop the report to avoid blocking the ingest path.\n\t\tb.logger.Warn(\"SSE event counter channel is full, dropping report.\")\n\t}\n}\n\nfunc (b *SSEBroker) addClient(client chan []byte) {\n\tb.mu.Lock()\n\tdefer b.mu.Unlock()\n\tb.clients[client] = struct{}{}\n\tb.logger.Info(\"SSE client connected\")\n}\n\nfunc (b *SSEBroker) removeClient(client chan []byte) {\n\tb.mu.Lock()\n\tdefer b.mu.Unlock()\n\tif _, ok := b.clients[client]; ok {\n\t\tdelete(b.clients, client)\n\t\tclose(client)\n\t\tb.logger.Info(\"SSE client disconnected\")\n\t}\n}\n\nfunc (b *SSEBroker) broadcast(msg []byte) {\n\tb.mu.RLock()\n\tdefer b.mu.RUnlock()\n\tfor client := range b.clients {\n\t\tselect {\n\t\tcase client \u003c- msg:\n\t\tdefault:\n\t\t\t// Client channel is full, maybe slow client.\n\t\t\t// We don't block the broadcast for one slow client.\n\t\t}\n\t}\n}\n\n// run is the main processing loop for the broker.\nfunc (b *SSEBroker) run(ctx context.Context) {\n\tticker := time.NewTicker(1 * time.Second)\n\tdefer ticker.Stop()\n\n\tvar currentCount int\n\tlastTimestamp := time.Now()\n\n\tfor {\n\t\tselect {\n\t\tcase \u003c-ctx.Done():\n\t\t\treturn\n\t\tcase count := \u003c-b.eventCounter:\n\t\t\tcurrentCount += count\n\t\tcase \u003c-ticker.C:\n\t\t\tnow := time.Now()\n\t\t\tduration := now.Sub(lastTimestamp).Seconds()\n\t\t\trate := 0.0\n\t\t\tif duration \u003e 0 {\n\t\t\t\trate = float64(currentCount) / duration\n\t\t\t}\n\n\t\t\tmsg := SSEMessage{Rate: rate}\n\t\t\tjsonData, err := json.Marshal(msg)\n\t\t\tif err != nil {\n\t\t\t\tb.logger.Error(\"Failed to marshal SSE message\", \"error\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tb.broadcast(jsonData)\n\n\t\t\t// Reset for the next interval\n\t\t\tlastTimestamp = now\n\t\t\tcurrentCount = 0\n\t\t}\n\t}\n}\n```\n",
  "internal/adapter/api/middleware/auth.go": "package middleware\n\nimport (\n\t\"log/slog\"\n\t\"net/http\"\n\n\t\"github.com/user/log-ingestor/internal/domain\"\n)\n\nconst APIKeyHeader = \"X-API-Key\"\n\n// Auth is a middleware factory that returns a new authentication middleware.\n// It checks for a valid API key in the X-API-Key header.\nfunc Auth(repo domain.APIKeyRepository, logger *slog.Logger) func(http.Handler) http.Handler {\n\treturn func(next http.Handler) http.Handler {\n\t\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\t\tapiKey := r.Header.Get(APIKeyHeader)\n\t\t\tif apiKey == \"\" {\n\t\t\t\tlogger.Warn(\"API key missing from request\", \"remote_addr\", r.RemoteAddr)\n\t\t\t\thttp.Error(w, \"Unauthorized: API key required\", http.StatusUnauthorized)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tisValid, err := repo.IsValid(r.Context(), apiKey)\n\t\t\tif err != nil {\n\t\t\t\tlogger.Error(\"failed to validate API key\", \"error\", err)\n\t\t\t\thttp.Error(w, \"Internal Server Error\", http.StatusInternalServerError)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tif !isValid {\n\t\t\t\tlogger.Warn(\"invalid API key provided\", \"remote_addr\", r.RemoteAddr)\n\t\t\t\thttp.Error(w, \"Unauthorized: Invalid API key\", http.StatusUnauthorized)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tnext.ServeHTTP(w, r)\n\t\t})\n\t}\n}\n",
  "internal/adapter/api/router.go": "package api\n\nimport (\n\t\"log/slog\"\n\t\"net/http\"\n\n\t\"github.com/user/log-ingestor/internal/adapter/api/handler\"\n\t\"github.com/user/log-ingestor/internal/adapter/api/middleware\"\n\t\"github.com/user/log-ingestor/internal/adapter/metrics\"\n\t\"github.com/user/log-ingestor/internal/domain\"\n\t\"github.com/user/log-ingestor/internal/pkg/config\"\n\t\"github.com/user/log-ingestor/internal/usecase\"\n)\n\n// NewRouter creates and configures the main HTTP router for the ingest service.\nfunc NewRouter(\n\tcfg *config.Config,\n\tlogger *slog.Logger,\n\tapiKeyRepo domain.APIKeyRepository,\n\tingestUseCase *usecase.IngestLogUseCase,\n\tm *metrics.IngestMetrics,\n\tsseBroker *handler.SSEBroker,\n) http.Handler {\n\tmux := http.NewServeMux()\n\n\tauthMiddleware := middleware.Auth(apiKeyRepo, logger)\n\n\t// Ingest Handler\n\tingestHandler := handler.NewIngestHandler(ingestUseCase, logger, cfg.MaxEventSize, m, sseBroker)\n\n\t// Routes\n\tmux.Handle(\"POST /ingest\", authMiddleware(ingestHandler))\n\tmux.Handle(\"/events\", sseBroker)\n\n\t// Health check\n\tmux.HandleFunc(\"/health\", func(w http.ResponseWriter, r *http.Request) {\n\t\tw.WriteHeader(http.StatusOK)\n\t\tw.Write([]byte(\"OK\"))\n\t})\n\n\treturn mux\n}\n",
  "internal/adapter/metrics/metrics.go": "```go\npackage metrics\n\nimport (\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n)\n\n// IngestMetrics holds all Prometheus metrics for the ingest service.\ntype IngestMetrics struct {\n\tEventsTotal       *prometheus.CounterVec\n\tBytesTotal        prometheus.Counter\n\tWALActive         prometheus.Gauge\n\tAPIKeyCacheHits   prometheus.Counter\n\tAPIKeyCacheMisses prometheus.Counter\n}\n\n// NewIngestMetrics initializes and registers the Prometheus metrics.\nfunc NewIngestMetrics() *IngestMetrics {\n\treturn \u0026IngestMetrics{\n\t\tEventsTotal: promauto.NewCounterVec(prometheus.CounterOpts{\n\t\t\tNamespace: \"log_ingestor\",\n\t\t\tSubsystem: \"ingest\",\n\t\t\tName:      \"events_total\",\n\t\t\tHelp:      \"Total number of ingested events by status.\",\n\t\t}, []string{\"status\"}), // status: accepted, error_parse, error_size, error_buffer, error_media_type\n\t\tBytesTotal: promauto.NewCounter(prometheus.CounterOpts{\n\t\t\tNamespace: \"log_ingestor\",\n\t\t\tSubsystem: \"ingest\",\n\t\t\tName:      \"bytes_total\",\n\t\t\tHelp:      \"Total number of bytes ingested.\",\n\t\t}),\n\t\tWALActive: promauto.NewGauge(prometheus.GaugeOpts{\n\t\t\tNamespace: \"log_ingestor\",\n\t\t\tSubsystem: \"ingest\",\n\t\t\tName:      \"wal_active_gauge\",\n\t\t\tHelp:      \"Indicates if the Write-Ahead Log is currently active (1 for active, 0 for inactive).\",\n\t\t}),\n\t\tAPIKeyCacheHits: promauto.NewCounter(prometheus.CounterOpts{\n\t\t\tNamespace: \"log_ingestor\",\n\t\t\tSubsystem: \"auth\",\n\t\t\tName:      \"api_key_cache_hits_total\",\n\t\t\tHelp:      \"Total number of API key cache hits.\",\n\t\t}),\n\t\tAPIKeyCacheMisses: promauto.NewCounter(prometheus.CounterOpts{\n\t\t\tNamespace: \"log_ingestor\",\n\t\t\tSubsystem: \"auth\",\n\t\t\tName:      \"api_key_cache_misses_total\",\n\t\t\tHelp:      \"Total number of API key cache misses.\",\n\t\t}),\n\t}\n}\n```\n",
  "internal/adapter/pii/redactor.go": "package pii\n\nimport (\n\t\"encoding/json\"\n\t\"log/slog\"\n\n\t\"github.com/user/log-ingestor/internal/domain\"\n)\n\nconst RedactedPlaceholder = \"[REDACTED]\"\n\n// Redactor is responsible for redacting sensitive information from log events.\ntype Redactor struct {\n\tfieldsToRedact map[string]struct{} // Use a map for O(1) lookups\n\tlogger         *slog.Logger\n}\n\n// NewRedactor creates a new Redactor instance with a given set of fields to redact.\nfunc NewRedactor(fields []string, logger *slog.Logger) *Redactor {\n\tfieldSet := make(map[string]struct{}, len(fields))\n\tfor _, field := range fields {\n\t\tfieldSet[field] = struct{}{}\n\t}\n\treturn \u0026Redactor{\n\t\tfieldsToRedact: fieldSet,\n\t\tlogger:         logger,\n\t}\n}\n\n// Redact modifies the LogEvent in place to remove PII from its metadata.\n// It returns an error if JSON processing fails.\nfunc (r *Redactor) Redact(event *domain.LogEvent) error {\n\tif len(r.fieldsToRedact) == 0 || len(event.Metadata) == 0 {\n\t\treturn nil\n\t}\n\n\tvar metadata map[string]interface{}\n\tif err := json.Unmarshal(event.Metadata, \u0026metadata); err != nil {\n\t\tr.logger.Warn(\"failed to unmarshal metadata for PII redaction\", \"error\", err, \"event_id\", event.ID)\n\t\t// We can't process it, so we leave it as is.\n\t\treturn err\n\t}\n\n\tredacted := false\n\tfor field := range r.fieldsToRedact {\n\t\tif _, ok := metadata[field]; ok {\n\t\t\tmetadata[field] = RedactedPlaceholder\n\t\t\tredacted = true\n\t\t}\n\t}\n\n\tif redacted {\n\t\tevent.PIIRedacted = true\n\t\tmodifiedMetadata, err := json.Marshal(metadata)\n\t\tif err != nil {\n\t\t\tr.logger.Error(\"failed to marshal modified metadata after PII redaction\", \"error\", err, \"event_id\", event.ID)\n\t\t\t// This is a more serious internal error.\n\t\t\treturn err\n\t\t}\n\t\tevent.Metadata = modifiedMetadata\n\t}\n\n\treturn nil\n}\n",
  "internal/adapter/pii/redactor_test.go": "package pii\n\nimport (\n\t\"encoding/json\"\n\t\"log/slog\"\n\t\"testing\"\n\n\t\"github.com/user/log-ingestor/internal/domain\"\n)\n\nfunc TestRedactor(t *testing.T) {\n\tlogger := slog.New(slog.NewJSONHandler(nil, nil))\n\tredactor := NewRedactor([]string{\"email\", \"ssn\"}, logger)\n\n\ttests := []struct {\n\t\tname              string\n\t\tinputMetadata     string\n\t\texpectedMetadata  string\n\t\texpectRedacted    bool\n\t\texpectErr         bool\n\t}{\n\t\t{\n\t\t\tname:              \"Redact single field\",\n\t\t\tinputMetadata:     `{\"email\": \"test@example.com\", \"user_id\": 123}`,\n\t\t\texpectedMetadata:  `{\"email\":\"[REDACTED]\",\"user_id\":123}`,\n\t\t\texpectRedacted:    true,\n\t\t\texpectErr:         false,\n\t\t},\n\t\t{\n\t\t\tname:              \"Redact multiple fields\",\n\t\t\tinputMetadata:     `{\"email\": \"test@example.com\", \"ssn\": \"000-00-0000\"}`,\n\t\t\texpectedMetadata:  `{\"email\":\"[REDACTED]\",\"ssn\":\"[REDACTED]\"}`,\n\t\t\texpectRedacted:    true,\n\t\t\texpectErr:         false,\n\t\t},\n\t\t{\n\t\t\tname:              \"No fields to redact\",\n\t\t\tinputMetadata:     `{\"user_id\": 123, \"action\": \"login\"}`,\n\t\t\texpectedMetadata:  `{\"action\":\"login\",\"user_id\":123}`,\n\t\t\texpectRedacted:    false,\n\t\t\texpectErr:         false,\n\t\t},\n\t\t{\n\t\t\tname:              \"Empty metadata\",\n\t\t\tinputMetadata:     `{}`,\n\t\t\texpectedMetadata:  `{}`,\n\t\t\texpectRedacted:    false,\n\t\t\texpectErr:         false,\n\t\t},\n\t\t{\n\t\t\tname:              \"Invalid JSON metadata\",\n\t\t\tinputMetadata:     `{\"email\": \"test@example.com\"`,\n\t\t\texpectedMetadata:  \"\",\n\t\t\texpectRedacted:    false,\n\t\t\texpectErr:         true,\n\t\t},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tevent := \u0026domain.LogEvent{\n\t\t\t\tMetadata: json.RawMessage(tt.inputMetadata),\n\t\t\t}\n\n\t\t\terr := redactor.Redact(event)\n\n\t\t\tif (err != nil) != tt.expectErr {\n\t\t\t\tt.Fatalf(\"Redact() error = %v, wantErr %v\", err, tt.expectErr)\n\t\t\t}\n\n\t\t\tif err != nil {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tif event.PIIRedacted != tt.expectRedacted {\n\t\t\t\tt.Errorf(\"event.PIIRedacted got = %v, want %v\", event.PIIRedacted, tt.expectRedacted)\n\t\t\t}\n\n\t\t\t// Unmarshal both to compare maps, avoiding key order issues\n\t\t\tvar expectedMap, actualMap map[string]interface{}\n\t\t\tif err := json.Unmarshal([]byte(tt.expectedMetadata), \u0026expectedMap); err != nil {\n\t\t\t\tt.Fatalf(\"failed to unmarshal expected metadata: %v\", err)\n\t\t\t}\n\t\t\tif err := json.Unmarshal(event.Metadata, \u0026actualMap); err != nil {\n\t\t\t\tt.Fatalf(\"failed to unmarshal actual metadata: %v\", err)\n\t\t\t}\n\n\t\t\tif len(expectedMap) != len(actualMap) {\n\t\t\t\tt.Errorf(\"metadata map length mismatch: got %d, want %d\", len(actualMap), len(expectedMap))\n\t\t\t}\n\t\t\tfor k, v := range expectedMap {\n\t\t\t\tif actualMap[k] != v {\n\t\t\t\t\tt.Errorf(\"metadata mismatch for key %s: got %v, want %v\", k, actualMap[k], v)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n",
  "internal/adapter/repository/postgres/apikey_repository.go": "package postgres\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"log/slog\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/user/log-ingestor/internal/adapter/metrics\"\n)\n\ntype cacheEntry struct {\n\tisValid   bool\n\texpiresAt time.Time\n}\n\n// APIKeyRepository implements the domain.APIKeyRepository interface using PostgreSQL\n// as the source of truth and an in-memory, time-based cache.\ntype APIKeyRepository struct {\n\tdb       *sql.DB\n\tlogger   *slog.Logger\n\tcache    map[string]cacheEntry\n\tmu       sync.RWMutex\n\tcacheTTL time.Duration\n\tmetrics  *metrics.IngestMetrics\n}\n\n// NewAPIKeyRepository creates a new instance of the PostgreSQL API key repository.\nfunc NewAPIKeyRepository(db *sql.DB, logger *slog.Logger, cacheTTL time.Duration, m *metrics.IngestMetrics) *APIKeyRepository {\n\treturn \u0026APIKeyRepository{\n\t\tdb:       db,\n\t\tlogger:   logger,\n\t\tcache:    make(map[string]cacheEntry),\n\t\tcacheTTL: cacheTTL,\n\t\tmetrics:  m,\n\t}\n}\n\n// IsValid checks if an API key is valid. It first checks a local cache and falls\n// back to the database if the key is not found or the cache entry has expired.\nfunc (r *APIKeyRepository) IsValid(ctx context.Context, key string) (bool, error) {\n\t// 1. Check cache with a read lock\n\tr.mu.RLock()\n\tentry, found := r.cache[key]\n\tr.mu.RUnlock()\n\n\tif found \u0026\u0026 time.Now().Before(entry.expiresAt) {\n\t\tif r.metrics != nil {\n\t\t\tr.metrics.APIKeyCacheHits.Inc()\n\t\t}\n\t\treturn entry.isValid, nil\n\t}\n\n\t// 2. Cache miss or expired, query DB and update cache with a write lock\n\tif r.metrics != nil {\n\t\tr.metrics.APIKeyCacheMisses.Inc()\n\t}\n\n\tr.mu.Lock()\n\tdefer r.mu.Unlock()\n\n\t// Double-check cache in case another goroutine populated it while waiting for the lock\n\tentry, found = r.cache[key]\n\tif found \u0026\u0026 time.Now().Before(entry.expiresAt) {\n\t\treturn entry.isValid, nil\n\t}\n\n\t// 3. Query the database\n\tvar isValid bool\n\t// A key is valid if it exists, is active, and has not expired.\n\tquery := `SELECT EXISTS(SELECT 1 FROM api_keys WHERE key = $1 AND is_active = true AND (expires_at IS NULL OR expires_at \u003e NOW()))`\n\terr := r.db.QueryRowContext(ctx, query, key).Scan(\u0026isValid)\n\tif err != nil {\n\t\tr.logger.Error(\"failed to validate API key in database\", \"error\", err)\n\t\t// Don't cache errors, let the next request retry from the DB\n\t\treturn false, err\n\t}\n\n\t// 4. Update cache\n\tr.cache[key] = cacheEntry{\n\t\tisValid:   isValid,\n\t\texpiresAt: time.Now().Add(r.cacheTTL),\n\t}\n\n\treturn isValid, nil\n}\n",
  "internal/adapter/repository/postgres/log_repository.go": "package postgres\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"errors\"\n\t\"log/slog\"\n\n\t\"github.com/lib/pq\"\n\t\"github.com/user/log-ingestor/internal/domain\"\n)\n\nconst logsTableName = \"logs\"\n\n// LogRepository implements the sink part of the domain.LogRepository interface for PostgreSQL.\ntype LogRepository struct {\n\tdb     *sql.DB\n\tlogger *slog.Logger\n}\n\n// NewLogRepository creates a new PostgreSQL log repository.\nfunc NewLogRepository(db *sql.DB, logger *slog.Logger) *LogRepository {\n\treturn \u0026LogRepository{db: db, logger: logger}\n}\n\n// WriteLogBatch writes a batch of log events to PostgreSQL using the COPY protocol for high performance.\n// It uses an ON CONFLICT clause to perform an upsert, ensuring idempotency based on event_id.\nfunc (r *LogRepository) WriteLogBatch(ctx context.Context, events []domain.LogEvent) error {\n\tif len(events) == 0 {\n\t\treturn nil\n\t}\n\n\ttxn, err := r.db.BeginTx(ctx, nil)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer txn.Rollback() // Rollback is a no-op if Commit() is called\n\n\t// Use a temporary table to stage the data, then merge into the main table.\n\t// This is a common pattern for high-performance, idempotent bulk inserts.\n\ttempTableName := \"logs_temp_import\"\n\t_, err = txn.ExecContext(ctx, `CREATE TEMP TABLE `+tempTableName+` (LIKE logs INCLUDING DEFAULTS) ON COMMIT DROP;`)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tstmt, err := txn.Prepare(pq.CopyIn(tempTableName, \"event_id\", \"received_at\", \"event_time\", \"source\", \"level\", \"message\", \"metadata\"))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tfor _, event := range events {\n\t\t_, err = stmt.ExecContext(ctx, event.ID, event.ReceivedAt, event.EventTime, event.Source, event.Level, event.Message, event.Metadata)\n\t\tif err != nil {\n\t\t\t// Close the statement to avoid connection issues\n\t\t\t_ = stmt.Close()\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif err := stmt.Close(); err != nil {\n\t\treturn err\n\t}\n\n\t// Upsert from the temp table into the main table\n\tupsertQuery := `\n\t\tINSERT INTO logs (event_id, received_at, event_time, source, level, message, metadata)\n\t\tSELECT event_id, received_at, event_time, source, level, message, metadata FROM ` + tempTableName + `\n\t\tON CONFLICT (event_id) DO UPDATE SET\n\t\t\treceived_at = EXCLUDED.received_at,\n\t\t\tevent_time = EXCLUDED.event_time,\n\t\t\tsource = EXCLUDED.source,\n\t\t\tlevel = EXCLUDED.level,\n\t\t\tmessage = EXCLUDED.message,\n\t\t\tmetadata = EXCLUDED.metadata;\n\t`\n\t_, err = txn.ExecContext(ctx, upsertQuery)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn txn.Commit()\n}\n\n// The following methods are not implemented for the PostgreSQL sink repository.\nvar errNotImplemented = errors.New(\"method not implemented for this repository type\")\n\nfunc (r *LogRepository) BufferLog(ctx context.Context, event domain.LogEvent) error {\n\treturn errNotImplemented\n}\n\nfunc (r *LogRepository) ReadLogBatch(ctx context.Context, group, consumer string, count int) ([]domain.LogEvent, error) {\n\treturn nil, errNotImplemented\n}\n\nfunc (r *LogRepository) AcknowledgeLogs(ctx context.Context, group string, messageIDs ...string) error {\n\treturn errNotImplemented\n}\n",
  "internal/adapter/repository/redis/admin_repository.go": "package redis\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"time\"\n\n\t\"github.com/redis/go-redis/v9\"\n\t\"github.com/user/log-ingestor/internal/domain\"\n)\n\n// AdminRepository implements the domain.StreamAdminRepository interface for Redis.\ntype AdminRepository struct {\n\tclient *redis.Client\n\tlogger *slog.Logger\n}\n\n// NewAdminRepository creates a new Redis admin repository.\nfunc NewAdminRepository(client *redis.Client, logger *slog.Logger) *AdminRepository {\n\treturn \u0026AdminRepository{\n\t\tclient: client,\n\t\tlogger: logger,\n\t}\n}\n\n// GetGroupInfo retrieves information about all consumer groups for a given stream.\nfunc (r *AdminRepository) GetGroupInfo(ctx context.Context, stream string) ([]domain.ConsumerGroupInfo, error) {\n\tgroups, err := r.client.XInfoGroups(ctx, stream).Result()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to get group info for stream %s: %w\", stream, err)\n\t}\n\n\tresult := make([]domain.ConsumerGroupInfo, len(groups))\n\tfor i, g := range groups {\n\t\tresult[i] = domain.ConsumerGroupInfo{\n\t\t\tName:            g.Name,\n\t\t\tConsumers:       g.Consumers,\n\t\t\tPending:         g.Pending,\n\t\t\tLastDeliveredID: g.LastDeliveredID,\n\t\t}\n\t}\n\treturn result, nil\n}\n\n// GetConsumerInfo retrieves information about consumers in a specific group.\nfunc (r *AdminRepository) GetConsumerInfo(ctx context.Context, stream, group string) ([]domain.ConsumerInfo, error) {\n\tconsumers, err := r.client.XInfoConsumers(ctx, stream, group).Result()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to get consumer info for stream %s, group %s: %w\", stream, group, err)\n\t}\n\n\tresult := make([]domain.ConsumerInfo, len(consumers))\n\tfor i, c := range consumers {\n\t\tresult[i] = domain.ConsumerInfo{\n\t\t\tName:    c.Name,\n\t\t\tPending: c.Pending,\n\t\t\tIdle:    time.Duration(c.Idle) * time.Millisecond,\n\t\t}\n\t}\n\treturn result, nil\n}\n\n// GetPendingSummary retrieves a summary of pending messages for a group.\nfunc (r *AdminRepository) GetPendingSummary(ctx context.Context, stream, group string) (*domain.PendingMessageSummary, error) {\n\tpending, err := r.client.XPending(ctx, stream, group).Result()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to get pending summary for stream %s, group %s: %w\", stream, group, err)\n\t}\n\n\tsummary := \u0026domain.PendingMessageSummary{\n\t\tTotal:          pending.Count,\n\t\tFirstMessageID: pending.Lower,\n\t\tLastMessageID:  pending.Upper,\n\t\tConsumerTotals: pending.Consumers,\n\t}\n\treturn summary, nil\n}\n\n// GetPendingMessages retrieves detailed information about pending messages.\nfunc (r *AdminRepository) GetPendingMessages(ctx context.Context, stream, group, consumer string, startID string, count int64) ([]domain.PendingMessageDetail, error) {\n\targs := \u0026redis.XPendingExtArgs{\n\t\tStream:   stream,\n\t\tGroup:    group,\n\t\tStart:    startID,\n\t\tEnd:      \"+\",\n\t\tCount:    count,\n\t\tConsumer: consumer,\n\t}\n\n\tmessages, err := r.client.XPendingExt(ctx, args).Result()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to get pending messages: %w\", err)\n\t}\n\n\tresult := make([]domain.PendingMessageDetail, len(messages))\n\tfor i, m := range messages {\n\t\tresult[i] = domain.PendingMessageDetail{\n\t\t\tID:         m.ID,\n\t\t\tConsumer:   m.Consumer,\n\t\t\tIdleTime:   m.Idle,\n\t\t\tRetryCount: m.RetryCount,\n\t\t}\n\t}\n\treturn result, nil\n}\n\n// ClaimMessages claims pending messages for a new consumer.\nfunc (r *AdminRepository) ClaimMessages(ctx context.Context, stream, group, consumer string, minIdleTime time.Duration, messageIDs []string) ([]domain.LogEvent, error) {\n\targs := \u0026redis.XClaimArgs{\n\t\tStream:   stream,\n\t\tGroup:    group,\n\t\tConsumer: consumer,\n\t\tMinIdle:  minIdleTime,\n\t\tMessages: messageIDs,\n\t}\n\n\tclaimedMessages, err := r.client.XClaim(ctx, args).Result()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to claim messages: %w\", err)\n\t}\n\n\tevents := make([]domain.LogEvent, 0, len(claimedMessages))\n\tfor _, msg := range claimedMessages {\n\t\tvar event domain.LogEvent\n\t\t// Assuming the message value is a JSON-encoded LogEvent\n\t\tif data, ok := msg.Values[\"data\"].(string); ok {\n\t\t\tif err := json.Unmarshal([]byte(data), \u0026event); err == nil {\n\t\t\t\tevent.StreamMessageID = msg.ID\n\t\t\t\tevents = append(events, event)\n\t\t\t} else {\n\t\t\t\tr.logger.Warn(\"failed to unmarshal claimed message into LogEvent\", \"messageID\", msg.ID, \"error\", err)\n\t\t\t}\n\t\t}\n\t}\n\treturn events, nil\n}\n\n// AcknowledgeMessages acknowledges messages in a stream.\nfunc (r *AdminRepository) AcknowledgeMessages(ctx context.Context, stream, group string, messageIDs ...string) (int64, error) {\n\tif len(messageIDs) == 0 {\n\t\treturn 0, errors.New(\"at least one message ID is required\")\n\t}\n\treturn r.client.XAck(ctx, stream, group, messageIDs...).Result()\n}\n\n// TrimStream trims a stream to a maximum length.\nfunc (r *AdminRepository) TrimStream(ctx context.Context, stream string, maxLen int64) (int64, error) {\n\treturn r.client.XTrimMaxLen(ctx, stream, maxLen).Result()\n}\n",
  "internal/adapter/repository/redis/log_repository.go": "package redis\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net\"\n\t\"strings\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/redis/go-redis/v9\"\n\t\"github.com/user/log-ingestor/internal/adapter/metrics\"\n\t\"github.com/user/log-ingestor/internal/domain\"\n)\n\nconst logStreamKey = \"log_events\"\n\nvar errNotImplemented = errors.New(\"method not implemented for this repository type\")\n\n// LogRepository implements domain.LogRepository for Redis Streams with WAL failover.\ntype LogRepository struct {\n\tclient       *redis.Client\n\tlogger       *slog.Logger\n\twal          domain.WALRepository\n\tdlqStreamKey string\n\tisAvailable  atomic.Bool\n\tmetrics      *metrics.IngestMetrics\n}\n\n// NewLogRepository creates a new Redis LogRepository.\n// The WAL is optional; pass nil if not needed (e.g., for consumers).\nfunc NewLogRepository(client *redis.Client, logger *slog.Logger, group, consumer, dlqStreamKey string, wal domain.WALRepository, m *metrics.IngestMetrics) (*LogRepository, error) {\n\trepo := \u0026LogRepository{\n\t\tclient:       client,\n\t\tlogger:       logger.With(\"component\", \"redis_repository\"),\n\t\twal:          wal,\n\t\tdlqStreamKey: dlqStreamKey,\n\t\tmetrics:      m,\n\t}\n\trepo.isAvailable.Store(true) // Assume available initially\n\n\tif err := repo.setupConsumerGroup(context.Background(), group); err != nil \u0026\u0026 !isRedisBusyGroupError(err) {\n\t\trepo.isAvailable.Store(false)\n\t\trepo.logger.Warn(\"Could not setup Redis consumer group on init, Redis might be down\", \"error\", err)\n\t\t// Don't return error, allow startup with WAL\n\t}\n\n\treturn repo, nil\n}\n\n// StartHealthCheck starts a background goroutine to monitor Redis connectivity and trigger WAL replay.\nfunc (r *LogRepository) StartHealthCheck(ctx context.Context, interval time.Duration) {\n\tif r.wal == nil {\n\t\tr.logger.Info(\"WAL is not configured, skipping health check routine.\")\n\t\treturn\n\t}\n\n\tticker := time.NewTicker(interval)\n\tdefer ticker.Stop()\n\n\tr.logger.Info(\"Starting Redis health check and WAL replayer\")\n\n\tfor {\n\t\tselect {\n\t\tcase \u003c-ctx.Done():\n\t\t\tr.logger.Info(\"Stopping Redis health check routine\")\n\t\t\treturn\n\t\tcase \u003c-ticker.C:\n\t\t\twasAvailable := r.isAvailable.Load()\n\t\t\terr := r.client.Ping(ctx).Err()\n\t\t\tisCurrentlyAvailable := err == nil\n\n\t\t\tif isCurrentlyAvailable \u0026\u0026 !wasAvailable {\n\t\t\t\tr.logger.Info(\"Redis connection recovered. Starting WAL replay.\")\n\t\t\t\tif r.metrics != nil {\n\t\t\t\t\tr.metrics.WALActive.Set(0)\n\t\t\t\t}\n\t\t\t\tif err := r.ReplayWAL(ctx); err != nil {\n\t\t\t\t\tr.logger.Error(\"Failed to replay WAL after Redis recovery\", \"error\", err)\n\t\t\t\t} else {\n\t\t\t\t\tr.isAvailable.Store(true)\n\t\t\t\t}\n\t\t\t} else if !isCurrentlyAvailable \u0026\u0026 wasAvailable {\n\t\t\t\tr.logger.Warn(\"Redis connection lost. Activating WAL.\", \"error\", err)\n\t\t\t\tr.isAvailable.Store(false)\n\t\t\t\tif r.metrics != nil {\n\t\t\t\t\tr.metrics.WALActive.Set(1)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n// ReplayWAL replays events from the WAL to Redis and truncates the WAL on success.\nfunc (r *LogRepository) ReplayWAL(ctx context.Context) error {\n\tr.logger.Info(\"Starting WAL replay to Redis\")\n\tvar replayedCount int\n\treplayHandler := func(event domain.LogEvent) error {\n\t\tif err := r.bufferLogToRedis(ctx, event); err != nil {\n\t\t\t// If we can't write to Redis during replay, something is very wrong.\n\t\t\tr.logger.Error(\"Failed to buffer event from WAL to Redis\", \"event_id\", event.ID, \"error\", err)\n\t\t\treturn err\n\t\t}\n\t\treplayedCount++\n\t\treturn nil\n\t}\n\n\tif err := r.wal.Replay(ctx, replayHandler); err != nil {\n\t\treturn fmt.Errorf(\"WAL replay failed: %w\", err)\n\t}\n\n\tr.logger.Info(\"WAL replay finished\", \"replayed_count\", replayedCount)\n\tif err := r.wal.Truncate(ctx); err != nil {\n\t\tr.logger.Error(\"Failed to truncate WAL after successful replay\", \"error\", err)\n\t\treturn fmt.Errorf(\"failed to truncate WAL after successful replay: %w\", err)\n\t}\n\n\tr.logger.Info(\"WAL truncated successfully\")\n\treturn nil\n}\n\nfunc (r *LogRepository) setupConsumerGroup(ctx context.Context, group string) error {\n\terr := r.client.XGroupCreateMkStream(ctx, logStreamKey, group, \"0\").Err()\n\tif err != nil \u0026\u0026 !isRedisBusyGroupError(err) {\n\t\treturn fmt.Errorf(\"failed to create consumer group: %w\", err)\n\t}\n\treturn nil\n}\n\n// BufferLog adds a log event to the Redis Stream, falling back to the WAL if Redis is unavailable.\nfunc (r *LogRepository) BufferLog(ctx context.Context, event domain.LogEvent) error {\n\tif !r.isAvailable.Load() {\n\t\tif r.wal == nil {\n\t\t\treturn errors.New(\"redis is unavailable and WAL is not configured\")\n\t\t}\n\t\tr.logger.Warn(\"Redis is unavailable, writing to WAL\", \"event_id\", event.ID)\n\t\tif r.metrics != nil {\n\t\t\tr.metrics.WALActive.Set(1)\n\t\t}\n\t\treturn r.wal.Write(ctx, event)\n\t}\n\n\terr := r.bufferLogToRedis(ctx, event)\n\tif err != nil {\n\t\tif isNetworkError(err) {\n\t\t\tif r.isAvailable.CompareAndSwap(true, false) {\n\t\t\t\tr.logger.Error(\"Redis connection lost during write\", \"error\", err)\n\t\t\t\tif r.metrics != nil {\n\t\t\t\t\tr.metrics.WALActive.Set(1)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif r.wal == nil {\n\t\t\t\treturn fmt.Errorf(\"redis became unavailable and WAL is not configured: %w\", err)\n\t\t\t}\n\t\t\tr.logger.Warn(\"Redis became unavailable, writing to WAL\", \"event_id\", event.ID)\n\t\t\treturn r.wal.Write(ctx, event)\n\t\t}\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc (r *LogRepository) bufferLogToRedis(ctx context.Context, event domain.LogEvent) error {\n\tpayload, err := json.Marshal(event)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to marshal log event: %w\", err)\n\t}\n\n\targs := \u0026redis.XAddArgs{\n\t\tStream: logStreamKey,\n\t\tValues: map[string]interface{}{\"payload\": payload},\n\t}\n\n\tif err := r.client.XAdd(ctx, args).Err(); err != nil {\n\t\treturn fmt.Errorf(\"failed to XADD to redis stream: %w\", err)\n\t}\n\treturn nil\n}\n\n// ReadLogBatch reads a batch of log events from the Redis Stream for a consumer group.\nfunc (r *LogRepository) ReadLogBatch(ctx context.Context, group, consumer string, count int) ([]domain.LogEvent, error) {\n\targs := \u0026redis.XReadGroupArgs{\n\t\tGroup:    group,\n\t\tConsumer: consumer,\n\t\tStreams:  []string{logStreamKey, \"\u003e\"},\n\t\tCount:    int64(count),\n\t\tBlock:    2 * time.Second,\n\t}\n\n\tstreams, err := r.client.XReadGroup(ctx, args).Result()\n\tif err != nil {\n\t\tif errors.Is(err, redis.Nil) {\n\t\t\treturn nil, nil\n\t\t}\n\t\treturn nil, fmt.Errorf(\"failed to XREADGROUP from redis: %w\", err)\n\t}\n\n\tif len(streams) == 0 || len(streams[0].Messages) == 0 {\n\t\treturn nil, nil\n\t}\n\n\tmessages := streams[0].Messages\n\tevents := make([]domain.LogEvent, 0, len(messages))\n\tfor _, msg := range messages {\n\t\tpayload, ok := msg.Values[\"payload\"].(string)\n\t\tif !ok {\n\t\t\tr.logger.Warn(\"Invalid message format in stream, skipping\", \"message_id\", msg.ID)\n\t\t\tcontinue\n\t\t}\n\n\t\tvar event domain.LogEvent\n\t\tif err := json.Unmarshal([]byte(payload), \u0026event); err != nil {\n\t\t\tr.logger.Warn(\"Failed to unmarshal log event from stream, skipping\", \"message_id\", msg.ID, \"error\", err)\n\t\t\tcontinue\n\t\t}\n\t\tevent.StreamMessageID = msg.ID\n\t\tevents = append(events, event)\n\t}\n\n\treturn events, nil\n}\n\n// AcknowledgeLogs acknowledges processed messages in the Redis Stream.\nfunc (r *LogRepository) AcknowledgeLogs(ctx context.Context, group string, messageIDs ...string) error {\n\tif len(messageIDs) == 0 {\n\t\treturn nil\n\t}\n\tif err := r.client.XAck(ctx, logStreamKey, group, messageIDs...).Err(); err != nil {\n\t\treturn fmt.Errorf(\"failed to XACK messages in redis: %w\", err)\n\t}\n\treturn nil\n}\n\n// MoveToDLQ moves a batch of events to the Dead-Letter Queue stream.\nfunc (r *LogRepository) MoveToDLQ(ctx context.Context, events []domain.LogEvent) error {\n\tif len(events) == 0 {\n\t\treturn nil\n\t}\n\n\tpipe := r.client.Pipeline()\n\tfor _, event := range events {\n\t\tpayload, err := json.Marshal(event)\n\t\tif err != nil {\n\t\t\tr.logger.Error(\"Failed to marshal event for DLQ\", \"event_id\", event.ID, \"error\", err)\n\t\t\tcontinue\n\t\t}\n\t\targs := \u0026redis.XAddArgs{\n\t\t\tStream: r.dlqStreamKey,\n\t\t\tValues: map[string]interface{}{\n\t\t\t\t\"payload\":           payload,\n\t\t\t\t\"original_event_id\": event.ID,\n\t\t\t\t\"original_stream\":   logStreamKey,\n\t\t\t\t// \"failed_at\":       time.Now().UTC().Format(time.RFC3339), // Removed as per attempted content\n\t\t\t},\n\t\t}\n\t\tpipe.XAdd(ctx, args)\n\t}\n\n\t_, err := pipe.Exec(ctx)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to execute DLQ pipeline: %w\", err)\n\t}\n\tr.logger.Warn(\"Moved events to DLQ\", \"count\", len(events))\n\treturn nil\n}\n\n// WriteLogBatch is not implemented for this repository.\nfunc (r *LogRepository) WriteLogBatch(ctx context.Context, events []domain.LogEvent) error {\n\treturn errNotImplemented\n}\n\nfunc isRedisBusyGroupError(err error) bool {\n\treturn err != nil \u0026\u0026 strings.HasPrefix(err.Error(), \"BUSYGROUP\")\n}\n\nfunc isNetworkError(err error) bool {\n\tif err == nil {\n\t\treturn false\n\t}\n\treturn errors.Is(err, net.ErrClosed) ||\n\t\terrors.Is(err, context.Canceled) ||\n\t\terrors.Is(err, context.DeadlineExceeded) ||\n\t\tstrings.Contains(err.Error(), \"i/o timeout\") ||\n\t\tstrings.Contains(err.Error(), \"connection refused\") ||\n\t\terrors.Is(err, redis.ErrClosed) // Keep original redis.ErrClosed\n}\n",
  "internal/adapter/repository/wal/wal_repository.go": "package wal\n\nimport (\n\t\"bufio\"\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/user/log-ingestor/internal/domain\"\n)\n\nconst (\n\tsegmentPrefix = \"segment-\"\n\tfilePerm      = 0644\n)\n\n// WALRepository implements a file-based Write-Ahead Log.\ntype WALRepository struct {\n\tdir            string\n\tmaxSegmentSize int64\n\tmaxTotalSize   int64\n\tlogger         *slog.Logger\n\n\tmu             sync.Mutex\n\tcurrentSegment *os.File\n\tcurrentSize    int64\n}\n\n// NewWALRepository creates a new WALRepository.\nfunc NewWALRepository(dir string, maxSegmentSize, maxTotalSize int64, logger *slog.Logger) (*WALRepository, error) {\n\tif err := os.MkdirAll(dir, 0755); err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to create WAL directory %s: %w\", dir, err)\n\t}\n\n\tw := \u0026WALRepository{\n\t\tdir:            dir,\n\t\tmaxSegmentSize: maxSegmentSize,\n\t\tmaxTotalSize:   maxTotalSize,\n\t\tlogger:         logger.With(\"component\", \"wal_repository\"),\n\t}\n\n\tif err := w.openLatestSegment(); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn w, nil\n}\n\n// Write appends an event to the current WAL segment.\nfunc (w *WALRepository) Write(ctx context.Context, event domain.LogEvent) error {\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\n\tdata, err := json.Marshal(event)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to marshal log event for WAL: %w\", err)\n\t}\n\tdata = append(data, '\\n')\n\n\tif w.currentSegment == nil {\n\t\tif err := w.rotate(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Check total size before writing\n\ttotalSize, err := w.calculateTotalSize()\n\tif err != nil {\n\t\tw.logger.Error(\"Failed to calculate total WAL size\", \"error\", err)\n\t\treturn fmt.Errorf(\"could not verify WAL disk space: %w\", err)\n\t}\n\tif totalSize+int64(len(data)) \u003e w.maxTotalSize {\n\t\treturn fmt.Errorf(\"WAL max total size exceeded (%d \u003e %d)\", totalSize, w.maxTotalSize)\n\t}\n\n\tn, err := w.currentSegment.Write(data)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to write to WAL segment: %w\", err)\n\t}\n\tw.currentSize += int64(n)\n\n\tif w.currentSize \u003e= w.maxSegmentSize {\n\t\tif err := w.rotate(); err != nil {\n\t\t\tw.logger.Error(\"Failed to rotate WAL segment\", \"error\", err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Replay reads all WAL segments and calls the handler for each event.\nfunc (w *WALRepository) Replay(ctx context.Context, handler func(event domain.LogEvent) error) error {\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\n\tif w.currentSegment != nil {\n\t\tw.currentSegment.Close()\n\t\tw.currentSegment = nil\n\t}\n\n\tsegments, err := w.getSortedSegments()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif len(segments) == 0 {\n\t\tw.logger.Info(\"WAL is empty, nothing to replay\")\n\t\treturn nil\n\t}\n\tw.logger.Info(\"Starting WAL replay\", \"segment_count\", len(segments))\n\n\tfor _, segmentPath := range segments {\n\t\tfile, err := os.Open(segmentPath)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to open segment %s for replay: %w\", segmentPath, err)\n\t\t}\n\n\t\tscanner := bufio.NewScanner(file)\n\t\tfor scanner.Scan() {\n\t\t\tif ctx.Err() != nil {\n\t\t\t\tfile.Close()\n\t\t\t\treturn ctx.Err()\n\t\t\t}\n\t\t\tvar event domain.LogEvent\n\t\t\tif err := json.Unmarshal(scanner.Bytes(), \u0026event); err != nil {\n\t\t\t\tw.logger.Warn(\"Failed to unmarshal event from WAL, skipping\", \"error\", err, \"line\", scanner.Text())\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif err := handler(event); err != nil {\n\t\t\t\tfile.Close()\n\t\t\t\tw.logger.Error(\"WAL replay handler failed, stopping replay\", \"error\", err)\n\t\t\t\treturn fmt.Errorf(\"replay handler failed: %w\", err)\n\t\t\t}\n\t\t}\n\t\tif err := scanner.Err(); err != nil {\n\t\t\tfile.Close()\n\t\t\treturn fmt.Errorf(\"error scanning segment %s: %w\", segmentPath, err)\n\t\t}\n\t\tfile.Close()\n\t}\n\n\tw.logger.Info(\"WAL replay completed\")\n\treturn nil\n}\n\n// Truncate removes all WAL segment files.\nfunc (w *WALRepository) Truncate(ctx context.Context) error {\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\n\tif w.currentSegment != nil {\n\t\tw.currentSegment.Close()\n\t\tw.currentSegment = nil\n\t}\n\n\tsegments, err := w.getSortedSegments()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tfor _, segmentPath := range segments {\n\t\tif err := os.Remove(segmentPath); err != nil {\n\t\t\tw.logger.Error(\"Failed to remove WAL segment\", \"path\", segmentPath, \"error\", err)\n\t\t}\n\t}\n\n\tw.logger.Info(\"WAL truncated\")\n\treturn w.openLatestSegment()\n}\n\nfunc (w *WALRepository) rotate() error {\n\tif w.currentSegment != nil {\n\t\tif err := w.currentSegment.Sync(); err != nil {\n\t\t\tw.logger.Error(\"Failed to sync WAL segment before rotating\", \"error\", err)\n\t\t}\n\t\tif err := w.currentSegment.Close(); err != nil {\n\t\t\tw.logger.Error(\"Failed to close WAL segment before rotating\", \"error\", err)\n\t\t}\n\t\tw.currentSegment = nil\n\t}\n\n\tsegmentName := fmt.Sprintf(\"%s%d.log\", segmentPrefix, time.Now().UnixNano())\n\tpath := filepath.Join(w.dir, segmentName)\n\n\tf, err := os.OpenFile(path, os.O_APPEND|os.O_CREATE|os.O_WRONLY, filePerm)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to create new WAL segment %s: %w\", path, err)\n\t}\n\n\tw.currentSegment = f\n\tw.currentSize = 0\n\tw.logger.Info(\"Rotated to new WAL segment\", \"path\", path)\n\treturn nil\n}\n\nfunc (w *WALRepository) openLatestSegment() error {\n\tsegments, err := w.getSortedSegments()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif len(segments) == 0 {\n\t\treturn w.rotate()\n\t}\n\n\tlatestSegmentPath := segments[len(segments)-1]\n\tstat, err := os.Stat(latestSegmentPath)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to stat latest segment %s: %w\", latestSegmentPath, err)\n\t}\n\n\tf, err := os.OpenFile(latestSegmentPath, os.O_APPEND|os.O_WRONLY, filePerm)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to open latest segment %s: %w\", latestSegmentPath, err)\n\t}\n\n\tw.currentSegment = f\n\tw.currentSize = stat.Size()\n\tw.logger.Info(\"Opened existing WAL segment\", \"path\", latestSegmentPath, \"size\", w.currentSize)\n\n\tif w.currentSize \u003e= w.maxSegmentSize {\n\t\treturn w.rotate()\n\t}\n\n\treturn nil\n}\n\nfunc (w *WALRepository) getSortedSegments() ([]string, error) {\n\tentries, err := os.ReadDir(w.dir)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to read WAL directory: %w\", err)\n\t}\n\n\tvar segments []string\n\tfor _, entry := range entries {\n\t\tif !entry.IsDir() \u0026\u0026 strings.HasPrefix(entry.Name(), segmentPrefix) {\n\t\t\tsegments = append(segments, filepath.Join(w.dir, entry.Name()))\n\t\t}\n\t}\n\tsort.Strings(segments)\n\treturn segments, nil\n}\n\nfunc (w *WALRepository) calculateTotalSize() (int64, error) {\n\tvar totalSize int64\n\tentries, err := os.ReadDir(w.dir)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tfor _, entry := range entries {\n\t\tif !entry.IsDir() \u0026\u0026 strings.HasPrefix(entry.Name(), segmentPrefix) {\n\t\t\tinfo, err := entry.Info()\n\t\t\tif err != nil {\n\t\t\t\treturn 0, err\n\t\t\t}\n\t\t\ttotalSize += info.Size()\n\t\t}\n\t}\n\treturn totalSize, nil\n}\n\n// Close ensures the current segment is closed gracefully.\nfunc (w *WALRepository) Close() error {\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\tif w.currentSegment != nil {\n\t\treturn w.currentSegment.Close()\n\t}\n\treturn nil\n}\n",
  "internal/adapter/repository/wal/wal_repository_test.go": "package wal\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"log/slog\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/google/uuid\"\n\t\"github.com/user/log-ingestor/internal/domain\"\n)\n\nfunc setupTestWAL(t *testing.T, maxSegmentSize, maxTotalSize int64) (*WALRepository, func()) {\n\tt.Helper()\n\tdir, err := os.MkdirTemp(\"\", \"wal_test\")\n\tif err != nil {\n\t\tt.Fatalf(\"failed to create temp dir: %v\", err)\n\t}\n\n\tlogger := slog.New(slog.NewJSONHandler(os.Stdout, nil))\n\twal, err := NewWALRepository(dir, maxSegmentSize, maxTotalSize, logger)\n\tif err != nil {\n\t\tt.Fatalf(\"failed to create WALRepository: %v\", err)\n\t}\n\n\tcleanup := func() {\n\t\twal.Close()\n\t\tos.RemoveAll(dir)\n\t}\n\n\treturn wal, cleanup\n}\n\nfunc TestWAL_WriteAndReplay(t *testing.T) {\n\twal, cleanup := setupTestWAL(t, 1024, 10*1024)\n\tdefer cleanup()\n\n\tevents := []domain.LogEvent{\n\t\t{ID: uuid.NewString(), Message: \"event 1\"},\n\t\t{ID: uuid.NewString(), Message: \"event 2\"},\n\t\t{ID: uuid.NewString(), Message: \"event 3\"},\n\t}\n\n\tfor _, event := range events {\n\t\tif err := wal.Write(context.Background(), event); err != nil {\n\t\t\tt.Fatalf(\"failed to write event: %v\", err)\n\t\t}\n\t}\n\twal.Close() // Close to ensure data is flushed\n\n\t// Re-open the WAL to simulate a restart\n\tvar err error\n\twal, err = NewWALRepository(wal.dir, 1024, 10*1024, wal.logger)\n\tif err != nil {\n\t\tt.Fatalf(\"failed to re-open WAL: %v\", err)\n\t}\n\n\tvar replayedEvents []domain.LogEvent\n\treplayHandler := func(event domain.LogEvent) error {\n\t\treplayedEvents = append(replayedEvents, event)\n\t\treturn nil\n\t}\n\n\tif err := wal.Replay(context.Background(), replayHandler); err != nil {\n\t\tt.Fatalf(\"failed to replay events: %v\", err)\n\t}\n\n\tif len(replayedEvents) != len(events) {\n\t\tt.Fatalf(\"expected %d replayed events, got %d\", len(events), len(replayedEvents))\n\t}\n\n\tfor i, event := range events {\n\t\tif replayedEvents[i].ID != event.ID || replayedEvents[i].Message != event.Message {\n\t\t\tt.Errorf(\"replayed event mismatch at index %d: got %+v, want %+v\", i, replayedEvents[i], event)\n\t\t}\n\t}\n}\n\nfunc TestWAL_SegmentRotation(t *testing.T) {\n\t// Set a very small segment size to force rotation\n\twal, cleanup := setupTestWAL(t, 100, 1024)\n\tdefer cleanup()\n\n\tevent := domain.LogEvent{ID: uuid.NewString(), Message: \"a message long enough to cause rotation\"}\n\teventBytes, _ := json.Marshal(event)\n\teventSize := len(eventBytes)\n\n\t// Write enough events to create at least 2 segments\n\tnumWrites := (100 / eventSize) + 2\n\tfor i := 0; i \u003c numWrites; i++ {\n\t\tif err := wal.Write(context.Background(), event); err != nil {\n\t\t\tt.Fatalf(\"failed to write event: %v\", err)\n\t\t}\n\t}\n\n\tsegments, err := wal.getSortedSegments()\n\tif err != nil {\n\t\tt.Fatalf(\"failed to get segments: %v\", err)\n\t}\n\n\tif len(segments) \u003c 2 {\n\t\tt.Errorf(\"expected at least 2 segments, got %d\", len(segments))\n\t}\n}\n\nfunc TestWAL_Truncate(t *testing.T) {\n\twal, cleanup := setupTestWAL(t, 1024, 1024)\n\tdefer cleanup()\n\n\tevent := domain.LogEvent{ID: uuid.NewString(), Message: \"some data\"}\n\tif err := wal.Write(context.Background(), event); err != nil {\n\t\tt.Fatalf(\"failed to write event: %v\", err)\n\t}\n\n\tsegments, _ := wal.getSortedSegments()\n\tif len(segments) == 0 {\n\t\tt.Fatal(\"expected at least one segment before truncate\")\n\t}\n\n\tif err := wal.Truncate(context.Background()); err != nil {\n\t\tt.Fatalf(\"failed to truncate WAL: %v\", err)\n\t}\n\n\tsegments, _ = wal.getSortedSegments()\n\tif len(segments) != 1 { // Truncate creates a new empty segment\n\t\tt.Errorf(\"expected 1 segment after truncate, got %d\", len(segments))\n\t}\n\tinfo, _ := os.Stat(segments[0])\n\tif info.Size() != 0 {\n\t\tt.Errorf(\"expected new segment to be empty, size is %d\", info.Size())\n\t}\n}\n\nfunc TestWAL_MaxTotalSize(t *testing.T) {\n\twal, cleanup := setupTestWAL(t, 100, 150) // Max total size is very small\n\tdefer cleanup()\n\n\tevent := domain.LogEvent{ID: uuid.NewString(), Message: \"some data that will fill up the WAL\"}\n\tvar err error\n\tfor i := 0; i \u003c 5; i++ { // Write until we expect an error\n\t\terr = wal.Write(context.Background(), event)\n\t\tif err != nil {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tif err == nil {\n\t\tt.Fatal(\"expected an error when writing beyond max total size, but got nil\")\n\t}\n}\n",
  "internal/domain/admin.go": "package domain\n\nimport \"time\"\n\n// ConsumerGroupInfo represents information about a Redis Stream consumer group.\ntype ConsumerGroupInfo struct {\n\tName            string `json:\"name\"`\n\tConsumers       int64  `json:\"consumers\"`\n\tPending         int64  `json:\"pending\"`\n\tLastDeliveredID string `json:\"last_delivered_id\"`\n}\n\n// ConsumerInfo represents information about a specific consumer in a group.\ntype ConsumerInfo struct {\n\tName    string        `json:\"name\"`\n\tPending int64         `json:\"pending\"`\n\tIdle    time.Duration `json:\"idle_ms\"`\n}\n\n// PendingMessageSummary provides a summary of pending messages for a consumer group.\ntype PendingMessageSummary struct {\n\tTotal          int64            `json:\"total\"`\n\tFirstMessageID string           `json:\"first_message_id,omitempty\"`\n\tLastMessageID  string           `json:\"last_message_id,omitempty\"`\n\tConsumerTotals map[string]int64 `json:\"consumer_totals,omitempty\"`\n}\n\n// PendingMessageDetail represents a detailed view of a single pending message.\ntype PendingMessageDetail struct {\n\tID         string        `json:\"id\"`\n\tConsumer   string        `json:\"consumer\"`\n\tIdleTime   time.Duration `json:\"idle_time_ms\"`\n\tRetryCount int64         `json:\"retry_count\"`\n}\n",
  "internal/domain/log.go": "package domain\n\nimport (\n\t\"encoding/json\"\n\t\"time\"\n)\n\n// LogEvent represents a single log event.\ntype LogEvent struct {\n\tID              string          `json:\"event_id\"`\n\tReceivedAt      time.Time       `json:\"received_at\"`\n\tEventTime       time.Time       `json:\"event_time\"`\n\tSource          string          `json:\"source,omitempty\"`\n\tLevel           string          `json:\"level,omitempty\"`\n\tMessage         string          `json:\"message\"`\n\tMetadata        json.RawMessage `json:\"metadata,omitempty\"`\n\tRawEvent        json.RawMessage `json:\"-\"` // The original raw event payload, not for final serialization.\n\tPIIRedacted     bool            `json:\"pii_redacted,omitempty\"`\n\tStreamMessageID string          `json:\"-\"` // Transient field for Redis Stream message ID, not serialized.\n}\n",
  "internal/domain/mocks/repository_mocks.go": "package mocks\n\nimport (\n\t\"context\"\n\t\"sync\"\n\n\t\"github.com/user/log-ingestor/internal/domain\"\n)\n\n// MockLogRepository is a mock implementation of domain.LogRepository for testing.\ntype MockLogRepository struct {\n\tmu              sync.Mutex\n\tBufferedEvents  []domain.LogEvent\n\tWrittenEvents   []domain.LogEvent\n\tAckedMessageIDs []string\n\tDLQEvents       []domain.LogEvent\n\tReadBatchResult []domain.LogEvent\n\tBufferErr       error\n\tReadErr         error\n\tWriteErr        error\n\tAckErr          error\n\tDLQErr          error\n}\n\nfunc (m *MockLogRepository) BufferLog(ctx context.Context, event domain.LogEvent) error {\n\tm.mu.Lock()\n\tdefer m.mu.Unlock()\n\tif m.BufferErr != nil {\n\t\treturn m.BufferErr\n\t}\n\tm.BufferedEvents = append(m.BufferedEvents, event)\n\treturn nil\n}\n\nfunc (m *MockLogRepository) ReadLogBatch(ctx context.Context, group, consumer string, count int) ([]domain.LogEvent, error) {\n\tm.mu.Lock()\n\tdefer m.mu.Unlock()\n\tif m.ReadErr != nil {\n\t\treturn nil, m.ReadErr\n\t}\n\treturn m.ReadBatchResult, nil\n}\n\nfunc (m *MockLogRepository) WriteLogBatch(ctx context.Context, events []domain.LogEvent) error {\n\tm.mu.Lock()\n\tdefer m.mu.Unlock()\n\tif m.WriteErr != nil {\n\t\treturn m.WriteErr\n\t}\n\tm.WrittenEvents = append(m.WrittenEvents, events...)\n\treturn nil\n}\n\nfunc (m *MockLogRepository) AcknowledgeLogs(ctx context.Context, group string, messageIDs ...string) error {\n\tm.mu.Lock()\n\tdefer m.mu.Unlock()\n\tif m.AckErr != nil {\n\t\treturn m.AckErr\n\t}\n\tm.AckedMessageIDs = append(m.AckedMessageIDs, messageIDs...)\n\treturn nil\n}\n\nfunc (m *MockLogRepository) MoveToDLQ(ctx context.Context, events []domain.LogEvent) error {\n\tm.mu.Lock()\n\tdefer m.mu.Unlock()\n\tif m.DLQErr != nil {\n\t\treturn m.DLQErr\n\t}\n\tm.DLQEvents = append(m.DLQEvents, events...)\n\treturn nil\n}\n",
  "internal/domain/repository.go": "package domain\n\nimport (\n\t\"context\"\n\t\"time\"\n)\n\n// LogRepository defines the interface for log event persistence and buffering.\ntype LogRepository interface {\n\tBufferLog(ctx context.Context, event LogEvent) error\n\tReadLogBatch(ctx context.Context, group, consumer string, count int) ([]LogEvent, error)\n\tWriteLogBatch(ctx context.Context, events []LogEvent) error\n\tAcknowledgeLogs(ctx context.Context, group string, messageIDs ...string) error\n\tMoveToDLQ(ctx context.Context, events []LogEvent) error\n}\n\n// APIKeyRepository defines the interface for validating API keys.\ntype APIKeyRepository interface {\n\tIsValid(ctx context.Context, key string) (bool, error)\n}\n\n// WALRepository defines the interface for a Write-Ahead Log.\ntype WALRepository interface {\n\tWrite(ctx context.Context, event LogEvent) error\n\tReplay(ctx context.Context, handler func(event LogEvent) error) error\n\tTruncate(ctx context.Context) error\n\tClose() error\n}\n\n// StreamAdminRepository defines the interface for administrative operations on a stream.\ntype StreamAdminRepository interface {\n\tGetGroupInfo(ctx context.Context, stream string) ([]ConsumerGroupInfo, error)\n\tGetConsumerInfo(ctx context.Context, stream, group string) ([]ConsumerInfo, error)\n\tGetPendingSummary(ctx context.Context, stream, group string) (*PendingMessageSummary, error)\n\tGetPendingMessages(ctx context.Context, stream, group, consumer string, startID string, count int64) ([]PendingMessageDetail, error)\n\tClaimMessages(ctx context.Context, stream, group, consumer string, minIdleTime time.Duration, messageIDs []string) ([]LogEvent, error)\n\tAcknowledgeMessages(ctx context.Context, stream, group string, messageIDs ...string) (int64, error)\n\tTrimStream(ctx context.Context, stream string, maxLen int64) (int64, error)\n}",
  "internal/pkg/config/config.go": "package config\n\nimport (\n\t\"time\"\n\n\t\"github.com/caarlos0/env/v10\"\n\t\"github.com/joho/godotenv\"\n)\n\n// Config holds all application configuration parameters.\ntype Config struct {\n\tLogLevel             string        `env:\"LOG_LEVEL\" envDefault:\"info\"`\n\tMaxEventSize         int64         `env:\"MAX_EVENT_SIZE\" envDefault:\"1048576\"`    // 1MB\n\tWALPath              string        `env:\"WAL_PATH\" envDefault:\"./wal\"`             // Path for Write-Ahead Log files\n\tWALSegmentSize       int64         `env:\"WAL_SEGMENT_SIZE\" envDefault:\"104857600\"` // 100MB\n\tWALMaxDiskSize       int64         `env:\"WAL_MAX_DISK_SIZE\" envDefault:\"1073741824\"` // 1GB\n\tBackpressurePolicy   string        `env:\"BACKPRESSURE_POLICY\" envDefault:\"block\"`\n\tRedisAddr            string        `env:\"REDIS_ADDR,required\"`\n\tRedisDLQStream       string        `env:\"REDIS_DLQ_STREAM\" envDefault:\"log_events_dlq\"`\n\tPostgresURL          string        `env:\"POSTGRES_URL,required\"`\n\tAPIKeyCacheTTL       time.Duration `env:\"API_KEY_CACHE_TTL\" envDefault:\"5m\"`\n\tPIIRedactionFields   string        `env:\"PII_REDACTION_FIELDS\" envDefault:\"email,password,credit_card,ssn\"`\n\tIngestServerAddr     string        `env:\"INGEST_SERVER_ADDR\" envDefault:\":8080\"`\n\tConsumerRetryCount   int           `env:\"CONSUMER_RETRY_COUNT\" envDefault:\"3\"`\n\tConsumerRetryBackoff time.Duration `env:\"CONSUMER_RETRY_BACKOFF\" envDefault:\"1s\"`\n}\n\n// Load reads configuration from environment variables.\nfunc Load() (*Config, error) {\n\t// Load .env file if it exists (for local development)\n\t_ = godotenv.Load()\n\n\tcfg := \u0026Config{}\n\tif err := env.Parse(cfg); err != nil {\n\t\treturn nil, err\n\t}\n\treturn cfg, nil\n}\n",
  "internal/pkg/logger/logger.go": "package logger\n\nimport (\n\t\"log/slog\"\n\t\"os\"\n\t\"strings\"\n)\n\n// New creates and configures a new slog.Logger.\nfunc New(level string) *slog.Logger {\n\tvar logLevel slog.Level\n\n\tswitch strings.ToLower(level) {\n\tcase \"debug\":\n\t\tlogLevel = slog.LevelDebug\n\tcase \"info\":\n\t\tlogLevel = slog.LevelInfo\n\tcase \"warn\", \"warning\":\n\t\tlogLevel = slog.LevelWarn\n\tcase \"error\":\n\t\tlogLevel = slog.LevelError\n\tdefault:\n\t\tlogLevel = slog.LevelInfo\n\t}\n\n\topts := \u0026slog.HandlerOptions{\n\t\tLevel: logLevel,\n\t}\n\n\thandler := slog.NewJSONHandler(os.Stdout, opts)\n\tlogger := slog.New(handler)\n\n\treturn logger\n}\n```",
  "internal/usecase/admin_stream.go": "package usecase\n\nimport (\n\t\"context\"\n\t\"time\"\n\n\t\"github.com/user/log-ingestor/internal/domain\"\n)\n\n// AdminStreamUseCase provides use cases for stream administration.\ntype AdminStreamUseCase struct {\n\trepo domain.StreamAdminRepository\n}\n\n// NewAdminStreamUseCase creates a new AdminStreamUseCase.\nfunc NewAdminStreamUseCase(repo domain.StreamAdminRepository) *AdminStreamUseCase {\n\treturn \u0026AdminStreamUseCase{repo: repo}\n}\n\nfunc (uc *AdminStreamUseCase) GetGroupInfo(ctx context.Context, stream string) ([]domain.ConsumerGroupInfo, error) {\n\treturn uc.repo.GetGroupInfo(ctx, stream)\n}\n\nfunc (uc *AdminStreamUseCase) GetConsumerInfo(ctx context.Context, stream, group string) ([]domain.ConsumerInfo, error) {\n\treturn uc.repo.GetConsumerInfo(ctx, stream, group)\n}\n\nfunc (uc *AdminStreamUseCase) GetPendingSummary(ctx context.Context, stream, group string) (*domain.PendingMessageSummary, error) {\n\treturn uc.repo.GetPendingSummary(ctx, stream, group)\n}\n\nfunc (uc *AdminStreamUseCase) GetPendingMessages(ctx context.Context, stream, group, consumer string, startID string, count int64) ([]domain.PendingMessageDetail, error) {\n\tif startID == \"\" {\n\t\tstartID = \"-\"\n\t}\n\tif count \u003c= 0 {\n\t\tcount = 100 // Default count\n\t}\n\treturn uc.repo.GetPendingMessages(ctx, stream, group, consumer, startID, count)\n}\n\nfunc (uc *AdminStreamUseCase) ClaimMessages(ctx context.Context, stream, group, consumer string, minIdleTime time.Duration, messageIDs []string) ([]domain.LogEvent, error) {\n\treturn uc.repo.ClaimMessages(ctx, stream, group, consumer, minIdleTime, messageIDs)\n}\n\nfunc (uc *AdminStreamUseCase) AcknowledgeMessages(ctx context.Context, stream, group string, messageIDs ...string) (int64, error) {\n\treturn uc.repo.AcknowledgeMessages(ctx, stream, group, messageIDs...)\n}\n\nfunc (uc *AdminStreamUseCase) TrimStream(ctx context.Context, stream string, maxLen int64) (int64, error) {\n\treturn uc.repo.TrimStream(ctx, stream, maxLen)\n}\n",
  "internal/usecase/ingest_log.go": "package usecase\n\nimport (\n\t\"context\"\n\t\"log/slog\"\n\t\"time\"\n\n\t\"github.com/google/uuid\"\n\t\"github.com/user/log-ingestor/internal/adapter/pii\"\n\t\"github.com/user/log-ingestor/internal/domain\"\n)\n\n// IngestLogUseCase handles the business logic for ingesting a log event.\ntype IngestLogUseCase struct {\n\trepo     domain.LogRepository\n\tredactor *pii.Redactor\n\tlogger   *slog.Logger\n}\n\n// NewIngestLogUseCase creates a new IngestLogUseCase.\nfunc NewIngestLogUseCase(repo domain.LogRepository, redactor *pii.Redactor, logger *slog.Logger) *IngestLogUseCase {\n\treturn \u0026IngestLogUseCase{\n\t\trepo:     repo,\n\t\tredactor: redactor,\n\t\tlogger:   logger,\n\t}\n}\n\n// Ingest validates, enriches, redacts, and buffers a log event.\nfunc (uc *IngestLogUseCase) Ingest(ctx context.Context, event *domain.LogEvent) error {\n\t// 1. Enrich with server-side data\n\tevent.ReceivedAt = time.Now().UTC()\n\tif event.ID == \"\" {\n\t\tevent.ID = uuid.NewString()\n\t}\n\n\t// 2. Redact PII\n\tif err := uc.redactor.Redact(event); err != nil {\n\t\tuc.logger.Warn(\"failed to redact PII, proceeding with original event\", \"error\", err, \"event_id\", event.ID)\n\t\t// Non-fatal error, we still ingest the log\n\t}\n\n\t// 3. Buffer the log\n\tif err := uc.repo.BufferLog(ctx, *event); err != nil {\n\t\tuc.logger.Error(\"failed to buffer log event\", \"error\", err, \"event_id\", event.ID)\n\t\t// TODO: Implement WAL fallback logic here\n\t\treturn err\n\t}\n\n\treturn nil\n}\n",
  "internal/usecase/ingest_log_test.go": "package usecase\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"io\"\n\t\"log/slog\"\n\t\"testing\"\n\n\t\"github.com/user/log-ingestor/internal/adapter/pii\"\n\t\"github.com/user/log-ingestor/internal/domain\"\n\t\"github.com/user/log-ingestor/internal/domain/mocks\"\n)\n\nfunc TestIngestLogUseCase_Ingest(t *testing.T) {\n\tlogger := slog.New(slog.NewTextHandler(io.Discard, nil))\n\tredactor := pii.NewRedactor([]string{\"email\"}, logger)\n\n\tt.Run(\"Successful Ingestion\", func(t *testing.T) {\n\t\tmockRepo := \u0026mocks.MockLogRepository{}\n\t\tuc := NewIngestLogUseCase(mockRepo, redactor, logger)\n\n\t\tevent := \u0026domain.LogEvent{Message: \"test message\"}\n\t\terr := uc.Ingest(context.Background(), event)\n\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"expected no error, got %v\", err)\n\t\t}\n\t\tif event.ID == \"\" {\n\t\t\tt.Error(\"expected event ID to be generated\")\n\t\t}\n\t\tif event.ReceivedAt.IsZero() {\n\t\t\tt.Error(\"expected ReceivedAt to be set\")\n\t\t}\n\t\tif len(mockRepo.BufferedEvents) != 1 {\n\t\t\tt.Errorf(\"expected 1 event to be buffered, got %d\", len(mockRepo.BufferedEvents))\n\t\t}\n\t\tif mockRepo.BufferedEvents[0].ID != event.ID {\n\t\t\tt.Error(\"buffered event ID mismatch\")\n\t\t}\n\t})\n\n\tt.Run(\"Repository Error\", func(t *testing.T) {\n\t\tmockRepo := \u0026mocks.MockLogRepository{\n\t\t\tBufferErr: errors.New(\"buffer is full\"),\n\t\t}\n\t\tuc := NewIngestLogUseCase(mockRepo, redactor, logger)\n\n\t\tevent := \u0026domain.LogEvent{Message: \"test message\"}\n\t\terr := uc.Ingest(context.Background(), event)\n\n\t\tif err == nil {\n\t\t\tt.Fatal(\"expected an error, got nil\")\n\t\t}\n\t\tif err.Error() != \"buffer is full\" {\n\t\t\tt.Errorf(\"unexpected error message: got %q\", err.Error())\n\t\t}\n\t})\n\n\tt.Run(\"PII Redaction\", func(t *testing.T) {\n\t\tmockRepo := \u0026mocks.MockLogRepository{}\n\t\tuc := NewIngestLogUseCase(mockRepo, redactor, logger)\n\n\t\tevent := \u0026domain.LogEvent{\n\t\t\tMessage:  \"user login\",\n\t\t\tMetadata: []byte(`{\"email\": \"test@example.com\"}`),\n\t\t}\n\t\terr := uc.Ingest(context.Background(), event)\n\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"expected no error, got %v\", err)\n\t\t}\n\t\tif !mockRepo.BufferedEvents[0].PIIRedacted {\n\t\t\tt.Error(\"expected PIIRedacted flag to be true\")\n\t\t}\n\t\texpectedMetadata := `{\"email\":\"[REDACTED]\"}`\n\t\tif string(mockRepo.BufferedEvents[0].Metadata) != expectedMetadata {\n\t\t\tt.Errorf(\"expected metadata to be redacted: got %s, want %s\", string(mockRepo.BufferedEvents[0].Metadata), expectedMetadata)\n\t\t}\n\t})\n}\n",
  "internal/usecase/process_logs.go": "package usecase\n\nimport (\n\t\"context\"\n\t\"log/slog\"\n\t\"math\"\n\t\"time\"\n\n\t\"github.com/user/log-ingestor/internal/domain\"\n)\n\nconst (\n\tdefaultBatchSize = 1000\n)\n\n// ProcessLogsUseCase orchestrates reading logs from a buffer and writing to a sink.\ntype ProcessLogsUseCase struct {\n\tbufferRepo   domain.LogRepository\n\tsinkRepo     domain.LogRepository\n\tlogger       *slog.Logger\n\tgroup        string\n\tconsumer     string\n\tretryCount   int\n\tretryBackoff time.Duration\n}\n\n// NewProcessLogsUseCase creates a new ProcessLogsUseCase.\nfunc NewProcessLogsUseCase(bufferRepo, sinkRepo domain.LogRepository, logger *slog.Logger, group, consumer string, retryCount int, retryBackoff time.Duration) *ProcessLogsUseCase {\n\treturn \u0026ProcessLogsUseCase{\n\t\tbufferRepo:   bufferRepo,\n\t\tsinkRepo:     sinkRepo,\n\t\tlogger:       logger.With(\"component\", \"process_logs_usecase\"),\n\t\tgroup:        group,\n\t\tconsumer:     consumer,\n\t\tretryCount:   retryCount,\n\t\tretryBackoff: retryBackoff,\n\t}\n}\n\n// ProcessBatch reads a batch of logs, attempts to write them to the sink with retries,\n// moves to DLQ on failure, and acknowledges on success.\nfunc (u *ProcessLogsUseCase) ProcessBatch(ctx context.Context) (int, error) {\n\tevents, err := u.bufferRepo.ReadLogBatch(ctx, u.group, u.consumer, defaultBatchSize)\n\tif err != nil {\n\t\tu.logger.Error(\"Failed to read log batch from buffer\", \"error\", err)\n\t\treturn 0, err\n\t}\n\n\tif len(events) == 0 {\n\t\treturn 0, nil\n\t}\n\n\tu.logger.Debug(\"Read batch from buffer\", \"count\", len(events))\n\n\terr = u.writeWithRetry(ctx, events)\n\tif err != nil {\n\t\tu.logger.Error(\"Failed to write batch to sink after all retries, moving to DLQ\", \"error\", err, \"batch_size\", len(events))\n\t\tif dlqErr := u.bufferRepo.MoveToDLQ(ctx, events); dlqErr != nil {\n\t\t\tu.logger.Error(\"CRITICAL: Failed to move events to DLQ. Events will be re-processed.\", \"error\", dlqErr)\n\t\t\treturn 0, dlqErr\n\t\t}\n\t}\n\n\tmessageIDs := make([]string, len(events))\n\tfor i, event := range events {\n\t\tmessageIDs[i] = event.StreamMessageID\n\t}\n\n\tif ackErr := u.bufferRepo.AcknowledgeLogs(ctx, u.group, messageIDs...); ackErr != nil {\n\t\tu.logger.Error(\"Failed to acknowledge processed logs\", \"error\", ackErr)\n\t\treturn 0, ackErr\n\t}\n\n\tu.logger.Info(\"Successfully processed batch\", \"count\", len(events), \"final_status\", map[bool]string{true: \"SINKED\", false: \"DLQED\"}[err == nil])\n\treturn len(events), nil\n}\n\nfunc (u *ProcessLogsUseCase) writeWithRetry(ctx context.Context, events []domain.LogEvent) error {\n\tvar lastErr error\n\n\tfor i := 0; i \u003c u.retryCount; i++ {\n\t\terr := u.sinkRepo.WriteLogBatch(ctx, events)\n\t\tif err == nil {\n\t\t\treturn nil // Success\n\t\t}\n\t\tlastErr = err\n\n\t\tif i == u.retryCount-1 || ctx.Err() != nil {\n\t\t\tbreak\n\t\t}\n\n\t\t// Exponential backoff: 1s, 2s, 4s, ... for default backoff of 1s\n\t\tdelay := time.Duration(float64(u.retryBackoff) * math.Pow(2, float64(i)))\n\t\tu.logger.Warn(\"Failed to write to sink, retrying...\", \"attempt\", i+1, \"delay\", delay, \"error\", err)\n\n\t\tselect {\n\t\tcase \u003c-time.After(delay):\n\t\tcase \u003c-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\t}\n\t}\n\treturn lastErr\n}\n",
  "internal/usecase/process_logs_test.go": "package usecase\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"io\"\n\t\"log/slog\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/user/log-ingestor/internal/domain\"\n\t\"github.com/user/log-ingestor/internal/domain/mocks\"\n)\n\nfunc TestProcessLogsUseCase_ProcessBatch(t *testing.T) {\n\tlogger := slog.New(slog.NewTextHandler(io.Discard, nil))\n\ttestEvents := []domain.LogEvent{\n\t\t{ID: \"1\", StreamMessageID: \"msg1\", Message: \"event 1\"},\n\t\t{ID: \"2\", StreamMessageID: \"msg2\", Message: \"event 2\"},\n\t}\n\n\tt.Run(\"Successful Processing\", func(t *testing.T) {\n\t\tbufferRepo := \u0026mocks.MockLogRepository{ReadBatchResult: testEvents}\n\t\tsinkRepo := \u0026mocks.MockLogRepository{}\n\t\tuc := NewProcessLogsUseCase(bufferRepo, sinkRepo, logger, \"group\", \"consumer\", 3, 1*time.Millisecond)\n\n\t\tcount, err := uc.ProcessBatch(context.Background())\n\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"expected no error, got %v\", err)\n\t\t}\n\t\tif count != len(testEvents) {\n\t\t\tt.Errorf(\"expected processed count to be %d, got %d\", len(testEvents), count)\n\t\t}\n\t\tif len(sinkRepo.WrittenEvents) != 2 {\n\t\t\tt.Errorf(\"expected 2 events written to sink, got %d\", len(sinkRepo.WrittenEvents))\n\t\t}\n\t\tif len(bufferRepo.AckedMessageIDs) != 2 {\n\t\t\tt.Errorf(\"expected 2 messages to be acked, got %d\", len(bufferRepo.AckedMessageIDs))\n\t\t}\n\t\tif len(bufferRepo.DLQEvents) != 0 {\n\t\t\tt.Errorf(\"expected 0 events in DLQ, got %d\", len(bufferRepo.DLQEvents))\n\t\t}\n\t})\n\n\tt.Run(\"Sink Failure with Retry and DLQ\", func(t *testing.T) {\n\t\tbufferRepo := \u0026mocks.MockLogRepository{ReadBatchResult: testEvents}\n\t\tsinkRepo := \u0026mocks.MockLogRepository{WriteErr: errors.New(\"database is down\")}\n\t\tuc := NewProcessLogsUseCase(bufferRepo, sinkRepo, logger, \"group\", \"consumer\", 2, 1*time.Millisecond)\n\n\t\tcount, err := uc.ProcessBatch(context.Background())\n\n\t\tif err == nil {\n\t\t\tt.Fatal(\"expected an error, got nil\")\n\t\t}\n\t\tif count != 0 {\n\t\t\tt.Errorf(\"expected processed count to be 0, got %d\", count)\n\t\t}\n\t\tif len(sinkRepo.WrittenEvents) != 0 {\n\t\t\tt.Errorf(\"expected 0 events written to sink, got %d\", len(sinkRepo.WrittenEvents))\n\t\t}\n\t\tif len(bufferRepo.DLQEvents) != 2 {\n\t\t\tt.Errorf(\"expected 2 events in DLQ, got %d\", len(bufferRepo.DLQEvents))\n\t\t}\n\t\t// Messages should be acked even if they go to DLQ\n\t\tif len(bufferRepo.AckedMessageIDs) != 2 {\n\t\t\tt.Errorf(\"expected 2 messages to be acked, got %d\", len(bufferRepo.AckedMessageIDs))\n\t\t}\n\t})\n\n\tt.Run(\"Buffer Read Error\", func(t *testing.T) {\n\t\tbufferRepo := \u0026mocks.MockLogRepository{ReadErr: errors.New(\"redis connection failed\")}\n\t\tsinkRepo := \u0026mocks.MockLogRepository{}\n\t\tuc := NewProcessLogsUseCase(bufferRepo, sinkRepo, logger, \"group\", \"consumer\", 3, 1*time.Millisecond)\n\n\t\tcount, err := uc.ProcessBatch(context.Background())\n\n\t\tif err == nil {\n\t\t\tt.Fatal(\"expected an error, got nil\")\n\t\t}\n\t\tif count != 0 {\n\t\t\tt.Errorf(\"expected processed count to be 0, got %d\", count)\n\t\t}\n\t})\n\n\tt.Run(\"No Events to Process\", func(t *testing.T) {\n\t\tbufferRepo := \u0026mocks.MockLogRepository{ReadBatchResult: []domain.LogEvent{}}\n\t\tsinkRepo := \u0026mocks.MockLogRepository{}\n\t\tuc := NewProcessLogsUseCase(bufferRepo, sinkRepo, logger, \"group\", \"consumer\", 3, 1*time.Millisecond)\n\n\t\tcount, err := uc.ProcessBatch(context.Background())\n\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"expected no error, got %v\", err)\n\t\t}\n\t\tif count != 0 {\n\t\t\tt.Errorf(\"expected processed count to be 0, got %d\", count)\n\t\t}\n\t\tif len(sinkRepo.WrittenEvents) != 0 {\n\t\t\tt.Error(\"sink should not be called with no events\")\n\t\t}\n\t})\n}\n",
  "tests/integration/ingest_flow_test.go": "package integration\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"database/sql\"\n\t\"fmt\"\n\t\"net/http\"\n\t\"os\"\n\t\"os/exec\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/google/uuid\"\n\t_ \"github.com/lib/pq\"\n)\n\nconst (\n\tingestorURL = \"http://localhost:8080/ingest\"\n\tpostgresDSN = \"postgres://testuser:testpassword@localhost:5432/testdb?sslmode=disable\"\n)\n\n// TestMain manages the lifecycle of the docker-compose environment for integration tests.\nfunc TestMain(m *testing.M) {\n\t// Start docker-compose\n\tcmd := exec.Command(\"docker-compose\", \"-f\", \"../../docker-compose.yml\", \"up\", \"-d\", \"--build\")\n\tif err := cmd.Run(); err != nil {\n\t\tfmt.Printf(\"Failed to start docker-compose: %v\\n\", err)\n\t\tos.Exit(1)\n\t}\n\n\t// Wait for services to be healthy\n\tif !waitForPostgres() {\n\t\tfmt.Println(\"PostgreSQL did not become healthy in time\")\n\t\tshutdown()\n\t\tos.Exit(1)\n\t}\n\n\t// Run tests\n\tcode := m.Run()\n\n\t// Shutdown docker-compose\n\tshutdown()\n\n\tos.Exit(code)\n}\n\nfunc shutdown() {\n\tcmd := exec.Command(\"docker-compose\", \"-f\", \"../../docker-compose.yml\", \"down\", \"-v\")\n\tif err := cmd.Run(); err != nil {\n\t\tfmt.Printf(\"Failed to stop docker-compose: %v\\n\", err)\n\t}\n}\n\nfunc waitForPostgres() bool {\n\tfor i := 0; i \u003c 30; i++ {\n\t\tdb, err := sql.Open(\"postgres\", postgresDSN)\n\t\tif err == nil {\n\t\t\tdefer db.Close()\n\t\t\tif err = db.Ping(); err == nil {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\treturn false\n}\n\nfunc countLogsInDB(t *testing.T) int {\n\tdb, err := sql.Open(\"postgres\", postgresDSN)\n\tif err != nil {\n\t\tt.Fatalf(\"Failed to connect to postgres: %v\", err)\n\t}\n\tdefer db.Close()\n\n\tvar count int\n\terr = db.QueryRow(\"SELECT COUNT(*) FROM logs\").Scan(\u0026count)\n\tif err != nil {\n\t\tt.Fatalf(\"Failed to query log count: %v\", err)\n\t}\n\treturn count\n}\n\nfunc TestIngestionFlow(t *testing.T) {\n\t// Give consumer a moment to start up and connect\n\ttime.Sleep(5 * time.Second)\n\n\t// 1. Initial state check\n\tinitialCount := countLogsInDB(t)\n\tif initialCount != 0 {\n\t\tt.Fatalf(\"Expected initial log count to be 0, got %d\", initialCount)\n\t}\n\n\t// 2. Ingest a batch of unique events\n\tbatchSize := 100\n\tvar ndjsonBody bytes.Buffer\n\tfor i := 0; i \u003c batchSize; i++ {\n\t\teventID := uuid.NewString()\n\t\tlogLine := fmt.Sprintf(`{\"event_id\": \"%s\", \"message\": \"integration test event %d\"}`, eventID, i)\n\t\tndjsonBody.WriteString(logLine + \"\\n\")\n\t}\n\n\treq, _ := http.NewRequest(http.MethodPost, ingestorURL, \u0026ndjsonBody)\n\treq.Header.Set(\"Content-Type\", \"application/x-ndjson\")\n\treq.Header.Set(\"X-API-Key\", \"supersecretkey\") // From init.sql\n\n\tresp, err := http.DefaultClient.Do(req)\n\tif err != nil {\n\t\tt.Fatalf(\"Failed to send ingest request: %v\", err)\n\t}\n\tdefer resp.Body.Close()\n\n\tif resp.StatusCode != http.StatusAccepted {\n\t\tt.Fatalf(\"Expected status 202 Accepted, got %d\", resp.StatusCode)\n\t}\n\n\t// 3. Verify events are processed and stored\n\tvar finalCount int\n\t// Retry logic to allow for processing time\n\tfor i := 0; i \u003c 10; i++ {\n\t\tfinalCount = countLogsInDB(t)\n\t\tif finalCount == batchSize {\n\t\t\tbreak\n\t\t}\n\t\ttime.Sleep(1 * time.Second)\n\t}\n\n\tif finalCount != batchSize {\n\t\tt.Fatalf(\"Expected %d logs in DB after ingest, got %d\", batchSize, finalCount)\n\t}\n\n\t// 4. Ingest the *same* batch again to test idempotency\n\treq, _ = http.NewRequest(http.MethodPost, ingestorURL, \u0026ndjsonBody)\n\treq.Header.Set(\"Content-Type\", \"application/x-ndjson\")\n\treq.Header.Set(\"X-API-Key\", \"supersecretkey\")\n\n\tresp, err = http.DefaultClient.Do(req)\n\tif err != nil {\n\t\tt.Fatalf(\"Failed to send second ingest request: %v\", err)\n\t}\n\tdefer resp.Body.Close()\n\n\tif resp.StatusCode != http.StatusAccepted {\n\t\tt.Fatalf(\"Expected status 202 Accepted on second request, got %d\", resp.StatusCode)\n\t}\n\n\t// 5. Verify no new rows were added\n\ttime.Sleep(5 * time.Second) // Allow time for processing\n\tidempotentCount := countLogsInDB(t)\n\tif idempotentCount != batchSize {\n\t\tt.Fatalf(\"Idempotency test failed: expected count to remain %d, but got %d\", batchSize, idempotentCount)\n\t}\n}\n",
  "tools/load-tester/main.go": "package main\n\nimport (\n\t\"bytes\"\n\t\"flag\"\n\t\"fmt\"\n\t\"log\"\n\t\"net/http\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/google/uuid\"\n\t\"golang.org/x/time/rate\"\n)\n\nfunc main() {\n\ttargetURL := flag.String(\"url\", \"http://localhost:8080/ingest\", \"Target URL for ingestion\")\n\tapiKey := flag.String(\"api-key\", \"supersecretkey\", \"API Key for authentication\")\n\tconcurrency := flag.Int(\"c\", 10, \"Number of concurrent workers\")\n\tduration := flag.Duration(\"d\", 30*time.Second, \"Duration of the load test\")\n\trps := flag.Int(\"rps\", 1000, \"Requests per second limit\")\n\tflag.Parse()\n\n\tlog.Printf(\"Starting load test on %s\", *targetURL)\n\tlog.Printf(\"Concurrency: %d, Duration: %s, RPS: %d\", *concurrency, *duration, *rps)\n\n\tvar wg sync.WaitGroup\n\tvar successCount, errorCount atomic.Int64\n\tctx, cancel := context.WithTimeout(context.Background(), *duration)\n\tdefer cancel()\n\n\tlimiter := rate.NewLimiter(rate.Limit(*rps), 100) // Allow bursts up to 100\n\n\tfor i := 0; i \u003c *concurrency; i++ {\n\t\twg.Add(1)\n\t\tgo func(workerID int) {\n\t\t\tdefer wg.Done()\n\t\t\tclient := \u0026http.Client{\n\t\t\t\tTimeout: 5 * time.Second,\n\t\t\t}\n\n\t\t\tfor {\n\t\t\t\tselect {\n\t\t\t\tcase \u003c-ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\tdefault:\n\t\t\t\t\tlimiter.Wait(ctx) // Wait for token from rate limiter\n\n\t\t\t\t\teventID := uuid.NewString()\n\t\t\t\t\tpayload := fmt.Sprintf(`{\"event_id\": \"%s\", \"message\": \"load test event from worker %d\", \"timestamp\": \"%s\"}`,\n\t\t\t\t\t\teventID, workerID, time.Now().Format(time.RFC3339Nano))\n\n\t\t\t\t\treq, err := http.NewRequestWithContext(ctx, http.MethodPost, *targetURL, bytes.NewBufferString(payload))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tcontinue // Should not happen\n\t\t\t\t\t}\n\t\t\t\t\treq.Header.Set(\"Content-Type\", \"application/json\")\n\t\t\t\t\treq.Header.Set(\"X-API-Key\", *apiKey)\n\n\t\t\t\t\tresp, err := client.Do(req)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\terrorCount.Add(1)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\n\t\t\t\t\tif resp.StatusCode == http.StatusAccepted {\n\t\t\t\t\t\tsuccessCount.Add(1)\n\t\t\t\t\t} else {\n\t\t\t\t\t\terrorCount.Add(1)\n\t\t\t\t\t}\n\t\t\t\t\tresp.Body.Close()\n\t\t\t\t}\n\t\t\t}\n\t\t}(i)\n\t}\n\n\twg.Wait()\n\n\ttotalRequests := successCount.Load() + errorCount.Load()\n\tactualRPS := float64(totalRequests) / duration.Seconds()\n\n\tlog.Println(\"Load test finished.\")\n\tlog.Printf(\"Total Requests: %d\", totalRequests)\n\tlog.Printf(\"Successful (202 Accepted): %d\", successCount.Load())\n\tlog.Printf(\"Errors: %d\", errorCount.Load())\n\tlog.Printf(\"Actual RPS: %.2f\", actualRPS)\n}\n```"
}