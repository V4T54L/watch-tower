{
  "cmd/consumer/main.go": "package main\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"fmt\"\n\t\"log\"\n\t\"log/slog\"\n\t\"os\"\n\t\"os/signal\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/google/uuid\"\n\t\"github.com/redis/go-redis/v9\"\n\t\"github.com/user/log-ingestor/internal/adapter/repository/postgres\"\n\tredisrepo \"github.com/user/log-ingestor/internal/adapter/repository/redis\"\n\t\"github.com/user/log-ingestor/internal/pkg/config\"\n\t\"github.com/user/log-ingestor/internal/pkg/logger\"\n\t\"github.com/user/log-ingestor/internal/usecase\"\n\n\t_ \"github.com/lib/pq\"\n)\n\nconst (\n\tconsumerGroup    = \"log-processors\"\n\tprocessingInterval = 1 * time.Second\n)\n\nfunc main() {\n\tcfg, err := config.Load()\n\tif err != nil {\n\t\tlog.Fatalf(\"failed to load config: %v\", err)\n\t}\n\n\tappLogger := logger.New(cfg.LogLevel)\n\thostname, _ := os.Hostname()\n\tconsumerName := fmt.Sprintf(\"%s-%s\", hostname, uuid.NewString()[:8])\n\tappLogger = appLogger.With(\"consumer_name\", consumerName)\n\n\t// Database Connection\n\tdb, err := sql.Open(\"postgres\", cfg.PostgresURL)\n\tif err != nil {\n\t\tlog.Fatalf(\"failed to connect to postgres: %v\", err)\n\t}\n\tdefer db.Close()\n\n\t// Redis Client\n\tredisClient := redis.NewClient(\u0026redis.Options{\n\t\tAddr: cfg.RedisAddr,\n\t})\n\tif err := redisClient.Ping(context.Background()).Err(); err != nil {\n\t\tlog.Fatalf(\"failed to connect to redis: %v\", err)\n\t}\n\n\t// Repositories\n\t// The consumer doesn't need a WAL, so we pass nil.\n\tredisBufferRepo, err := redisrepo.NewLogRepository(redisClient, appLogger, consumerGroup, consumerName, cfg.RedisDLQStream, nil)\n\tif err != nil {\n\t\tlog.Fatalf(\"failed to create redis buffer repository: %v\", err)\n\t}\n\tpgSinkRepo := postgres.NewLogRepository(db, appLogger)\n\n\t// Use Case\n\tprocessUseCase := usecase.NewProcessLogsUseCase(\n\t\tredisBufferRepo,\n\t\tpgSinkRepo,\n\t\tappLogger,\n\t\tconsumerGroup,\n\t\tconsumerName,\n\t\tcfg.ConsumerRetryCount,\n\t\tcfg.ConsumerRetryBackoff,\n\t)\n\n\t// Graceful Shutdown Context\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\tstop := make(chan os.Signal, 1)\n\tsignal.Notify(stop, syscall.SIGINT, syscall.SIGTERM)\n\n\tgo func() {\n\t\t\u003c-stop\n\t\tappLogger.Info(\"Shutting down consumer...\")\n\t\tcancel()\n\t}()\n\n\tappLogger.Info(\"Starting consumer worker\")\n\tticker := time.NewTicker(processingInterval)\n\tdefer ticker.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase \u003c-ctx.Done():\n\t\t\tappLogger.Info(\"Consumer stopped\")\n\t\t\treturn\n\t\tcase \u003c-ticker.C:\n\t\t\tprocessed, err := processUseCase.ProcessBatch(ctx)\n\t\t\tif err != nil {\n\t\t\t\tappLogger.Error(\"Error processing batch\", \"error\", err)\n\t\t\t}\n\t\t\tif processed \u003e 0 {\n\t\t\t\tappLogger.Debug(\"Processed batch\", \"count\", processed)\n\t\t\t}\n\t\t}\n\t}\n}",
  "cmd/ingest/main.go": "package main\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"log\"\n\t\"log/slog\"\n\t\"net/http\"\n\t\"os\"\n\t\"os/signal\"\n\t\"strings\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/redis/go-redis/v9\"\n\t\"github.com/user/log-ingestor/internal/adapter/api\"\n\t\"github.com/user/log-ingestor/internal/adapter/pii\"\n\t\"github.com/user/log-ingestor/internal/adapter/repository/postgres\"\n\tredisrepo \"github.com/user/log-ingestor/internal/adapter/repository/redis\"\n\t\"github.com/user/log-ingestor/internal/adapter/repository/wal\"\n\t\"github.com/user/log-ingestor/internal/pkg/config\"\n\t\"github.com/user/log-ingestor/internal/pkg/logger\"\n\t\"github.com/user/log-ingestor/internal/usecase\"\n\n\t_ \"github.com/lib/pq\"\n)\n\nfunc main() {\n\tcfg, err := config.Load()\n\tif err != nil {\n\t\tlog.Fatalf(\"failed to load config: %v\", err)\n\t}\n\n\tappLogger := logger.New(cfg.LogLevel)\n\n\t// Database Connection\n\tdb, err := sql.Open(\"postgres\", cfg.PostgresURL)\n\tif err != nil {\n\t\tlog.Fatalf(\"failed to connect to postgres: %v\", err)\n\t}\n\tdefer db.Close()\n\n\t// Redis Client\n\tredisClient := redis.NewClient(\u0026redis.Options{\n\t\tAddr: cfg.RedisAddr,\n\t})\n\tif err := redisClient.Ping(context.Background()).Err(); err != nil {\n\t\tappLogger.Warn(\"Could not connect to Redis on startup, will operate in degraded mode\", \"error\", err)\n\t}\n\n\t// WAL Repository\n\twalRepo, err := wal.NewWALRepository(cfg.WALPath, cfg.WALSegmentSize, cfg.WALMaxDiskSize, appLogger)\n\tif err != nil {\n\t\tlog.Fatalf(\"failed to initialize WAL repository: %v\", err)\n\t}\n\tdefer walRepo.Close()\n\n\t// Repositories\n\tapiKeyRepo := postgres.NewAPIKeyRepository(db, appLogger, cfg.APIKeyCacheTTL)\n\tredisRepo, err := redisrepo.NewLogRepository(redisClient, appLogger, \"ingest-group\", \"ingest-worker\", cfg.RedisDLQStream, walRepo)\n\tif err != nil {\n\t\tlog.Fatalf(\"failed to create redis log repository: %v\", err)\n\t}\n\n\t// Start Redis health checker and WAL replayer in the background\n\tappCtx, cancelApp := context.WithCancel(context.Background())\n\tdefer cancelApp()\n\tgo redisRepo.StartHealthCheck(appCtx, 10*time.Second)\n\n\t// PII Redactor\n\tpiiFields := strings.Split(cfg.PIIRedactionFields, \",\")\n\tredactor := pii.NewRedactor(piiFields, appLogger)\n\n\t// Use Case\n\tingestUseCase := usecase.NewIngestLogUseCase(redisRepo, redactor, appLogger)\n\n\t// HTTP Server\n\trouter := api.NewRouter(cfg, appLogger, apiKeyRepo, ingestUseCase)\n\tserver := \u0026http.Server{\n\t\tAddr:    cfg.IngestServerAddr,\n\t\tHandler: router,\n\t}\n\n\tgo func() {\n\t\tappLogger.Info(\"Starting ingest server\", \"addr\", server.Addr)\n\t\tif err := server.ListenAndServe(); err != nil \u0026\u0026 err != http.ErrServerClosed {\n\t\t\tlog.Fatalf(\"could not listen on %s: %v\\n\", server.Addr, err)\n\t\t}\n\t}()\n\n\t// Graceful Shutdown\n\tstop := make(chan os.Signal, 1)\n\tsignal.Notify(stop, syscall.SIGINT, syscall.SIGTERM)\n\t\u003c-stop\n\n\tappLogger.Info(\"Shutting down server...\")\n\n\tshutdownCtx, cancelShutdown := context.WithTimeout(context.Background(), 5*time.Second)\n\tdefer cancelShutdown()\n\n\tif err := server.Shutdown(shutdownCtx); err != nil {\n\t\tlog.Fatalf(\"Server Shutdown Failed:%+v\", err)\n\t}\n\tappLogger.Info(\"Server gracefully stopped\")\n}\n",
  "go.mod": "module github.com/user/log-ingestor\n\ngo 1.21\n\nrequire (\n\tgithub.com/caarlos0/env/v10 v10.0.0\n\tgithub.com/google/uuid v1.6.0\n\tgithub.com/joho/godotenv v1.5.1\n\tgithub.com/lib/pq v1.10.9\n\tgithub.com/redis/go-redis/v9 v9.5.1\n)\n\nrequire (\n\tgithub.com/cespare/xxhash/v2 v2.2.0 // indirect\n\tgithub.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f // indirect\n)\n",
  "internal/adapter/api/handler/ingest_handler.go": "package handler\n\nimport (\n\t\"bufio\"\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"io\"\n\t\"log/slog\"\n\t\"net/http\"\n\n\t\"github.com/user/log-ingestor/internal/domain\"\n\t\"github.com/user/log-ingestor/internal/usecase\"\n)\n\n// IngestHandler handles HTTP requests for log ingestion.\ntype IngestHandler struct {\n\tuseCase      *usecase.IngestLogUseCase\n\tlogger       *slog.Logger\n\tmaxEventSize int64\n}\n\n// NewIngestHandler creates a new IngestHandler.\nfunc NewIngestHandler(uc *usecase.IngestLogUseCase, logger *slog.Logger, maxEventSize int64) *IngestHandler {\n\treturn \u0026IngestHandler{\n\t\tuseCase:      uc,\n\t\tlogger:       logger,\n\t\tmaxEventSize: maxEventSize,\n\t}\n}\n\n// ServeHTTP processes incoming log ingestion requests.\nfunc (h *IngestHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n\tif r.Method != http.MethodPost {\n\t\thttp.Error(w, \"Method not allowed\", http.StatusMethodNotAllowed)\n\t\treturn\n\t}\n\n\t// Enforce max body size\n\tr.Body = http.MaxBytesReader(w, r.Body, h.maxEventSize)\n\n\tcontentType := r.Header.Get(\"Content-Type\")\n\tvar err error\n\n\tswitch contentType {\n\tcase \"application/json\":\n\t\terr = h.handleSingleJSON(r.Context(), r.Body)\n\tcase \"application/x-ndjson\":\n\t\terr = h.handleNDJSON(r.Context(), r.Body)\n\tdefault:\n\t\thttp.Error(w, \"Unsupported Content-Type\", http.StatusUnsupportedMediaType)\n\t\treturn\n\t}\n\n\tif err != nil {\n\t\tvar maxBytesErr *http.MaxBytesError\n\t\tif errors.As(err, \u0026maxBytesErr) {\n\t\t\thttp.Error(w, \"Payload too large\", http.StatusRequestEntityTooLarge)\n\t\t\treturn\n\t\t}\n\t\th.logger.Error(\"failed to process ingest request\", \"error\", err)\n\t\thttp.Error(w, \"Bad request\", http.StatusBadRequest)\n\t\treturn\n\t}\n\n\tw.WriteHeader(http.StatusAccepted)\n}\n\nfunc (h *IngestHandler) handleSingleJSON(ctx context.Context, body io.Reader) error {\n\tvar event domain.LogEvent\n\tdecoder := json.NewDecoder(body)\n\tdecoder.DisallowUnknownFields()\n\n\t// To capture the raw event, we need to read the body first\n\trawBody, err := io.ReadAll(body)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif err := json.Unmarshal(rawBody, \u0026event); err != nil {\n\t\treturn err\n\t}\n\n\tevent.RawEvent = rawBody // Store raw event before redaction\n\n\treturn h.useCase.Ingest(ctx, \u0026event)\n}\n\nfunc (h *IngestHandler) handleNDJSON(ctx context.Context, body io.Reader) error {\n\tscanner := bufio.NewScanner(body)\n\tfor scanner.Scan() {\n\t\tline := scanner.Bytes()\n\t\tif len(line) == 0 {\n\t\t\tcontinue\n\t\t}\n\n\t\tvar event domain.LogEvent\n\t\tif err := json.Unmarshal(line, \u0026event); err != nil {\n\t\t\t// Log the error but continue processing other lines\n\t\t\th.logger.Warn(\"failed to unmarshal ndjson line\", \"error\", err, \"line\", string(line))\n\t\t\tcontinue\n\t\t}\n\n\t\tevent.RawEvent = line // Store raw event before redaction\n\n\t\tif err := h.useCase.Ingest(ctx, \u0026event); err != nil {\n\t\t\t// If a single event fails to be ingested, we log it and continue.\n\t\t\t// A more robust strategy might involve a DLQ at this stage or returning a partial success.\n\t\t\th.logger.Error(\"failed to ingest event from ndjson stream\", \"error\", err, \"event_id\", event.ID)\n\t\t}\n\t}\n\n\treturn scanner.Err()\n}\n",
  "internal/adapter/api/middleware/auth.go": "package middleware\n\nimport (\n\t\"log/slog\"\n\t\"net/http\"\n\n\t\"github.com/user/log-ingestor/internal/domain\"\n)\n\nconst APIKeyHeader = \"X-API-Key\"\n\n// Auth is a middleware factory that returns a new authentication middleware.\n// It checks for a valid API key in the X-API-Key header.\nfunc Auth(repo domain.APIKeyRepository, logger *slog.Logger) func(http.Handler) http.Handler {\n\treturn func(next http.Handler) http.Handler {\n\t\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\t\tapiKey := r.Header.Get(APIKeyHeader)\n\t\t\tif apiKey == \"\" {\n\t\t\t\tlogger.Warn(\"API key missing from request\", \"remote_addr\", r.RemoteAddr)\n\t\t\t\thttp.Error(w, \"Unauthorized: API key required\", http.StatusUnauthorized)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tisValid, err := repo.IsValid(r.Context(), apiKey)\n\t\t\tif err != nil {\n\t\t\t\tlogger.Error(\"failed to validate API key\", \"error\", err)\n\t\t\t\thttp.Error(w, \"Internal Server Error\", http.StatusInternalServerError)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tif !isValid {\n\t\t\t\tlogger.Warn(\"invalid API key provided\", \"remote_addr\", r.RemoteAddr)\n\t\t\t\thttp.Error(w, \"Unauthorized: Invalid API key\", http.StatusUnauthorized)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tnext.ServeHTTP(w, r)\n\t\t})\n\t}\n}\n",
  "internal/adapter/api/router.go": "package api\n\nimport (\n\t\"log/slog\"\n\t\"net/http\"\n\n\t\"github.com/user/log-ingestor/internal/adapter/api/handler\"\n\t\"github.com/user/log-ingestor/internal/adapter/api/middleware\"\n\t\"github.com/user/log-ingestor/internal/domain\"\n\t\"github.com/user/log-ingestor/internal/pkg/config\"\n\t\"github.com/user/log-ingestor/internal/usecase\"\n)\n\n// NewRouter creates and configures the main HTTP router for the ingest service.\nfunc NewRouter(\n\tcfg *config.Config,\n\tlogger *slog.Logger,\n\tapiKeyRepo domain.APIKeyRepository,\n\tingestUseCase *usecase.IngestLogUseCase,\n) http.Handler {\n\tmux := http.NewServeMux()\n\n\t// Ingest Handler\n\tingestHandler := handler.NewIngestHandler(ingestUseCase, logger, cfg.MaxEventSize)\n\n\t// Middleware\n\tauthMiddleware := middleware.Auth(apiKeyRepo, logger)\n\n\t// Routes\n\tmux.Handle(\"POST /ingest\", authMiddleware(ingestHandler))\n\n\t// Health check\n\tmux.HandleFunc(\"/health\", func(w http.ResponseWriter, r *http.Request) {\n\t\tw.WriteHeader(http.StatusOK)\n\t\tw.Write([]byte(\"OK\"))\n\t})\n\n\t// TODO: Add /metrics and /events endpoints in future steps\n\n\treturn mux\n}\n",
  "internal/adapter/pii/redactor.go": "package pii\n\nimport (\n\t\"encoding/json\"\n\t\"log/slog\"\n\n\t\"github.com/user/log-ingestor/internal/domain\"\n)\n\nconst RedactedPlaceholder = \"[REDACTED]\"\n\n// Redactor is responsible for redacting sensitive information from log events.\ntype Redactor struct {\n\tfieldsToRedact map[string]struct{} // Use a map for O(1) lookups\n\tlogger         *slog.Logger\n}\n\n// NewRedactor creates a new Redactor instance with a given set of fields to redact.\nfunc NewRedactor(fields []string, logger *slog.Logger) *Redactor {\n\tfieldSet := make(map[string]struct{}, len(fields))\n\tfor _, field := range fields {\n\t\tfieldSet[field] = struct{}{}\n\t}\n\treturn \u0026Redactor{\n\t\tfieldsToRedact: fieldSet,\n\t\tlogger:         logger,\n\t}\n}\n\n// Redact modifies the LogEvent in place to remove PII from its metadata.\n// It returns an error if JSON processing fails.\nfunc (r *Redactor) Redact(event *domain.LogEvent) error {\n\tif len(r.fieldsToRedact) == 0 || len(event.Metadata) == 0 {\n\t\treturn nil\n\t}\n\n\tvar metadata map[string]interface{}\n\tif err := json.Unmarshal(event.Metadata, \u0026metadata); err != nil {\n\t\tr.logger.Warn(\"failed to unmarshal metadata for PII redaction\", \"error\", err, \"event_id\", event.ID)\n\t\t// We can't process it, so we leave it as is.\n\t\treturn err\n\t}\n\n\tredacted := false\n\tfor field := range r.fieldsToRedact {\n\t\tif _, ok := metadata[field]; ok {\n\t\t\tmetadata[field] = RedactedPlaceholder\n\t\t\tredacted = true\n\t\t}\n\t}\n\n\tif redacted {\n\t\tevent.PIIRedacted = true\n\t\tmodifiedMetadata, err := json.Marshal(metadata)\n\t\tif err != nil {\n\t\t\tr.logger.Error(\"failed to marshal modified metadata after PII redaction\", \"error\", err, \"event_id\", event.ID)\n\t\t\t// This is a more serious internal error.\n\t\t\treturn err\n\t\t}\n\t\tevent.Metadata = modifiedMetadata\n\t}\n\n\treturn nil\n}\n",
  "internal/adapter/repository/postgres/apikey_repository.go": "package postgres\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"log/slog\"\n\t\"sync\"\n\t\"time\"\n)\n\ntype cacheEntry struct {\n\tisValid   bool\n\texpiresAt time.Time\n}\n\n// APIKeyRepository implements the domain.APIKeyRepository interface using PostgreSQL\n// as the source of truth and an in-memory, time-based cache.\ntype APIKeyRepository struct {\n\tdb       *sql.DB\n\tlogger   *slog.Logger\n\tcache    map[string]cacheEntry\n\tmu       sync.RWMutex\n\tcacheTTL time.Duration\n}\n\n// NewAPIKeyRepository creates a new instance of the PostgreSQL API key repository.\nfunc NewAPIKeyRepository(db *sql.DB, logger *slog.Logger, cacheTTL time.Duration) *APIKeyRepository {\n\treturn \u0026APIKeyRepository{\n\t\tdb:       db,\n\t\tlogger:   logger,\n\t\tcache:    make(map[string]cacheEntry),\n\t\tcacheTTL: cacheTTL,\n\t}\n}\n\n// IsValid checks if an API key is valid. It first checks a local cache and falls\n// back to the database if the key is not found or the cache entry has expired.\nfunc (r *APIKeyRepository) IsValid(ctx context.Context, key string) (bool, error) {\n\t// 1. Check cache with a read lock\n\tr.mu.RLock()\n\tentry, found := r.cache[key]\n\tif found \u0026\u0026 time.Now().Before(entry.expiresAt) {\n\t\tr.mu.RUnlock()\n\t\treturn entry.isValid, nil\n\t}\n\tr.mu.RUnlock()\n\n\t// 2. If not in cache or expired, query DB and update cache with a write lock\n\tr.mu.Lock()\n\tdefer r.mu.Unlock()\n\n\t// 2a. Double-check cache in case another goroutine populated it while waiting for the lock\n\tentry, found = r.cache[key]\n\tif found \u0026\u0026 time.Now().Before(entry.expiresAt) {\n\t\treturn entry.isValid, nil\n\t}\n\n\t// 3. Query the database\n\tvar isValid bool\n\t// A key is valid if it exists, is active, and has not expired.\n\tquery := `SELECT EXISTS(SELECT 1 FROM api_keys WHERE key = $1 AND is_active = true AND (expires_at IS NULL OR expires_at \u003e NOW()))`\n\terr := r.db.QueryRowContext(ctx, query, key).Scan(\u0026isValid)\n\tif err != nil {\n\t\tr.logger.Error(\"failed to validate API key in database\", \"error\", err)\n\t\t// Don't cache errors, let the next request retry from the DB\n\t\treturn false, err\n\t}\n\n\t// 4. Update cache\n\tr.cache[key] = cacheEntry{\n\t\tisValid:   isValid,\n\t\texpiresAt: time.Now().Add(r.cacheTTL),\n\t}\n\n\treturn isValid, nil\n}\n```",
  "internal/adapter/repository/postgres/log_repository.go": "package postgres\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"errors\"\n\t\"log/slog\"\n\n\t\"github.com/lib/pq\"\n\t\"github.com/user/log-ingestor/internal/domain\"\n)\n\nconst logsTableName = \"logs\"\n\n// LogRepository implements the sink part of the domain.LogRepository interface for PostgreSQL.\ntype LogRepository struct {\n\tdb     *sql.DB\n\tlogger *slog.Logger\n}\n\n// NewLogRepository creates a new PostgreSQL log repository.\nfunc NewLogRepository(db *sql.DB, logger *slog.Logger) *LogRepository {\n\treturn \u0026LogRepository{db: db, logger: logger}\n}\n\n// WriteLogBatch writes a batch of log events to PostgreSQL using the COPY protocol for high performance.\n// It uses an ON CONFLICT clause to perform an upsert, ensuring idempotency based on event_id.\nfunc (r *LogRepository) WriteLogBatch(ctx context.Context, events []domain.LogEvent) error {\n\tif len(events) == 0 {\n\t\treturn nil\n\t}\n\n\ttxn, err := r.db.BeginTx(ctx, nil)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer txn.Rollback() // Rollback is a no-op if Commit() is called\n\n\t// Use a temporary table to stage the data, then merge into the main table.\n\t// This is a common pattern for high-performance, idempotent bulk inserts.\n\ttempTableName := \"logs_temp_import\"\n\t_, err = txn.ExecContext(ctx, `CREATE TEMP TABLE `+tempTableName+` (LIKE logs INCLUDING DEFAULTS) ON COMMIT DROP;`)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tstmt, err := txn.Prepare(pq.CopyIn(tempTableName, \"event_id\", \"received_at\", \"event_time\", \"source\", \"level\", \"message\", \"metadata\"))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tfor _, event := range events {\n\t\t_, err = stmt.ExecContext(ctx, event.ID, event.ReceivedAt, event.EventTime, event.Source, event.Level, event.Message, event.Metadata)\n\t\tif err != nil {\n\t\t\t// Close the statement to avoid connection issues\n\t\t\t_ = stmt.Close()\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif err := stmt.Close(); err != nil {\n\t\treturn err\n\t}\n\n\t// Upsert from the temp table into the main table\n\tupsertQuery := `\n\t\tINSERT INTO logs (event_id, received_at, event_time, source, level, message, metadata)\n\t\tSELECT event_id, received_at, event_time, source, level, message, metadata FROM ` + tempTableName + `\n\t\tON CONFLICT (event_id) DO UPDATE SET\n\t\t\treceived_at = EXCLUDED.received_at,\n\t\t\tevent_time = EXCLUDED.event_time,\n\t\t\tsource = EXCLUDED.source,\n\t\t\tlevel = EXCLUDED.level,\n\t\t\tmessage = EXCLUDED.message,\n\t\t\tmetadata = EXCLUDED.metadata;\n\t`\n\t_, err = txn.ExecContext(ctx, upsertQuery)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn txn.Commit()\n}\n\n// The following methods are not implemented for the PostgreSQL sink repository.\nvar errNotImplemented = errors.New(\"method not implemented for this repository type\")\n\nfunc (r *LogRepository) BufferLog(ctx context.Context, event domain.LogEvent) error {\n\treturn errNotImplemented\n}\n\nfunc (r *LogRepository) ReadLogBatch(ctx context.Context, group, consumer string, count int) ([]domain.LogEvent, error) {\n\treturn nil, errNotImplemented\n}\n\nfunc (r *LogRepository) AcknowledgeLogs(ctx context.Context, group string, messageIDs ...string) error {\n\treturn errNotImplemented\n}\n",
  "internal/adapter/repository/redis/log_repository.go": "package redis\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/redis/go-redis/v9\"\n\t\"github.com/user/log-ingestor/internal/domain\"\n)\n\nconst (\n\tlogStreamKey = \"log_events\"\n)\n\nvar errNotImplemented = errors.New(\"method not implemented for this repository type\")\n\n// LogRepository implements the domain.LogRepository interface using Redis Streams.\n// It also includes a Write-Ahead Log (WAL) for failover.\ntype LogRepository struct {\n\tclient       *redis.Client\n\tlogger       *slog.Logger\n\twal          domain.WALRepository\n\tdlqStreamKey string\n\tisAvailable  atomic.Bool\n}\n\n// NewLogRepository creates a new Redis-backed LogRepository.\n// The WAL is optional; pass nil if not needed (e.g., for consumers).\nfunc NewLogRepository(client *redis.Client, logger *slog.Logger, group, consumer, dlqStreamKey string, wal domain.WALRepository) (*LogRepository, error) {\n\trepo := \u0026LogRepository{\n\t\tclient:       client,\n\t\tlogger:       logger.With(\"component\", \"redis_repository\"),\n\t\twal:          wal,\n\t\tdlqStreamKey: dlqStreamKey,\n\t}\n\trepo.isAvailable.Store(true) // Assume available initially\n\n\tif err := repo.setupConsumerGroup(context.Background(), group); err != nil {\n\t\trepo.isAvailable.Store(false)\n\t\trepo.logger.Error(\"Failed to setup consumer group, Redis may be unavailable on startup\", \"error\", err)\n\t}\n\n\treturn repo, nil\n}\n\n// StartHealthCheck starts a background goroutine to monitor Redis connectivity and trigger WAL replay.\nfunc (r *LogRepository) StartHealthCheck(ctx context.Context, interval time.Duration) {\n\tif r.wal == nil {\n\t\tr.logger.Info(\"WAL is not configured, skipping health check/replayer\")\n\t\treturn\n\t}\n\n\tticker := time.NewTicker(interval)\n\tdefer ticker.Stop()\n\n\tr.logger.Info(\"Starting Redis health check and WAL replayer\")\n\n\tfor {\n\t\tselect {\n\t\tcase \u003c-ctx.Done():\n\t\t\tr.logger.Info(\"Stopping Redis health check\")\n\t\t\treturn\n\t\tcase \u003c-ticker.C:\n\t\t\terr := r.client.Ping(ctx).Err()\n\t\t\tif err != nil {\n\t\t\t\tif r.isAvailable.CompareAndSwap(true, false) {\n\t\t\t\t\tr.logger.Error(\"Redis connection lost\", \"error\", err)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif r.isAvailable.CompareAndSwap(false, true) {\n\t\t\t\t\tr.logger.Info(\"Redis connection recovered\")\n\t\t\t\t\tif err := r.ReplayWAL(ctx); err != nil {\n\t\t\t\t\t\tr.logger.Error(\"Failed to replay WAL after Redis recovery\", \"error\", err)\n\t\t\t\t\t\tr.isAvailable.Store(false)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n// ReplayWAL replays events from the WAL to Redis and truncates the WAL on success.\nfunc (r *LogRepository) ReplayWAL(ctx context.Context) error {\n\tr.logger.Info(\"Attempting to replay WAL to Redis\")\n\treplayHandler := func(event domain.LogEvent) error {\n\t\treturn r.bufferLogToRedis(ctx, event)\n\t}\n\n\tif err := r.wal.Replay(ctx, replayHandler); err != nil {\n\t\treturn fmt.Errorf(\"WAL replay failed: %w\", err)\n\t}\n\n\tif err := r.wal.Truncate(ctx); err != nil {\n\t\treturn fmt.Errorf(\"failed to truncate WAL after successful replay: %w\", err)\n\t}\n\n\tr.logger.Info(\"WAL replay to Redis completed successfully\")\n\treturn nil\n}\n\nfunc (r *LogRepository) setupConsumerGroup(ctx context.Context, group string) error {\n\terr := r.client.XGroupCreateMkStream(ctx, logStreamKey, group, \"0\").Err()\n\tif err != nil \u0026\u0026 !isRedisBusyGroupError(err) {\n\t\treturn fmt.Errorf(\"failed to create consumer group: %w\", err)\n\t}\n\treturn nil\n}\n\n// BufferLog adds a log event to the Redis Stream, falling back to the WAL if Redis is unavailable.\nfunc (r *LogRepository) BufferLog(ctx context.Context, event domain.LogEvent) error {\n\tif !r.isAvailable.Load() {\n\t\tif r.wal == nil {\n\t\t\treturn errors.New(\"redis is unavailable and WAL is not configured\")\n\t\t}\n\t\tr.logger.Warn(\"Redis is unavailable, writing to WAL\", \"event_id\", event.ID)\n\t\treturn r.wal.Write(ctx, event)\n\t}\n\n\terr := r.bufferLogToRedis(ctx, event)\n\tif err != nil {\n\t\tif isNetworkError(err) {\n\t\t\tif r.isAvailable.CompareAndSwap(true, false) {\n\t\t\t\tr.logger.Error(\"Redis connection lost during write\", \"error\", err)\n\t\t\t}\n\t\t\tif r.wal == nil {\n\t\t\t\treturn fmt.Errorf(\"redis became unavailable and WAL is not configured: %w\", err)\n\t\t\t}\n\t\t\tr.logger.Warn(\"Redis became unavailable, writing to WAL\", \"event_id\", event.ID)\n\t\t\treturn r.wal.Write(ctx, event)\n\t\t}\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc (r *LogRepository) bufferLogToRedis(ctx context.Context, event domain.LogEvent) error {\n\tpayload, err := json.Marshal(event)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to marshal log event: %w\", err)\n\t}\n\n\targs := \u0026redis.XAddArgs{\n\t\tStream: logStreamKey,\n\t\tValues: map[string]interface{}{\"payload\": payload},\n\t}\n\n\tif err := r.client.XAdd(ctx, args).Err(); err != nil {\n\t\treturn fmt.Errorf(\"failed to XADD to redis stream: %w\", err)\n\t}\n\treturn nil\n}\n\n// ReadLogBatch reads a batch of log events from the Redis Stream for a consumer group.\nfunc (r *LogRepository) ReadLogBatch(ctx context.Context, group, consumer string, count int) ([]domain.LogEvent, error) {\n\targs := \u0026redis.XReadGroupArgs{\n\t\tGroup:    group,\n\t\tConsumer: consumer,\n\t\tStreams:  []string{logStreamKey, \"\u003e\"},\n\t\tCount:    int64(count),\n\t\tBlock:    2 * time.Second,\n\t}\n\n\tstreams, err := r.client.XReadGroup(ctx, args).Result()\n\tif err != nil {\n\t\tif errors.Is(err, redis.Nil) {\n\t\t\treturn nil, nil\n\t\t}\n\t\treturn nil, fmt.Errorf(\"failed to XREADGROUP from redis: %w\", err)\n\t}\n\n\tif len(streams) == 0 || len(streams[0].Messages) == 0 {\n\t\treturn nil, nil\n\t}\n\n\tmessages := streams[0].Messages\n\tevents := make([]domain.LogEvent, 0, len(messages))\n\tfor _, msg := range messages {\n\t\tpayload, ok := msg.Values[\"payload\"].(string)\n\t\tif !ok {\n\t\t\tr.logger.Warn(\"Invalid message format in stream, skipping\", \"message_id\", msg.ID)\n\t\t\tcontinue\n\t\t}\n\n\t\tvar event domain.LogEvent\n\t\tif err := json.Unmarshal([]byte(payload), \u0026event); err != nil {\n\t\t\tr.logger.Warn(\"Failed to unmarshal log event from stream, skipping\", \"message_id\", msg.ID, \"error\", err)\n\t\t\tcontinue\n\t\t}\n\t\tevent.StreamMessageID = msg.ID\n\t\tevents = append(events, event)\n\t}\n\n\treturn events, nil\n}\n\n// AcknowledgeLogs acknowledges processed messages in the Redis Stream.\nfunc (r *LogRepository) AcknowledgeLogs(ctx context.Context, group string, messageIDs ...string) error {\n\tif len(messageIDs) == 0 {\n\t\treturn nil\n\t}\n\tif err := r.client.XAck(ctx, logStreamKey, group, messageIDs...).Err(); err != nil {\n\t\treturn fmt.Errorf(\"failed to XACK messages in redis: %w\", err)\n\t}\n\treturn nil\n}\n\n// MoveToDLQ moves a batch of events to the Dead-Letter Queue stream.\nfunc (r *LogRepository) MoveToDLQ(ctx context.Context, events []domain.LogEvent) error {\n\tif len(events) == 0 {\n\t\treturn nil\n\t}\n\n\tpipe := r.client.Pipeline()\n\tfor _, event := range events {\n\t\tpayload, err := json.Marshal(event)\n\t\tif err != nil {\n\t\t\tr.logger.Error(\"Failed to marshal event for DLQ\", \"event_id\", event.ID, \"error\", err)\n\t\t\tcontinue\n\t\t}\n\t\targs := \u0026redis.XAddArgs{\n\t\t\tStream: r.dlqStreamKey,\n\t\t\tValues: map[string]interface{}{\n\t\t\t\t\"payload\":         payload,\n\t\t\t\t\"original_stream\": logStreamKey,\n\t\t\t\t\"original_msg_id\": event.StreamMessageID,\n\t\t\t\t\"failed_at\":       time.Now().UTC().Format(time.RFC3339),\n\t\t\t},\n\t\t}\n\t\tpipe.XAdd(ctx, args)\n\t}\n\n\t_, err := pipe.Exec(ctx)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to execute DLQ pipeline: %w\", err)\n\t}\n\tr.logger.Warn(\"Moved events to DLQ\", \"count\", len(events))\n\treturn nil\n}\n\n// WriteLogBatch is not implemented for this repository.\nfunc (r *LogRepository) WriteLogBatch(ctx context.Context, events []domain.LogEvent) error {\n\treturn errNotImplemented\n}\n\nfunc isRedisBusyGroupError(err error) bool {\n\treturn err != nil \u0026\u0026 err.Error() == \"BUSYGROUP Consumer Group name already exists\"\n}\n\nfunc isNetworkError(err error) bool {\n\tvar netErr net.Error\n\treturn errors.As(err, \u0026netErr) || errors.Is(err, redis.ErrClosed) || errors.Is(err, context.Canceled) || errors.Is(err, context.DeadlineExceeded)\n}\n",
  "internal/adapter/repository/wal/wal_repository.go": "package wal\n\nimport (\n\t\"bufio\"\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/user/log-ingestor/internal/domain\"\n)\n\nconst (\n\tsegmentPrefix = \"segment-\"\n\tfilePerm      = 0644\n)\n\n// WALRepository implements a file-based Write-Ahead Log.\ntype WALRepository struct {\n\tdir            string\n\tmaxSegmentSize int64\n\tmaxTotalSize   int64\n\tlogger         *slog.Logger\n\n\tmu             sync.Mutex\n\tcurrentSegment *os.File\n\tcurrentSize    int64\n}\n\n// NewWALRepository creates a new WALRepository.\nfunc NewWALRepository(dir string, maxSegmentSize, maxTotalSize int64, logger *slog.Logger) (*WALRepository, error) {\n\tif err := os.MkdirAll(dir, 0755); err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to create WAL directory %s: %w\", dir, err)\n\t}\n\n\tw := \u0026WALRepository{\n\t\tdir:            dir,\n\t\tmaxSegmentSize: maxSegmentSize,\n\t\tmaxTotalSize:   maxTotalSize,\n\t\tlogger:         logger.With(\"component\", \"wal_repository\"),\n\t}\n\n\tif err := w.openLatestSegment(); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn w, nil\n}\n\n// Write appends an event to the current WAL segment.\nfunc (w *WALRepository) Write(ctx context.Context, event domain.LogEvent) error {\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\n\tdata, err := json.Marshal(event)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to marshal log event for WAL: %w\", err)\n\t}\n\tdata = append(data, '\\n')\n\n\tif w.currentSegment == nil {\n\t\tif err := w.rotate(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Check total size before writing\n\ttotalSize, err := w.calculateTotalSize()\n\tif err != nil {\n\t\tw.logger.Error(\"Failed to calculate total WAL size\", \"error\", err)\n\t\treturn fmt.Errorf(\"could not verify WAL disk space: %w\", err)\n\t}\n\tif totalSize+int64(len(data)) \u003e w.maxTotalSize {\n\t\treturn fmt.Errorf(\"WAL max total size exceeded (%d \u003e %d)\", totalSize, w.maxTotalSize)\n\t}\n\n\tn, err := w.currentSegment.Write(data)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to write to WAL segment: %w\", err)\n\t}\n\tw.currentSize += int64(n)\n\n\tif w.currentSize \u003e= w.maxSegmentSize {\n\t\tif err := w.rotate(); err != nil {\n\t\t\tw.logger.Error(\"Failed to rotate WAL segment\", \"error\", err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// Replay reads all WAL segments and calls the handler for each event.\nfunc (w *WALRepository) Replay(ctx context.Context, handler func(event domain.LogEvent) error) error {\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\n\tif w.currentSegment != nil {\n\t\tw.currentSegment.Close()\n\t\tw.currentSegment = nil\n\t}\n\n\tsegments, err := w.getSortedSegments()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif len(segments) == 0 {\n\t\tw.logger.Info(\"WAL is empty, nothing to replay\")\n\t\treturn nil\n\t}\n\tw.logger.Info(\"Starting WAL replay\", \"segment_count\", len(segments))\n\n\tfor _, segmentPath := range segments {\n\t\tfile, err := os.Open(segmentPath)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to open segment %s for replay: %w\", segmentPath, err)\n\t\t}\n\n\t\tscanner := bufio.NewScanner(file)\n\t\tfor scanner.Scan() {\n\t\t\tif ctx.Err() != nil {\n\t\t\t\tfile.Close()\n\t\t\t\treturn ctx.Err()\n\t\t\t}\n\t\t\tvar event domain.LogEvent\n\t\t\tif err := json.Unmarshal(scanner.Bytes(), \u0026event); err != nil {\n\t\t\t\tw.logger.Warn(\"Failed to unmarshal event from WAL, skipping\", \"error\", err, \"line\", scanner.Text())\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif err := handler(event); err != nil {\n\t\t\t\tfile.Close()\n\t\t\t\tw.logger.Error(\"WAL replay handler failed, stopping replay\", \"error\", err)\n\t\t\t\treturn fmt.Errorf(\"replay handler failed: %w\", err)\n\t\t\t}\n\t\t}\n\t\tif err := scanner.Err(); err != nil {\n\t\t\tfile.Close()\n\t\t\treturn fmt.Errorf(\"error scanning segment %s: %w\", segmentPath, err)\n\t\t}\n\t\tfile.Close()\n\t}\n\n\tw.logger.Info(\"WAL replay completed\")\n\treturn nil\n}\n\n// Truncate removes all WAL segment files.\nfunc (w *WALRepository) Truncate(ctx context.Context) error {\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\n\tif w.currentSegment != nil {\n\t\tw.currentSegment.Close()\n\t\tw.currentSegment = nil\n\t}\n\n\tsegments, err := w.getSortedSegments()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tfor _, segmentPath := range segments {\n\t\tif err := os.Remove(segmentPath); err != nil {\n\t\t\tw.logger.Error(\"Failed to remove WAL segment\", \"path\", segmentPath, \"error\", err)\n\t\t}\n\t}\n\n\tw.logger.Info(\"WAL truncated\")\n\treturn w.openLatestSegment()\n}\n\nfunc (w *WALRepository) rotate() error {\n\tif w.currentSegment != nil {\n\t\tif err := w.currentSegment.Sync(); err != nil {\n\t\t\tw.logger.Error(\"Failed to sync WAL segment before rotating\", \"error\", err)\n\t\t}\n\t\tif err := w.currentSegment.Close(); err != nil {\n\t\t\tw.logger.Error(\"Failed to close WAL segment before rotating\", \"error\", err)\n\t\t}\n\t\tw.currentSegment = nil\n\t}\n\n\tsegmentName := fmt.Sprintf(\"%s%d.log\", segmentPrefix, time.Now().UnixNano())\n\tpath := filepath.Join(w.dir, segmentName)\n\n\tf, err := os.OpenFile(path, os.O_APPEND|os.O_CREATE|os.O_WRONLY, filePerm)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to create new WAL segment %s: %w\", path, err)\n\t}\n\n\tw.currentSegment = f\n\tw.currentSize = 0\n\tw.logger.Info(\"Rotated to new WAL segment\", \"path\", path)\n\treturn nil\n}\n\nfunc (w *WALRepository) openLatestSegment() error {\n\tsegments, err := w.getSortedSegments()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif len(segments) == 0 {\n\t\treturn w.rotate()\n\t}\n\n\tlatestSegmentPath := segments[len(segments)-1]\n\tstat, err := os.Stat(latestSegmentPath)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to stat latest segment %s: %w\", latestSegmentPath, err)\n\t}\n\n\tf, err := os.OpenFile(latestSegmentPath, os.O_APPEND|os.O_WRONLY, filePerm)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to open latest segment %s: %w\", latestSegmentPath, err)\n\t}\n\n\tw.currentSegment = f\n\tw.currentSize = stat.Size()\n\tw.logger.Info(\"Opened existing WAL segment\", \"path\", latestSegmentPath, \"size\", w.currentSize)\n\n\tif w.currentSize \u003e= w.maxSegmentSize {\n\t\treturn w.rotate()\n\t}\n\n\treturn nil\n}\n\nfunc (w *WALRepository) getSortedSegments() ([]string, error) {\n\tentries, err := os.ReadDir(w.dir)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to read WAL directory: %w\", err)\n\t}\n\n\tvar segments []string\n\tfor _, entry := range entries {\n\t\tif !entry.IsDir() \u0026\u0026 strings.HasPrefix(entry.Name(), segmentPrefix) {\n\t\t\tsegments = append(segments, filepath.Join(w.dir, entry.Name()))\n\t\t}\n\t}\n\tsort.Strings(segments)\n\treturn segments, nil\n}\n\nfunc (w *WALRepository) calculateTotalSize() (int64, error) {\n\tvar totalSize int64\n\tentries, err := os.ReadDir(w.dir)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tfor _, entry := range entries {\n\t\tif !entry.IsDir() \u0026\u0026 strings.HasPrefix(entry.Name(), segmentPrefix) {\n\t\t\tinfo, err := entry.Info()\n\t\t\tif err != nil {\n\t\t\t\treturn 0, err\n\t\t\t}\n\t\t\ttotalSize += info.Size()\n\t\t}\n\t}\n\treturn totalSize, nil\n}\n\n// Close ensures the current segment is closed gracefully.\nfunc (w *WALRepository) Close() error {\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\tif w.currentSegment != nil {\n\t\treturn w.currentSegment.Close()\n\t}\n\treturn nil\n}\n",
  "internal/domain/log.go": "package domain\n\nimport (\n\t\"encoding/json\"\n\t\"time\"\n)\n\n// LogEvent represents a single log event.\ntype LogEvent struct {\n\tID              string          `json:\"event_id\"`\n\tReceivedAt      time.Time       `json:\"received_at\"`\n\tEventTime       time.Time       `json:\"event_time\"`\n\tSource          string          `json:\"source,omitempty\"`\n\tLevel           string          `json:\"level,omitempty\"`\n\tMessage         string          `json:\"message\"`\n\tMetadata        json.RawMessage `json:\"metadata,omitempty\"`\n\tRawEvent        json.RawMessage `json:\"-\"` // The original raw event payload, not for final serialization.\n\tPIIRedacted     bool            `json:\"pii_redacted,omitempty\"`\n\tStreamMessageID string          `json:\"-\"` // Transient field for Redis Stream message ID, not serialized.\n}\n",
  "internal/domain/repository.go": "package domain\n\nimport (\n\t\"context\"\n)\n\n// LogRepository defines the contract for storing, retrieving, and managing log events.\ntype LogRepository interface {\n\t// BufferLog adds a single log event to a durable buffer (e.g., Redis Stream, WAL).\n\tBufferLog(ctx context.Context, event LogEvent) error\n\n\t// ReadLogBatch reads a batch of log events from the buffer for a specific consumer group.\n\tReadLogBatch(ctx context.Context, group, consumer string, count int) ([]LogEvent, error)\n\n\t// WriteLogBatch writes a batch of log events to the final persistent storage (e.g., PostgreSQL).\n\tWriteLogBatch(ctx context.Context, events []LogEvent) error\n\n\t// AcknowledgeLogs marks log events as successfully processed in the buffer.\n\tAcknowledgeLogs(ctx context.Context, group string, messageIDs ...string) error\n\n\t// MoveToDLQ moves a batch of events to a Dead-Letter Queue.\n\tMoveToDLQ(ctx context.Context, events []LogEvent) error\n}\n\n// APIKeyRepository defines the contract for validating API keys.\ntype APIKeyRepository interface {\n\tIsValid(ctx context.Context, key string) (bool, error)\n}\n\n// WALRepository defines the contract for a Write-Ahead Log.\ntype WALRepository interface {\n\tWrite(ctx context.Context, event LogEvent) error\n\tReplay(ctx context.Context, handler func(event LogEvent) error) error\n\tTruncate(ctx context.Context) error\n\tClose() error\n}\n",
  "internal/pkg/config/config.go": "package config\n\nimport (\n\t\"time\"\n\n\t\"github.com/caarlos0/env/v10\"\n\t\"github.com/joho/godotenv\"\n)\n\n// Config holds all application configuration parameters.\ntype Config struct {\n\tLogLevel             string        `env:\"LOG_LEVEL\" envDefault:\"info\"`\n\tMaxEventSize         int64         `env:\"MAX_EVENT_SIZE\" envDefault:\"1048576\"`    // 1MB\n\tWALPath              string        `env:\"WAL_PATH\" envDefault:\"./wal\"`             // Path for Write-Ahead Log files\n\tWALSegmentSize       int64         `env:\"WAL_SEGMENT_SIZE\" envDefault:\"104857600\"` // 100MB\n\tWALMaxDiskSize       int64         `env:\"WAL_MAX_DISK_SIZE\" envDefault:\"1073741824\"` // 1GB\n\tBackpressurePolicy   string        `env:\"BACKPRESSURE_POLICY\" envDefault:\"block\"`\n\tRedisAddr            string        `env:\"REDIS_ADDR,required\"`\n\tRedisDLQStream       string        `env:\"REDIS_DLQ_STREAM\" envDefault:\"log_events_dlq\"`\n\tPostgresURL          string        `env:\"POSTGRES_URL,required\"`\n\tAPIKeyCacheTTL       time.Duration `env:\"API_KEY_CACHE_TTL\" envDefault:\"5m\"`\n\tPIIRedactionFields   string        `env:\"PII_REDACTION_FIELDS\" envDefault:\"email,password,credit_card,ssn\"`\n\tIngestServerAddr     string        `env:\"INGEST_SERVER_ADDR\" envDefault:\":8080\"`\n\tConsumerRetryCount   int           `env:\"CONSUMER_RETRY_COUNT\" envDefault:\"3\"`\n\tConsumerRetryBackoff time.Duration `env:\"CONSUMER_RETRY_BACKOFF\" envDefault:\"1s\"`\n}\n\n// Load reads configuration from environment variables.\nfunc Load() (*Config, error) {\n\t// Load .env file if it exists (for local development)\n\t_ = godotenv.Load()\n\n\tcfg := \u0026Config{}\n\tif err := env.Parse(cfg); err != nil {\n\t\treturn nil, err\n\t}\n\treturn cfg, nil\n}\n",
  "internal/pkg/logger/logger.go": "package logger\n\nimport (\n\t\"log/slog\"\n\t\"os\"\n\t\"strings\"\n)\n\n// New creates and configures a new slog.Logger.\nfunc New(level string) *slog.Logger {\n\tvar logLevel slog.Level\n\n\tswitch strings.ToLower(level) {\n\tcase \"debug\":\n\t\tlogLevel = slog.LevelDebug\n\tcase \"info\":\n\t\tlogLevel = slog.LevelInfo\n\tcase \"warn\", \"warning\":\n\t\tlogLevel = slog.LevelWarn\n\tcase \"error\":\n\t\tlogLevel = slog.LevelError\n\tdefault:\n\t\tlogLevel = slog.LevelInfo\n\t}\n\n\topts := \u0026slog.HandlerOptions{\n\t\tLevel: logLevel,\n\t}\n\n\thandler := slog.NewJSONHandler(os.Stdout, opts)\n\tlogger := slog.New(handler)\n\n\treturn logger\n}\n```",
  "internal/usecase/ingest_log.go": "package usecase\n\nimport (\n\t\"context\"\n\t\"log/slog\"\n\t\"time\"\n\n\t\"github.com/google/uuid\"\n\t\"github.com/user/log-ingestor/internal/adapter/pii\"\n\t\"github.com/user/log-ingestor/internal/domain\"\n)\n\n// IngestLogUseCase handles the business logic for ingesting a log event.\ntype IngestLogUseCase struct {\n\trepo     domain.LogRepository\n\tredactor *pii.Redactor\n\tlogger   *slog.Logger\n}\n\n// NewIngestLogUseCase creates a new IngestLogUseCase.\nfunc NewIngestLogUseCase(repo domain.LogRepository, redactor *pii.Redactor, logger *slog.Logger) *IngestLogUseCase {\n\treturn \u0026IngestLogUseCase{\n\t\trepo:     repo,\n\t\tredactor: redactor,\n\t\tlogger:   logger,\n\t}\n}\n\n// Ingest validates, enriches, redacts, and buffers a log event.\nfunc (uc *IngestLogUseCase) Ingest(ctx context.Context, event *domain.LogEvent) error {\n\t// 1. Enrich with server-side data\n\tevent.ReceivedAt = time.Now().UTC()\n\tif event.ID == \"\" {\n\t\tevent.ID = uuid.NewString()\n\t}\n\n\t// 2. Redact PII\n\tif err := uc.redactor.Redact(event); err != nil {\n\t\tuc.logger.Warn(\"failed to redact PII, proceeding with original event\", \"error\", err, \"event_id\", event.ID)\n\t\t// Non-fatal error, we still ingest the log\n\t}\n\n\t// 3. Buffer the log\n\tif err := uc.repo.BufferLog(ctx, *event); err != nil {\n\t\tuc.logger.Error(\"failed to buffer log event\", \"error\", err, \"event_id\", event.ID)\n\t\t// TODO: Implement WAL fallback logic here\n\t\treturn err\n\t}\n\n\treturn nil\n}\n",
  "internal/usecase/process_logs.go": "package usecase\n\nimport (\n\t\"context\"\n\t\"log/slog\"\n\t\"math\"\n\t\"time\"\n\n\t\"github.com/user/log-ingestor/internal/domain\"\n)\n\nconst (\n\tdefaultBatchSize = 1000\n)\n\n// ProcessLogsUseCase orchestrates reading logs from a buffer and writing to a sink.\ntype ProcessLogsUseCase struct {\n\tbufferRepo   domain.LogRepository\n\tsinkRepo     domain.LogRepository\n\tlogger       *slog.Logger\n\tgroup        string\n\tconsumer     string\n\tretryCount   int\n\tretryBackoff time.Duration\n}\n\n// NewProcessLogsUseCase creates a new ProcessLogsUseCase.\nfunc NewProcessLogsUseCase(bufferRepo, sinkRepo domain.LogRepository, logger *slog.Logger, group, consumer string, retryCount int, retryBackoff time.Duration) *ProcessLogsUseCase {\n\treturn \u0026ProcessLogsUseCase{\n\t\tbufferRepo:   bufferRepo,\n\t\tsinkRepo:     sinkRepo,\n\t\tlogger:       logger.With(\"component\", \"process_logs_usecase\"),\n\t\tgroup:        group,\n\t\tconsumer:     consumer,\n\t\tretryCount:   retryCount,\n\t\tretryBackoff: retryBackoff,\n\t}\n}\n\n// ProcessBatch reads a batch of logs, attempts to write them to the sink with retries,\n// moves to DLQ on failure, and acknowledges on success.\nfunc (u *ProcessLogsUseCase) ProcessBatch(ctx context.Context) (int, error) {\n\tevents, err := u.bufferRepo.ReadLogBatch(ctx, u.group, u.consumer, defaultBatchSize)\n\tif err != nil {\n\t\tu.logger.Error(\"Failed to read log batch from buffer\", \"error\", err)\n\t\treturn 0, err\n\t}\n\n\tif len(events) == 0 {\n\t\treturn 0, nil\n\t}\n\n\tu.logger.Debug(\"Read batch from buffer\", \"count\", len(events))\n\n\terr = u.writeWithRetry(ctx, events)\n\tif err != nil {\n\t\tu.logger.Error(\"Failed to write batch to sink after all retries, moving to DLQ\", \"error\", err, \"batch_size\", len(events))\n\t\tif dlqErr := u.bufferRepo.MoveToDLQ(ctx, events); dlqErr != nil {\n\t\t\tu.logger.Error(\"CRITICAL: Failed to move events to DLQ. Events will be re-processed.\", \"error\", dlqErr)\n\t\t\treturn 0, dlqErr\n\t\t}\n\t}\n\n\tmessageIDs := make([]string, len(events))\n\tfor i, event := range events {\n\t\tmessageIDs[i] = event.StreamMessageID\n\t}\n\n\tif ackErr := u.bufferRepo.AcknowledgeLogs(ctx, u.group, messageIDs...); ackErr != nil {\n\t\tu.logger.Error(\"Failed to acknowledge processed logs\", \"error\", ackErr)\n\t\treturn 0, ackErr\n\t}\n\n\tu.logger.Info(\"Successfully processed batch\", \"count\", len(events), \"final_status\", map[bool]string{true: \"SINKED\", false: \"DLQED\"}[err == nil])\n\treturn len(events), nil\n}\n\nfunc (u *ProcessLogsUseCase) writeWithRetry(ctx context.Context, events []domain.LogEvent) error {\n\tvar lastErr error\n\n\tfor i := 0; i \u003c u.retryCount; i++ {\n\t\terr := u.sinkRepo.WriteLogBatch(ctx, events)\n\t\tif err == nil {\n\t\t\treturn nil // Success\n\t\t}\n\t\tlastErr = err\n\n\t\tif i == u.retryCount-1 || ctx.Err() != nil {\n\t\t\tbreak\n\t\t}\n\n\t\t// Exponential backoff: 1s, 2s, 4s, ... for default backoff of 1s\n\t\tdelay := time.Duration(float64(u.retryBackoff) * math.Pow(2, float64(i)))\n\t\tu.logger.Warn(\"Failed to write to sink, retrying...\", \"attempt\", i+1, \"delay\", delay, \"error\", err)\n\n\t\tselect {\n\t\tcase \u003c-time.After(delay):\n\t\tcase \u003c-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\t}\n\t}\n\treturn lastErr\n}\n"
}